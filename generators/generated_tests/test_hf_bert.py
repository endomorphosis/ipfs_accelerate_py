#\!/usr/bin/env python3
"""
Test for bert model with comprehensive hardware platform support
Generated by fixed_merged_test_generator.py using templates
Template version: 1.0.1
"""

import os
import sys
import unittest
import importlib.util
import logging
import torch
import numpy as np
from transformers import AutoModel, AutoTokenizer, AutoConfig

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Hardware detection with centralized system
try:
    from centralized_hardware_detection.hardware_detection import detect_hardware_capabilities
    HAS_CENTRALIZED_DETECTION = True
except ImportError:
    HAS_CENTRALIZED_DETECTION = False
    
# Fallback hardware detection
HAS_CUDA = torch.cuda.is_available()
HAS_ROCM = (HAS_CUDA and hasattr(torch, '_C') and hasattr(torch._C, '_rocm_version')) or ('ROCM_HOME' in os.environ)
HAS_MPS = hasattr(torch, "mps") and hasattr(torch.mps, "is_available") and torch.mps.is_available()
HAS_OPENVINO = importlib.util.find_spec("openvino") is not None
HAS_QUALCOMM = importlib.util.find_spec("qnn_wrapper") is not None or importlib.util.find_spec("qti") is not None
HAS_WEBNN = importlib.util.find_spec("webnn") is not None or "WEBNN_AVAILABLE" in os.environ
HAS_WEBGPU = importlib.util.find_spec("webgpu") is not None or "WEBGPU_AVAILABLE" in os.environ

class TestBertModels(unittest.TestCase):
    """Test bert model with cross-platform hardware support."""
    
    def setUp(self):
        """Set up the test environment."""
        self.model_id = "bert-base-uncased"
        self.tokenizer = None
        self.model = None
        self.processor = None
        self.modality = "text"
        
        # Detect hardware capabilities
        if HAS_CENTRALIZED_DETECTION:
            self.hardware_capabilities = detect_hardware_capabilities()
        else:
            self.hardware_capabilities = {
                "cuda": HAS_CUDA,
                "rocm": HAS_ROCM,
                "mps": HAS_MPS,
                "openvino": HAS_OPENVINO,
                "qualcomm": HAS_QUALCOMM,
                "webnn": HAS_WEBNN,
                "webgpu": HAS_WEBGPU
            }
            
    def test_cpu(self):
        """Test bert with CPU."""
        self.run_test_on_platform("cpu")
            
    def test_cuda(self):
        """Test bert with CUDA."""
        if not HAS_CUDA:
            self.skipTest("CUDA not available")
        self.run_test_on_platform("cuda")
            
    def test_rocm(self):
        """Test bert with ROCm."""
        if not HAS_ROCM:
            self.skipTest("ROCm not available")
        self.run_test_on_platform("cuda")  # ROCm uses CUDA API
            
    def test_mps(self):
        """Test bert with MPS."""
        if not HAS_MPS:
            self.skipTest("MPS not available")
        self.run_test_on_platform("mps")
            
    def test_openvino(self):
        """Test bert with OpenVINO."""
        if not HAS_OPENVINO:
            self.skipTest("OpenVINO not available")
        self.run_test_on_platform("openvino")
            
    def test_qualcomm(self):
        """Test bert with Qualcomm AI Engine."""
        if not HAS_QUALCOMM:
            self.skipTest("Qualcomm AI Engine not available")
        self.run_test_on_platform("qualcomm")
            
    def test_webnn(self):
        """Test bert with WebNN."""
        if not HAS_WEBNN:
            self.skipTest("WebNN not available")
        self.run_test_on_platform("webnn")
            
    def test_webgpu(self):
        """Test bert with WebGPU."""
        if not HAS_WEBGPU:
            self.skipTest("WebGPU not available")
        self.run_test_on_platform("webgpu")
    
    def run_test_on_platform(self, platform):
        """Run test on the specified platform."""
        logger.info(f"Testing {self.model_id} on {platform}")
        
        try:
            # Initialize tokenizer
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_id)
            
            # Initialize model
            self.model = AutoModel.from_pretrained(self.model_id)
            
            # Move model to device if not CPU
            device = platform
            if platform == "cuda" or platform == "rocm":
                device = "cuda"
            elif platform == "openvino" or platform == "qualcomm" or platform == "webnn" or platform == "webgpu":
                device = "cpu"  # Using CPU with platform-specific optimizations
                
            if device != "cpu":
                self.model = self.model.to(device)
            
            # Prepare input
            inputs = self.tokenizer("Test input for bert", return_tensors="pt")
            
            # Move inputs to device if not CPU
            if device != "cpu":
                inputs = {k: v.to(device) for k, v in inputs.items()}
            
            # Run inference
            with torch.no_grad():
                outputs = self.model(**inputs)
            
            # Verify outputs
            self.assertIsNotNone(outputs)
            self.assertIn("last_hidden_state", outputs)
            
            # Log success
            logger.info(f"Successfully tested {self.model_id} on {platform}")
            
        except Exception as e:
            logger.error(f"Error testing {self.model_id} on {platform}: {str(e)}")
            raise

if __name__ == "__main__":
    unittest.main()
