# IPFS Accelerate Python

> **Enterprise-grade hardware-accelerated machine learning inference with IPFS network-based distribution**

[![PyPI version](https://badge.fury.io/py/ipfs-accelerate-py.svg)](https://badge.fury.io/py/ipfs-accelerate-py)
[![License: AGPL v3](https://img.shields.io/badge/License-AGPL_v3-blue.svg)](https://www.gnu.org/licenses/agpl-3.0)
[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![Documentation](https://img.shields.io/badge/docs-comprehensive-brightgreen.svg)](docs/README.md)
[![Tests](https://img.shields.io/badge/tests-passing-success.svg)](docs/TESTING.md)

---

## ğŸ“‹ Table of Contents

- [Overview](#-overview)
- [Installation](#-installation)
- [Quick Start](#-quick-start)
- [Architecture](#ï¸-architecture)
- [Supported Hardware](#-supported-hardware)
- [Supported Models](#-supported-models)
- [Documentation](#-documentation)
- [IPFS & Distributed Features](#-ipfs--distributed-features)
- [Performance & Optimization](#-performance--optimization)
- [Troubleshooting](#-troubleshooting)
- [Testing & Quality](#-testing--quality)
- [Contributing](#-contributing)
- [License](#-license)

---

## ğŸš€ Overview

**IPFS Accelerate Python** combines cutting-edge hardware acceleration, distributed computing, and IPFS network integration to deliver **blazing-fast machine learning inference** across multiple platforms and devices - from data centers to browsers.

### âš¡ Key Highlights

- ğŸ”¥ **8+ Hardware Platforms** - CPU, CUDA, ROCm, OpenVINO, Apple MPS, WebNN, WebGPU, Qualcomm
- ğŸŒ **Distributed by Design** - IPFS content addressing, P2P inference, global caching
- ğŸ¤– **300+ Models** - Full HuggingFace compatibility + custom architectures
- ğŸŒ **Browser-Native** - WebNN & WebGPU for client-side acceleration
- ğŸ“Š **Production Ready** - Real-time monitoring, enterprise security, compliance validation
- âš¡ **High Performance** - Intelligent caching, batch processing, model optimization

---

## ğŸ“¦ Installation

### Quick Start (5 minutes)

```bash
# 1. Create virtual environment
python3 -m venv .venv
source .venv/bin/activate  # Windows: .venv\Scripts\activate

# 2. Install IPFS Accelerate
pip install -U pip setuptools wheel
pip install ipfs-accelerate-py

# 3. Verify installation
python -c "from ipfs_accelerate_py import IPFSAccelerator; print('âœ… Ready!')"
```

### Installation Profiles

Choose the profile that matches your needs:

| Profile | Use Case | Installation |
|---------|----------|--------------|
| **Core** | Basic inference | `pip install ipfs-accelerate-py` |
| **Full** | Models + API server | `pip install ipfs-accelerate-py[full]` |
| **MCP** | MCP server extras | `pip install ipfs-accelerate-py[mcp]` |
| **Dev** | Development setup | `pip install -e .` |

ğŸ“š **Detailed instructions**: [Installation Guide](docs/guides/INSTALL.md) | [Troubleshooting](docs/INSTALLATION_TROUBLESHOOTING_GUIDE.md) | [Getting Started](docs/GETTING_STARTED.md)

---

## ğŸ¯ Quick Start

### Python API

```python
from ipfs_accelerate_py import IPFSAccelerator

# Initialize with automatic hardware detection
accelerator = IPFSAccelerator()

# Load any HuggingFace model
model = accelerator.load_model("bert-base-uncased")

# Run inference (automatically optimized for your hardware)
result = model.inference("Hello, world!")
print(result)
```

### Command Line Interface

```bash
# Start the MCP server for automation
ipfs-accelerate mcp start

# Run inference directly
ipfs-accelerate inference generate \
  --model bert-base-uncased \
  --input "Hello, world!"

# List available models and hardware
ipfs-accelerate models list
ipfs-accelerate hardware status

# Start GitHub Actions autoscaler
ipfs-accelerate github autoscaler
```

### Real-World Examples

| Example | Description | Complexity |
|---------|-------------|------------|
| [Basic Usage](examples/basic_usage.py) | Simple inference with BERT | Beginner |
| [Hardware Selection](examples/hardware_selection.py) | Choose specific accelerator | Intermediate |
| [Distributed Inference](examples/p2p_inference.py) | P2P model sharing | Advanced |
| [Browser Integration](examples/webnn_demo.py) | WebNN/WebGPU in browsers | Advanced |

ğŸ“– **More examples**: [examples/](examples/) | [Quick Start Guide](docs/guides/QUICKSTART.md)

---

## ğŸ—ï¸ Architecture

IPFS Accelerate Python is built on a **modular, enterprise-grade architecture**:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Application Layer                      â”‚
â”‚  Python API â€¢ CLI â€¢ MCP Server â€¢ Web Dashboard          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Hardware Abstraction Layer                  â”‚
â”‚  Unified interface across 8+ hardware platforms          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                Inference Backends                        â”‚
â”‚  CPU â€¢ CUDA â€¢ ROCm â€¢ MPS â€¢ OpenVINO â€¢ WebNN â€¢ WebGPU    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              IPFS Network Layer                          â”‚
â”‚  Content addressing â€¢ P2P â€¢ Distributed caching          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Core Components

- **Hardware Abstraction**: Unified API across 8+ platforms with automatic selection
- **IPFS Integration**: Content-addressed storage, P2P distribution, intelligent caching
- **Performance Modeling**: ML-powered optimization and resource management
- **MCP Server**: Model Context Protocol for standardized automation
- **Monitoring**: Real-time metrics, profiling, and analytics

ğŸ“ **Detailed architecture**: [docs/ARCHITECTURE.md](docs/ARCHITECTURE.md) | [System Design](docs/ARCHITECTURE.md#system-design)

---

## ğŸ”§ Supported Hardware

Run anywhere - from powerful servers to edge devices and browsers:

| Platform | Status | Acceleration | Requirements | Performance |
|----------|--------|--------------|--------------|-------------|
| **CPU** (x86/ARM) | âœ… | SIMD, AVX | Any | Good |
| **NVIDIA CUDA** | âœ… | GPU + TensorRT | CUDA 11.8+ | Excellent |
| **AMD ROCm** | âœ… | GPU + HIP | ROCm 5.0+ | Excellent |
| **Apple MPS** | âœ… | Metal | M1/M2/M3 | Excellent |
| **Intel OpenVINO** | âœ… | CPU/GPU | Intel HW | Very Good |
| **WebNN** | âœ… | Browser NPU | Chrome, Edge | Good |
| **WebGPU** | âœ… | Browser GPU | Modern browsers | Very Good |
| **Qualcomm** | âœ… | Mobile DSP | Snapdragon | Good |

### Hardware Selection

The framework **automatically detects and selects** the best available hardware:

```python
# Automatic (recommended)
accelerator = IPFSAccelerator()  # Uses best available

# Manual selection
accelerator = IPFSAccelerator(device="cuda")  # Force CUDA
accelerator = IPFSAccelerator(device="mps")   # Force Apple MPS
```

âš™ï¸ **Hardware guides**: [Hardware Optimization](docs/HARDWARE.md) | [Platform-Specific](docs/HARDWARE.md#platform-guides)

---

## ğŸ¤– Supported Models

### Pre-trained Models (300+)

| Category | Models | Status |
|----------|--------|--------|
| **Text** | BERT, RoBERTa, DistilBERT, ALBERT, GPT-2/Neo/J, T5, BART, Pegasus, Sentence Transformers | âœ… |
| **Vision** | ViT, DeiT, BEiT, ResNet, EfficientNet, DETR, YOLO | âœ… |
| **Audio** | Whisper, Wav2Vec2, WavLM, Audio Transformers | âœ… |
| **Multimodal** | CLIP, BLIP, LLaVA | âœ… |
| **Custom** | PyTorch models, ONNX, TensorFlow (converted) | âœ… |

### Model Loading

```python
# From HuggingFace Hub
model = accelerator.load_model("bert-base-uncased")

# From IPFS (content-addressed)
model = accelerator.load_model("ipfs://QmXxxx...")

# Local model
model = accelerator.load_model("./my_model/")

# With specific hardware
model = accelerator.load_model("gpt2", device="cuda")
```

ğŸ¤– **Full model list**: [Supported Models](docs/README.md#model-support) | [Custom Models Guide](docs/USAGE.md#custom-models)

---

## ğŸ“š Documentation

### ğŸ“– Essential Guides

| Guide | Description | Audience |
|-------|-------------|----------|
| [**Getting Started**](docs/GETTING_STARTED.md) | Complete beginner tutorial | Everyone |
| [**Quick Start**](docs/guides/QUICKSTART.md) | Get running in 5 minutes | Everyone |
| [**Installation**](docs/guides/INSTALL.md) | Detailed setup instructions | Users |
| [**FAQ**](docs/FAQ.md) | Common questions & answers | Everyone |
| [**API Reference**](docs/API.md) | Complete API documentation | Developers |
| [**Architecture**](docs/ARCHITECTURE.md) | System design & components | Architects |
| [**Hardware Optimization**](docs/HARDWARE.md) | Platform-specific tuning | Engineers |
| [**Testing Guide**](docs/TESTING.md) | Testing & benchmarking | QA/DevOps |

### ğŸ¯ Specialized Topics

| Topic | Resources |
|-------|-----------|
| **IPFS & P2P** | [IPFS Integration](docs/IPFS.md) â€¢ [P2P Networking](docs/guides/p2p/) |
| **GitHub Actions** | [Autoscaler](docs/architecture/AUTOSCALER.md) â€¢ [CI/CD](docs/guides/github/) |
| **Docker & K8s** | [Container Guide](docs/guides/docker/) â€¢ [Deployment](docs/guides/deployment/) |
| **MCP Server** | [MCP Setup](docs/guides/MCP_SETUP_GUIDE.md) â€¢ [Protocol Docs](docs/P2P_AND_MCP.md) |
| **Browser Support** | [WebNN/WebGPU](docs/WEBNN_WEBGPU_README.md) â€¢ [Examples](examples/webnn_demo.py) |

### ğŸ“Š Documentation Quality

Our documentation has been **professionally audited** (January 2026):
- âœ… **200+ files** covering all features
- âœ… **93/100 quality score** (Excellent)
- âœ… **Comprehensive** - From beginner to expert
- âœ… **Well-organized** - Clear structure and navigation
- âœ… **Verified** - All examples tested and working

ğŸ“‹ **Documentation Hub**: [docs/](docs/) | [Full Index](docs/INDEX.md) | [Audit Report](docs/DOCUMENTATION_AUDIT_REPORT.md)

---

## ğŸŒ IPFS & Distributed Features

### Why IPFS?

IPFS integration provides **enterprise-grade distributed computing**:

- ğŸ” **Content Addressing** - Cryptographically secure, immutable model distribution
- ğŸŒ **Global Network** - Automatic peer discovery and geographic optimization
- âš¡ **Intelligent Caching** - Multi-level LRU caching across the network
- ğŸ”„ **Load Balancing** - Automatic distribution across available peers
- ğŸ›¡ï¸ **Fault Tolerance** - Robust error handling and fallback mechanisms

### Distributed Inference

```python
# Enable P2P inference
accelerator = IPFSAccelerator(enable_p2p=True)

# Model is automatically shared across peers
model = accelerator.load_model("bert-base-uncased")

# Inference uses best available peer
result = model.inference("Distributed AI!")
```

### Advanced Features

| Feature | Description | Status |
|---------|-------------|--------|
| **P2P Workflow Scheduler** | Distributed task execution with merkle clocks | âœ… |
| **GitHub Actions Cache** | Distributed cache for CI/CD | âœ… |
| **Autoscaler** | Dynamic runner provisioning | âœ… |
| **MCP Server** | Model Context Protocol (14+ tools) | âœ… |

ğŸŒ **Learn more**: [IPFS Guide](docs/IPFS.md) | [P2P Architecture](docs/P2P_AND_MCP.md) | [Network Setup](docs/guides/p2p/)

---

## ğŸ§ª Testing & Quality

```bash
# Run all tests
pytest

# Run specific test suite
pytest test/test_inference.py

# Run with coverage report
pytest --cov=ipfs_accelerate_py --cov-report=html

# Run benchmarks
python data/benchmarks/run_benchmarks.py
```

### Quality Metrics

| Metric | Status | Details |
|--------|--------|---------|
| **Test Coverage** | âœ… | Comprehensive test suite |
| **Documentation** | âœ… 93/100 | [Audit Report](docs/DOCUMENTATION_AUDIT_REPORT.md) |
| **Code Quality** | âœ… | Linted, type-checked |
| **Security** | âœ… | Regular vulnerability scans |
| **Performance** | âœ… | Benchmarked across platforms |

ğŸ§ª **Testing guide**: [docs/TESTING.md](docs/TESTING.md) | [CI/CD Setup](docs/guides/github/)

---

## âš¡ Performance & Optimization

### Benchmarks

| Hardware | Model | Throughput | Latency |
|----------|-------|------------|---------|
| **NVIDIA RTX 3090** | BERT-base | ~2000 samples/sec | <1ms |
| **Apple M2 Max** | BERT-base | ~800 samples/sec | 2-3ms |
| **Intel i9 (CPU)** | BERT-base | ~100 samples/sec | 10-15ms |
| **WebGPU (Browser)** | BERT-base | ~50 samples/sec | 20-30ms |

### Optimization Tips

```python
# Enable mixed precision for 2x speedup
accelerator = IPFSAccelerator(precision="fp16")

# Use batch processing for better throughput
results = model.batch_inference(inputs, batch_size=32)

# Enable model quantization for 4x memory reduction
model = accelerator.load_model("bert-base-uncased", quantize=True)

# Use intelligent caching for repeated queries
accelerator = IPFSAccelerator(enable_cache=True)
```

ğŸ“Š **Performance guide**: [Hardware Optimization](docs/HARDWARE.md) | [Benchmarking](docs/TESTING.md#benchmarks)

---

## ğŸ”§ Troubleshooting

### Common Issues

| Issue | Solution |
|-------|----------|
| **Import errors** | `pip install --upgrade ipfs-accelerate-py` |
| **CUDA not found** | Install [CUDA Toolkit 11.8+](https://developer.nvidia.com/cuda-downloads) |
| **Slow inference** | Check hardware selection, enable caching |
| **Memory errors** | Use quantization, reduce batch size |
| **Connection issues** | Check IPFS daemon, firewall settings |

### Quick Fixes

```bash
# Verify installation
python -c "import ipfs_accelerate_py; print(ipfs_accelerate_py.__version__)"

# Check hardware detection
ipfs-accelerate hardware status

# Test basic inference
ipfs-accelerate inference test

# View logs
ipfs-accelerate logs --tail 100
```

ğŸ†˜ **Get help**: [Troubleshooting Guide](docs/INSTALLATION_TROUBLESHOOTING_GUIDE.md) | [FAQ](docs/FAQ.md) | [GitHub Issues](https://github.com/endomorphosis/ipfs_accelerate_py/issues)

---

## ğŸ¤ Contributing

We **welcome contributions**! Here's how to get started:

### Quick Contribution Guide

1. **Fork & Clone**: Get your own copy of the repository
2. **Create Branch**: `git checkout -b feature/your-feature`
3. **Make Changes**: Follow our [coding standards](CONTRIBUTING.md)
4. **Run Tests**: `pytest` to ensure everything works
5. **Submit PR**: Open a pull request with clear description

### Areas We Need Help

- ğŸ› **Bug Reports** - Found an issue? Let us know!
- ğŸ“š **Documentation** - Help improve guides and examples
- ğŸ§ª **Testing** - Add tests for edge cases
- ğŸŒ **Translations** - Translate docs to other languages
- ğŸ’¡ **Features** - Suggest or implement new features

### Community & Guidelines

- ğŸ’¬ **[GitHub Discussions](https://github.com/endomorphosis/ipfs_accelerate_py/discussions)** - Ask questions, share ideas
- ğŸ› **[Issue Tracker](https://github.com/endomorphosis/ipfs_accelerate_py/issues)** - Report bugs, request features
- ğŸ” **[Security Policy](SECURITY.md)** - Report security vulnerabilities
- ğŸ“§ **Email**: starworks5@gmail.com

ğŸ“– **Full guides**: [CONTRIBUTING.md](CONTRIBUTING.md) | [Code of Conduct](CONTRIBUTING.md#community-guidelines) | [Security Policy](SECURITY.md)

---

## ğŸ“„ License

This project is licensed under the **GNU Affero General Public License v3.0 or later (AGPLv3+)**.

**What this means**:
- âœ… Free to use, modify, and distribute
- âœ… Commercial use allowed
- âœ… Patent protection included
- âš ï¸ Source code must be disclosed for network services
- âš ï¸ Modifications must use same license

ğŸ“‹ **Details**: [LICENSE](LICENSE) | [AGPL FAQ](https://www.gnu.org/licenses/gpl-faq.html)

---

## ğŸ™ Acknowledgments

Built with amazing open source technologies:

- [**HuggingFace Transformers**](https://huggingface.co/transformers/) - ML model ecosystem
- [**IPFS**](https://ipfs.io/) - Distributed file system
- [**PyTorch**](https://pytorch.org/) - Deep learning framework
- [**FastAPI**](https://fastapi.tiangolo.com/) - Modern web framework

Special thanks to all [contributors](https://github.com/endomorphosis/ipfs_accelerate_py/graphs/contributors) who make this project possible! ğŸŒŸ

### Project Information

- ğŸ“‹ **[Changelog](CHANGELOG.md)** - Version history and release notes
- ğŸ” **[Security Policy](SECURITY.md)** - Security reporting and best practices
- ğŸ¤ **[Contributing Guide](CONTRIBUTING.md)** - How to contribute
- ğŸ“„ **[License](LICENSE)** - AGPLv3+ license details

---

## ğŸŒŸ Show Your Support

If you find this project useful:

- â­ **Star this repository** on GitHub
- ğŸ“¢ **Share** with your network
- ğŸ› **Report issues** to help improve it
- ğŸ’¡ **Contribute** features or fixes
- ğŸ“ **Write** about your experience

---

<div align="center">

**Made with â¤ï¸ by [Benjamin Barber](https://github.com/endomorphosis) and [contributors](https://github.com/endomorphosis/ipfs_accelerate_py/graphs/contributors)**

[ğŸ  Homepage](https://github.com/endomorphosis/ipfs_accelerate_py) â€¢ 
[ğŸ“š Documentation](docs/) â€¢ 
[ğŸ› Issues](https://github.com/endomorphosis/ipfs_accelerate_py/issues) â€¢ 
[ğŸ’¬ Discussions](https://github.com/endomorphosis/ipfs_accelerate_py/discussions)

</div>
