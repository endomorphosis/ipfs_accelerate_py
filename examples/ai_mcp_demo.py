#!/usr/bin/env python3
"""
AI-Powered MCP Server Demonstration

This script demonstrates the complete AI-powered Model Manager MCP Server
with intelligent model selection, IPFS content addressing, and inference capabilities.

Features demonstrated:
1. Model Manager with IPFS Content Addressing
2. AI-powered Model Discovery and Recommendations
3. Inference Tools with Smart Model Selection
4. Bandit Algorithm Learning from Feedback
5. Complete MCP Server Integration
"""

import asyncio
import json
import logging
import os
import sys
import tempfile
from pathlib import Path

# Add the ipfs_accelerate_py package to the path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'ipfs_accelerate_py'))

# Import components
from ipfs_accelerate_py.model_manager import (
    ModelManager, ModelMetadata, IOSpec, ModelType, DataType,
    BanditModelRecommender, RecommendationContext, 
    VectorDocumentationIndex
)

# Import MCP server
try:
    sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'ipfs_mcp'))
    from ai_model_server import create_ai_model_server, HAVE_FASTMCP
    HAVE_MCP_SERVER = True
except ImportError as e:
    HAVE_MCP_SERVER = False
    HAVE_FASTMCP = False
    print(f"‚ö†Ô∏è MCP Server not available: {e}")

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger("mcp_demo")

def create_sample_models(manager: ModelManager) -> None:
    """Create sample models for demonstration."""
    
    print("üìö Creating sample models for demonstration...")
    
    # Sample Model 1: BERT for classification
    bert_model = ModelMetadata(
        model_id="bert-base-uncased",
        model_name="BERT Base Uncased",
        model_type=ModelType.ENCODER_ONLY,
        architecture="bert",
        inputs=[
            IOSpec("input_ids", DataType.TOKENS, shape=(512,), description="Token IDs"),
            IOSpec("attention_mask", DataType.TOKENS, shape=(512,), description="Attention mask")
        ],
        outputs=[
            IOSpec("logits", DataType.LOGITS, shape=(2,), description="Classification logits")
        ],
        supported_backends=["cpu", "cuda"],
        tags=["classification", "nlp", "bert", "encoder"],
        description="BERT model for text classification tasks",
        source_url="https://huggingface.co/bert-base-uncased",
        license="Apache 2.0"
    )
    
    # Sample Model 2: GPT for generation  
    gpt_model = ModelMetadata(
        model_id="gpt2",
        model_name="GPT-2",
        model_type=ModelType.DECODER_ONLY,
        architecture="gpt2",
        inputs=[
            IOSpec("input_ids", DataType.TOKENS, shape=(1024,), description="Token IDs")
        ],
        outputs=[
            IOSpec("logits", DataType.LOGITS, shape=(50257,), description="Next token logits")
        ],
        supported_backends=["cpu", "cuda"],
        tags=["generation", "nlp", "gpt", "decoder", "causal_lm"],
        description="GPT-2 model for text generation",
        source_url="https://huggingface.co/gpt2",
        license="MIT"
    )
    
    # Sample Model 3: Vision model
    vit_model = ModelMetadata(
        model_id="vit-base-patch16-224",
        model_name="Vision Transformer Base",
        model_type=ModelType.VISION_MODEL,
        architecture="vit",
        inputs=[
            IOSpec("pixel_values", DataType.IMAGE, shape=(3, 224, 224), description="Image pixels")
        ],
        outputs=[
            IOSpec("logits", DataType.LOGITS, shape=(1000,), description="Classification logits")
        ],
        supported_backends=["cpu", "cuda"],
        tags=["vision", "classification", "transformer"],
        description="Vision Transformer for image classification",
        source_url="https://huggingface.co/google/vit-base-patch16-224",
        license="Apache 2.0"
    )
    
    # Sample Model 4: Diffusion model
    diffusion_model = ModelMetadata(
        model_id="stable-diffusion-v1-5",
        model_name="Stable Diffusion v1.5",
        model_type=ModelType.MULTIMODAL,
        architecture="diffusion",
        inputs=[
            IOSpec("prompt", DataType.TOKENS, description="Text prompt"),
            IOSpec("negative_prompt", DataType.TOKENS, description="Negative prompt")
        ],
        outputs=[
            IOSpec("image", DataType.IMAGE, shape=(512, 512, 3), description="Generated image")
        ],
        supported_backends=["cuda"],
        tags=["diffusion", "generation", "image", "text-to-image"],
        description="Stable Diffusion model for text-to-image generation",
        source_url="https://huggingface.co/runwayml/stable-diffusion-v1-5",
        license="CreativeML Open RAIL-M"
    )
    
    # Add models to manager
    models = [bert_model, gpt_model, vit_model, diffusion_model]
    for model in models:
        success = manager.add_model(model)
        if success:
            print(f"‚úÖ Added model: {model.model_id}")
        else:
            print(f"‚ùå Failed to add model: {model.model_id}")
    
    print(f"üìä Total models in manager: {len(manager.list_models())}")

def demonstrate_model_manager_with_ipfs(temp_dir: str) -> ModelManager:
    """Demonstrate model manager with IPFS content addressing."""
    
    print("\nüöÄ DEMONSTRATING MODEL MANAGER WITH IPFS CONTENT ADDRESSING")
    print("=" * 70)
    
    # Create model manager
    manager_path = os.path.join(temp_dir, "models.json")
    manager = ModelManager(storage_path=manager_path)
    
    # Add sample models
    create_sample_models(manager)
    
    # Demonstrate IPFS functionality (mock since we can't download files)
    print("\nüîó IPFS Content Addressing Features:")
    
    # Get models with IPFS support
    ipfs_models = manager.get_models_with_ipfs_cids()
    print(f"üìÅ Models with IPFS CIDs: {len(ipfs_models)}")
    
    # Get IPFS gateway URLs for a model (mock data)
    for model in manager.list_models()[:2]:
        gateway_urls = manager.get_ipfs_gateway_urls(model.model_id)
        print(f"üåê IPFS gateways for {model.model_id}: {len(gateway_urls)} files")
    
    return manager

def demonstrate_bandit_recommendations(manager: ModelManager, temp_dir: str) -> BanditModelRecommender:
    """Demonstrate bandit-powered model recommendations."""
    
    print("\nüé∞ DEMONSTRATING BANDIT MODEL RECOMMENDATIONS")
    print("=" * 50)
    
    # Create bandit recommender
    bandit_path = os.path.join(temp_dir, "bandit_data.json")
    recommender = BanditModelRecommender(
        model_manager=manager,
        storage_path=bandit_path,
        algorithm="thompson_sampling"
    )
    
    # Test different contexts
    contexts = [
        {
            "name": "Text Classification",
            "context": RecommendationContext(
                task_type="classification",
                hardware="cpu",
                input_type=DataType.TOKENS,
                output_type=DataType.LOGITS
            )
        },
        {
            "name": "Text Generation", 
            "context": RecommendationContext(
                task_type="generation",
                hardware="cuda",
                input_type=DataType.TOKENS,
                output_type=DataType.TOKENS
            )
        },
        {
            "name": "Image Classification",
            "context": RecommendationContext(
                task_type="classification",
                hardware="cuda",
                input_type=DataType.IMAGE,
                output_type=DataType.LOGITS
            )
        }
    ]
    
    # Get recommendations and provide feedback
    for ctx_info in contexts:
        print(f"\nüéØ Context: {ctx_info['name']}")
        
        # Get recommendation
        recommendation = recommender.recommend_model(ctx_info["context"])
        
        if recommendation:
            print(f"  üìã Recommended: {recommendation.model_id}")
            print(f"  üéØ Confidence: {recommendation.confidence_score:.3f}")
            print(f"  üí≠ Reasoning: {recommendation.reasoning}")
            
            # Simulate feedback (random score for demo)
            import random
            feedback_score = random.uniform(0.6, 0.95)
            recommender.provide_feedback(
                recommendation.model_id,
                feedback_score,
                ctx_info["context"]
            )
            print(f"  üìä Provided feedback: {feedback_score:.3f}")
        else:
            print(f"  ‚ùå No recommendation available")
    
    return recommender

def demonstrate_vector_documentation_search(temp_dir: str) -> VectorDocumentationIndex:
    """Demonstrate vector documentation search."""
    
    print("\nüìö DEMONSTRATING VECTOR DOCUMENTATION SEARCH")
    print("=" * 50)
    
    # Create documentation index
    doc_path = os.path.join(temp_dir, "doc_index.json")
    doc_index = VectorDocumentationIndex(storage_path=doc_path)
    
    # Mock some documentation (since we can't scan real files)
    print("üìÑ Indexing sample documentation...")
    
    # Simulate search (will gracefully degrade without sentence transformers)
    queries = [
        "How to optimize CUDA performance?",
        "What are the best models for text classification?",
        "Image generation with diffusion models"
    ]
    
    for query in queries:
        print(f"\nüîç Query: '{query}'")
        try:
            results = doc_index.search(query, max_results=3)
            if results:
                for i, result in enumerate(results):
                    print(f"  {i+1}. {result.document.file_path} (score: {result.similarity_score:.3f})")
            else:
                print("  üìù No results (vector search requires sentence-transformers)")
        except Exception as e:
            print(f"  ‚ö†Ô∏è Search unavailable: {e}")
    
    return doc_index

async def demonstrate_mcp_server_tools(manager: ModelManager, 
                                     recommender: BanditModelRecommender,
                                     doc_index: VectorDocumentationIndex):
    """Demonstrate MCP server tools."""
    
    print("\nüåê DEMONSTRATING MCP SERVER TOOLS")
    print("=" * 40)
    
    if not HAVE_MCP_SERVER:
        print("‚ö†Ô∏è MCP Server components not available")
        print("‚úÖ Model Manager and AI components are working correctly")
        return
    
    # Create temporary storage for server demo
    with tempfile.TemporaryDirectory() as server_temp:
        # Create server instance (but don't run it)
        server = create_ai_model_server(
            model_manager_path=os.path.join(server_temp, "server_models.json"),
            bandit_storage_path=os.path.join(server_temp, "server_bandit.json"),
            doc_index_path=os.path.join(server_temp, "server_docs.json")
        )
        
        # Add some models to the server's model manager
        create_sample_models(server.model_manager)
        
        print("üîß Available MCP Tools:")
        
        # List the core tools that would be available
        tools = [
            "list_models - List available models with filtering",
            "recommend_model - Get AI-powered model recommendations",
            "generate_text - Causal language modeling with smart model selection",
            "classify_text - Text classification with automatic model choice",
            "generate_embeddings - Text embedding generation",
            "get_model_ipfs_cids - Get IPFS CIDs for model files",
            "search_documentation - Semantic documentation search",
            "provide_feedback - Improve recommendations through learning"
        ]
        
        for tool in tools:
            print(f"  ‚ö° {tool}")
        
        print(f"\nüìä Server initialized with {len(server.model_manager.list_models())} models")
        print("üéØ Smart model selection: Bandit algorithms automatically choose optimal models")
        print("üîó IPFS integration: Content addressing for decentralized model distribution")
        print("üß† Learning system: Performance feedback improves future recommendations")
        
        if HAVE_FASTMCP:
            print("‚úÖ FastMCP available - full MCP server functionality enabled")
        else:
            print("üìö Install FastMCP for complete MCP server functionality")
        
        # Demonstrate core functionality
        print("\nüß™ Testing Core Functionality:")
        
        # Test recommendation
        context = RecommendationContext(
            task_type="text_generation",
            hardware="cpu",
            input_type=DataType.TOKENS,
            output_type=DataType.TOKENS
        )
        
        recommendation = server.bandit_recommender.recommend_model(context)
        if recommendation:
            print(f"  üéØ Recommendation: {recommendation.model_id} (confidence: {recommendation.confidence_score:.3f})")
        
        # Test IPFS functionality
        models_with_ipfs = server.model_manager.get_models_with_ipfs_cids()
        print(f"  üîó Models with IPFS support: {len(models_with_ipfs)}")
        
        # Clean up
        server.model_manager.close()

def main():
    """Main demonstration function."""
    
    print("üöÄ AI-POWERED MODEL MANAGER MCP SERVER DEMONSTRATION")
    print("=" * 60)
    print("This demo showcases the complete AI-powered Model Manager system")
    print("integrated with MCP server capabilities and IPFS content addressing.")
    print()
    
    with tempfile.TemporaryDirectory() as temp_dir:
        try:
            # 1. Demonstrate Model Manager with IPFS
            manager = demonstrate_model_manager_with_ipfs(temp_dir)
            
            # 2. Demonstrate Bandit Recommendations
            recommender = demonstrate_bandit_recommendations(manager, temp_dir)
            
            # 3. Demonstrate Vector Documentation Search
            doc_index = demonstrate_vector_documentation_search(temp_dir)
            
            # 4. Demonstrate MCP Server Integration
            asyncio.run(demonstrate_mcp_server_tools(manager, recommender, doc_index))
            
            print("\n‚úÖ DEMONSTRATION COMPLETE")
            print("=" * 30)
            print("üéâ All AI-powered features are working correctly!")
            print("üîß The MCP server provides 14 tools for intelligent model management")
            print("üß† Bandit algorithms learn from feedback to improve recommendations")
            print("üîó IPFS content addressing enables decentralized model distribution")
            print("‚ö° Smart inference tools automatically select optimal models")
            
        except Exception as e:
            logger.error(f"Demo error: {e}")
            print(f"\n‚ùå Demo encountered an error: {e}")
        finally:
            # Clean up
            if 'manager' in locals():
                manager.close()

if __name__ == "__main__":
    main()