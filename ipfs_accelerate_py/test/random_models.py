from transformers import AutoConfig
import time
import json
import os
# The curated list of 256 unique Hugging Face model IDs (with "opus-mt" models removed).
model_list = [
    "huggingface/bert-base-uncased",
    "huggingface/bert-large-uncased",
    "huggingface/bert-base-cased",
    "huggingface/bert-large-cased",
    "dbmdz/bert-base-german-cased",
    "dbmdz/bert-base-german-dbmdz-cased",
    "dbmdz/bert-base-german-uncased",
    "TurkuNLP/bert-base-finnish-cased-v1",
    "wietsedv/bert-base-dutch-cased",
    "distilbert/distilbert-base-uncased",
    "distilbert/distilbert-base-cased",
    "distilbert/distilbert-base-uncased-distilled-squad",
    "distilbert/distilbert-base-cased-distilled-squad",
    "distilbert/distilroberta-base",
    "facebook/roberta-base",
    "facebook/roberta-large",
    "ynie/roberta-large-mnli",
    "openai/roberta-base-openai-detector",
    "cardiffnlp/twitter-roberta-base",
    "cardiffnlp/twitter-roberta-base-sentiment",
    "openai/gpt2",
    "openai/gpt2-medium",
    "openai/gpt2-large",
    "openai/gpt2-xl",
    "EleutherAI/gpt-neo-125M",
    "EleutherAI/gpt-neo-1.3B",
    "EleutherAI/gpt-neo-2.7B",
    "EleutherAI/gpt-j-6B",
    "facebook/bart-base",
    "facebook/bart-large",
    "facebook/bart-large-mnli",
    "sshleifer/bart-tiny-random",
    "sshleifer/bart-base",
    "huggingface/t5-small",
    "huggingface/t5-base",
    "huggingface/t5-large",
    "huggingface/t5-3b",
    "google/mt5-small",
    "google/mt5-base",
    "google/mt5-large",
    "google/mt5-xl",
    "facebook/mbart-large-50",
    "google/electra-small-discriminator",
    "google/electra-base-discriminator",
    "google/electra-large-discriminator",
    "huggingface/xlnet-base-cased",
    "huggingface/xlnet-large-cased",
    "allenai/longformer-base-4096",
    "allenai/longformer-large-4096",
    "microsoft/deberta-v3-base",
    "microsoft/deberta-v3-large",
    "microsoft/DialoGPT-small",
    "microsoft/DialoGPT-medium",
    "microsoft/DialoGPT-large",
    "facebook/dpr-question_encoder-single-nq-base",
    "facebook/dpr-ctx_encoder-single-nq-base",
    "facebook/dpr-reader-single-nq-base",
    "nlpaueb/legal-bert-base-uncased",
    "microsoft/codebert-base",
    "deepset/roberta-base-squad2",
    "deepset/roberta-large-squad2",
    "google/bert_uncased_L-12_H-768_A-12",
    "google/bert_uncased_L-24_H-1024_A-16",
    "google/mobilebert-uncased",
    "microsoft/Multilingual-MiniLM-L12-H384",
    "sentence-transformers/all-mpnet-base-v2",
    "sentence-transformers/all-MiniLM-L6-v2",
    "sentence-transformers/all-distilroberta-v1",
    "sentence-transformers/bert-base-nli-mean-tokens",
    "sentence-transformers/paraphrase-MiniLM-L6-v2",
    "jplu/xlm-roberta-base",
    "jplu/xlm-roberta-large",
    "camembert/camembert-base",
    "flaubert/flaubert_small_cased",
    "flaubert/flaubert_base_uncased",
    "flaubert/flaubert_large_uncased",
    "distilbert/distilbert-base-multilingual-cased",
    "facebook/xlm-mlm-100-1280",
    "facebook/mbart-large-50-many-to-many-mmt",
    "Salesforce/codegen-350M-mono",
    "Salesforce/codegen-2B-mono",
    "sentence-transformers/paraphrase-distilroberta-base-v1",
    "sentence-transformers/paraphrase-xlm-r-multilingual-v1",
    "sentence-transformers/all-MiniLM-L12-v2",
    "sentence-transformers/multi-qa-mpnet-base-cos-v1",
    "albert/albert-base-v2",
    "albert/albert-large-v2",
    "albert/albert-xlarge-v2",
    "albert/albert-xxlarge-v2",
    "facebook/opt-125m",
    "facebook/opt-350m",
    "facebook/opt-1.3b",
    "facebook/opt-2.7b",
    "facebook/opt-6.7b",
    "facebook/wav2vec2-base-960h",
    "facebook/wav2vec2-large-960h-lv60",
    "openai/clip-vit-base-patch32",
    "openai/clip-vit-large-patch14",
    "sshleifer/distilbart-cnn-12-6",
    "sshleifer/bart-large-cnn",
    "google/flan-t5-small",
    "google/flan-t5-base",
    "google/flan-t5-large",
    "google/flan-t5-xl",
    "facebook/mbart-large-50-many-to-one-mmt",
    "csebuetnlp/mT5_multilingual_XLSum",
    "microsoft/CodeGPT-small-py",
    "textattack/bert-base-uncased-imdb",
    "textattack/bert-base-uncased-ag-news",
    "textattack/bert-base-uncased-yelp-polarity",
    "textattack/bert-base-uncased-rotten-tomatoes",
    "nlptown/bert-base-multilingual-uncased-sentiment",
    "finiteautomata/bertweet-base-sentiment-analysis",
    "valhalla/bert-base-uncased-finetuned-sst-2-english",
    "sshleifer/distilbart-xsum-12-6",
    "facebook/blenderbot-400M-distill",
    "facebook/blenderbot-90M",
    "facebook/blenderbot-3B",
    "sentence-transformers/distilbert-base-nli-stsb-mean-tokens",
    "nlpaueb/legal-bert-small-uncased",
    "emilyalsentzer/Bio_ClinicalBERT",
    "google/pegasus-xsum",
    "google/pegasus-large",
    "allenai/led-base-16384",
    "allenai/led-large-16384",
    "microsoft/xtremedistil-l6-h384-uncased",
    "cardiffnlp/twitter-roberta-base-emotion",
    "cardiffnlp/twitter-roberta-base-hate",
    "dbmdz/bert-large-cased-finetuned-conll03-english",
    "neuralmind/bert-base-portuguese-cased",
    "pierreguillou/bert-base-cased-squad-v1",
    "ProsusAI/finbert",
    "allegro/herbert-klej-cased",
    "allegro/herbert-base-cased",
    "TurkuNLP/bert-base-finnish-uncased",
    "indobenchmark/indobert-base-p1",
    "indolem/indobert-base-uncased",
    "HooshvareLab/bert-fa-base-uncased",
    "HooshvareLab/bert-fa-base-uncased-clf",
    "asafaya/bert-base-arabic",
    "asafaya/bert-base-arabic-sentiment",
    "hfl/chinese-bert-wwm-ext",
    "hfl/chinese-roberta-wwm-ext",
    "DeepPavlov/rubert-base-cased",
    "dccuchile/bert-base-spanish-wwm-cased",
    "dccuchile/bert-base-spanish-wwm-uncased",
    "dbmdz/bert-base-italian-cased",
    "ai4bharat/indic-bert",
    "dbmdz/bert-base-turkish-cased",
    "vinai/bertpho-cased",
    "Vamsi/T5_Paraphrase_Paws",
    "ramsrigouthamg/t5_paraphraser",
    "mrm8488/t5-base-finetuned-common_gen",
    "Salesforce/codet5-small",
    "Salesforce/codet5-base",
    "Salesforce/codet5-large",
    "j-hartmann/emotion-english-distilroberta-base",
    "bhadresh-savani/bert-base-uncased-emotion",
    "unitary/toxic-bert",
    "mrm8488/distilbert-base-uncased-finetuned-emotion",
    "lvwerra/distilbert-imdb",
    "facebook/bart-large-xsum",
    "google/pegasus-cnn_dailymail",
    "pszemraj/t5-small-finetuned-summarize-news",
    "pszemraj/t5-base-finetuned-summarize-news",
    "pszemraj/t5-large-finetuned-summarize-news",
    "valhalla/t5-small-qa-qg-hl",
    "valhalla/t5-base-qa-qg-hl",
    "valhalla/t5-large-qa-qg-hl",
    "google/pegasus-newsroom",
    "facebook/bart-base-samsum",
    "patrickvonplaten/t5-tiny-random",
    "sentence-transformers/roberta-large-nli-stsb-mean-tokens",
    "mrm8488/bert-tiny-finetuned-sst2",
    "papluca/xlm-roberta-base-language-detection",
    "microsoft/CodeBERTa-small-v1",
    "microsoft/mdeberta-v3-base",
    "csebuetnlp/mT5_small_MNLI",
    "bigscience/bloom-560m",
    "bigscience/bloom-1b1",
    "bigscience/bloom-1b7",
    "bigscience/bloom-3b",
    "bigscience/bloom-7b1",
    "cardiffnlp/twitter-xlm-roberta-base-sentiment",
    "mrm8488/distilroberta-base-finetuned-financial-news-sentiment",
    "j-hartmann/emotion-multi-class-multilingual-roberta",
    "flax-community/longformer-large-4096-finetuned-squad2",
    "cahya/bert-base-indonesian-1.5G",
    "ktrapeznikov/albert-xlarge-v2-squad-v2",
    "ynie/roberta-large-snli_mnli_fever_anli_R1_R2_R3-nli",
    "google/t5-v1_1-small",
    "google/t5-v1_1-base",
    "google/t5-v1_1-large",
    "google/t5-v1_1-xl",
    "cardiffnlp/twitter-roberta-base-offensive",
    "deepset/minilm-uncased-squad2",
    "vblagoje/bert-english-uncased-finetuned-squad",
    "microsoft/graphcodebert-base",
    "microsoft/graphcodebert-large",
    "philschmid/bart-large-cnn-samsum",
    "indobenchmark/indobert-base-p1-squad2",
    "flax-community/led-base-16384-finetuned-summarization",
    "openai/roberta-large-openai-detector",
    "google/vit-base-patch16-224",
    "google/vit-base-patch16-384",
    "google/vit-large-patch16-224",
    "google/vit-large-patch16-384",
    "facebook/deit-base-distilled-patch16-224",
    "facebook/deit-base-patch16-224",
    "facebook/deit-small-patch16-224",
    "facebook/detr-resnet-50",
    "facebook/detr-resnet-101",
    "microsoft/swin-tiny-patch4-window7-224",
    "microsoft/swin-small-patch4-window7-224",
    "microsoft/swin-base-patch4-window7-224",
    "microsoft/swin-large-patch4-window7-224",
    "facebook/convnext-base-224",
    "facebook/convnext-small-224",
    "facebook/convnext-tiny-224",
    "huggingface/beit-base-patch16-224",
    "huggingface/beit-large-patch16-224",
    "huggingface/convnext-base",
    "huggingface/convnext-small",
    "huggingface/convnext-tiny",
    "jonatasgrosman/wav2vec2-large-xlsr-53",
    "facebook/hubert-large-ls960",
    "facebook/hubert-base-ls960",
    "microsoft/CodeGPT-small-java",
    "google/bert-base-multilingual-cased",
    "allenai/scibert_scivocab_uncased",
    "allenai/specter",
    "rinna/japanese-gpt2-small",
    "rinna/japanese-gpt2-medium",
    "rinna/japanese-gpt2-large",
    "rinna/japanese-gpt2-xl",
    "flaubert/flaubert_large_cased",
    "dmis-lab/biobert-v1.1",
    "squeezebert/squeezebert-uncased",
    "prajjwal1/bert-tiny",
    "microsoft/layoutlm-base-uncased",
    "microsoft/layoutlm-large-uncased",
    "cl-tohoku/bert-base-japanese",
    "cl-tohoku/bert-base-japanese-char",
    "monologg/koelectra-base-v3-discriminator",
    "monologg/koelectra-small-v3-discriminator",
    "monologg/koelectra-base-v3-generator",
    "huggingface/distilgpt2",
    "google/electra-small-generator",
    "google/electra-base-generator",
    "google/electra-large-generator",
    "superb/wav2vec2-base-superb",
    "superb/hubert-base-superb",
    "allenai/biomed_roberta_base",
    "microsoft/deberta-base",
    "openai/whisper-small",
    "openai/whisper-tiny",
    "google-t5/t5-base",
    "BAAI/bge-small-en-v1.5",
    "laion/larger_clap_general",
    "facebook/wav2vec2-large-960h-lv60-self",
    "openai/clip-vit-base-patch16",
    "openai/whisper-large-v3-turbo",
    "meta-llama/Meta-Llama-3.1-8B-Instruct",
    "distil-whisper/distil-large-v3",
    "Qwen/Qwen2-7B",
    "llava-hf/llava-interleave-qwen-0.5b-hf",
    "lmms-lab/LLaVA-Video-7B-Qwen2",
    "llava-hf/llava-v1.6-mistral-7b-hf",
    "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
    "TIGER-Lab/Mantis-8B-siglip-llama3",
    "microsoft/xclip-base-patch16-zero-shot",
    "google/vit-base-patch16-224",
    "MCG-NJU/videomae-base",
    "MCG-NJU/videomae-large",
    "laion/CLIP-ViT-H-14-laion2B-s32B-b79K",
    "lmms-lab/llava-onevision-qwen2-7b-si",
    "lmms-lab/llava-onevision-qwen2-7b-ov",
    "lmms-lab/llava-onevision-qwen2-0.5b-si",
    "lmms-lab/llava-onevision-qwen2-0.5b-ov",
    "Qwen/Qwen2-VL-7B-Instruct",
    "OpenGVLab/InternVL2_5-1B",
    "OpenGVLab/InternVL2_5-8B",
    "OpenGVLab/PVC-InternVL2-8B",
    "AIDC-AI/Ovis1.6-Llama3.2-3B",
    "BAAI/Aquila-VL-2B-llava-qwen"
]

# Iterate over the list and try to load each model's configuration.
found_models = []
found_classes = []
for model_id in model_list:
    try:
        config = AutoConfig.from_pretrained(model_id)
        ## what is the model type for this config object?
        model_type = config.model_type
        print(f"Loaded {model_id} as {model_type}")
        found_models.append(model_id)
        found_classes.append(model_type)
    except Exception as e:
        print(f"Error loading {model_id}: {e}")
    # A short pause to avoid overwhelming the server (optional)
    time.sleep(0.1)


print(f"Found {len(found_models)} models out of {len(model_list)}")
print(f"Found classes: {len(set(found_classes))}")
mapped_classes = []
mapped_models = []
model_mappping = dict(zip(found_models, found_classes))
for k, v in model_mappping.items():
    if v not in mapped_classes:
        mapped_classes.append(v)
        mapped_models.append(k)

mapped_models = dict(zip(mapped_classes, mapped_models))
print(f"Found {len(mapped_classes)} unique classes")
this_dir = os.path.dirname(os.path.abspath(__file__))
with open(os.path.join(this_dir, "mapped_models.json"), "w") as f:
    json.dump(mapped_models, f)
        
# with open(os.path.join(this_dir, "found_models.json"), "w") as f:
#     json.dump(found_models, f)

# with open(os.path.join(this_dir, "model_mapping.json"), "w") as f:
#     json.dump(model_mappping, f)