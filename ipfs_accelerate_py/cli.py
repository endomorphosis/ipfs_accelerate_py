#!/usr/bin/env python3
"""
IPFS Accelerate CLI Tool

This is the main CLI tool for IPFS Accelerate that provides a unified interface
for all functionality including MCP server management, inference operations,
file operations, and more.

Usage:
    ipfs-accelerate mcp start               # Start MCP server
    ipfs-accelerate mcp dashboard           # Start MCP server dashboard
    ipfs-accelerate mcp status              # Check MCP server status
    ipfs-accelerate inference generate      # Run text generation
    ipfs-accelerate files add               # Add files to IPFS
    ipfs-accelerate network status          # Check network status
    ipfs-accelerate models list             # List available models
    ipfs-accelerate --help                  # Show help for all commands
"""

import argparse
import anyio
import json
import logging
import os
import sys
import signal
import subprocess
import time
import webbrowser
from typing import Dict, Any, List, Optional, Union
from pathlib import Path

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger("ipfs_accelerate_cli")

# Defer heavy imports until needed - global variables for lazy loading
HAVE_CORE = None
shared_core = None
inference_ops = None
file_ops = None
model_ops = None
network_ops = None
queue_ops = None
test_ops = None
IPFSAccelerateMCPServer = None

def _load_heavy_imports():
    """Load heavy imports only when needed for actual command execution"""
    global HAVE_CORE, shared_core, inference_ops, file_ops, model_ops, network_ops, queue_ops, test_ops, IPFSAccelerateMCPServer
    
    if HAVE_CORE is not None:
        return  # Already loaded
    
    try:
        from ipfs_accelerate_py.mcp.server import IPFSAccelerateMCPServer as _IPFSAccelerateMCPServer
        from shared import SharedCore, InferenceOperations, FileOperations, ModelOperations, NetworkOperations, QueueOperations, TestOperations
        
        IPFSAccelerateMCPServer = _IPFSAccelerateMCPServer
        HAVE_CORE = True
        
        # Initialize core components
        shared_core = SharedCore()
        inference_ops = InferenceOperations(shared_core)
        file_ops = FileOperations(shared_core)
        model_ops = ModelOperations(shared_core)
        network_ops = NetworkOperations(shared_core)
        queue_ops = QueueOperations(shared_core)
        test_ops = TestOperations(shared_core)
        
    except ImportError as e:
        logger.warning(f"Core modules not available: {e}")
        try:
            # Try alternative import paths
            import sys
            import os
            sys.path.append(os.path.dirname(__file__))
            from shared import SharedCore, InferenceOperations, FileOperations, ModelOperations, NetworkOperations, QueueOperations, TestOperations
            from ipfs_accelerate_py.mcp.server import IPFSAccelerateMCPServer as _IPFSAccelerateMCPServer
            
            IPFSAccelerateMCPServer = _IPFSAccelerateMCPServer
            HAVE_CORE = True
            
            # Initialize core components
            shared_core = SharedCore()
            inference_ops = InferenceOperations(shared_core)
            file_ops = FileOperations(shared_core)
            model_ops = ModelOperations(shared_core)
            network_ops = NetworkOperations(shared_core)
            queue_ops = QueueOperations(shared_core)
            test_ops = TestOperations(shared_core)
            
        except ImportError as e2:
            logger.warning(f"Alternative import also failed: {e2}")
            HAVE_CORE = False
            
            # Fallback shared core for when imports fail
            class SharedCore:
                def __init__(self):
                    pass
                def get_status(self):
                    return {"error": "Core not available", "fallback": True}
            
            shared_core = SharedCore()
            inference_ops = None
            file_ops = None
            model_ops = None
            network_ops = None
            queue_ops = None
            test_ops = None

class IPFSAccelerateCLI:
    """Main CLI class for IPFS Accelerate"""
    
    def __init__(self):
        self.mcp_process = None
        self.dashboard_process = None
        
    def setup_signal_handlers(self):
        """Setup signal handlers for graceful shutdown"""
        def signal_handler(signum, frame):
            logger.info("Received shutdown signal")
            self.cleanup()
            sys.exit(0)
        
        signal.signal(signal.SIGINT, signal_handler)
        signal.signal(signal.SIGTERM, signal_handler)
    
    def cleanup(self):
        """Cleanup resources"""
        if self.mcp_process:
            self.mcp_process.terminate()
            self.mcp_process = None
        if self.dashboard_process:
            self.dashboard_process.terminate()
            self.dashboard_process = None
    
    def run_mcp_start(self, args):
        """Start MCP server with integrated dashboard, model manager, and queue monitoring"""
        logger.info("Starting IPFS Accelerate MCP Server with integrated dashboard...")
        
        # Load heavy imports only when needed
        _load_heavy_imports()
        
        # Always enable dashboard integration
        args.dashboard = True
        
        # Preferred path: Flask-based dashboard if available
        try:
            from ipfs_accelerate_py.mcp_dashboard import MCPDashboard  # requires Flask

            logger.info(f"Starting MCP Dashboard on port {args.port}")
            dashboard = MCPDashboard(port=args.port, host=args.host)

            # Open browser if requested
            if getattr(args, 'open_browser', False):
                import time, threading, webbrowser
                def open_browser_delayed():
                    time.sleep(2)
                    webbrowser.open(f"http://{args.host}:{args.port}/dashboard")
                threading.Thread(target=open_browser_delayed, daemon=True).start()

            dashboard.run(debug=False)
            return 0

        except (ImportError, ModuleNotFoundError) as e:
            # If Flask or its deps are missing, fall back automatically
            if 'flask' in str(e).lower() or 'Flask' in str(e):
                logger.warning("Flask not installed; falling back to integrated HTTP dashboard")
                return self._start_integrated_mcp_server(args)
            # Otherwise, re-raise to the generic handler
            raise
        except KeyboardInterrupt:
            logger.info("MCP server stopped by user")
            return 0
        except Exception as e:
            logger.error(f"Error starting MCP server: {e}")
            # Best-effort fallback to integrated HTTP server
            try:
                logger.info("Falling back to integrated HTTP dashboard")
                return self._start_integrated_mcp_server(args)
            except Exception as e2:
                logger.error(f"Integrated dashboard also failed: {e2}")
                import traceback; traceback.print_exc()
                return 1
    
    def run_mcp_dashboard(self, args):
        """Start MCP dashboard only"""
        # Dashboard command is the same as start with dashboard enabled
        args.dashboard = True
        return self.run_mcp_start(args)
    
    def run_mcp_status(self, args):
        """Check MCP server status"""
        logger.info(f"Checking MCP server status at {args.host}:{args.port}")
        
        import urllib.request
        import json
        
        try:
            # Try to connect to the health endpoint
            url = f"http://{args.host}:{args.port}/health"
            logger.debug(f"Checking health endpoint: {url}")
            
            with urllib.request.urlopen(url, timeout=5) as response:
                if response.status == 200:
                    data = json.loads(response.read().decode())
                    logger.info("‚úì MCP server is running")
                    logger.info(f"  Status: {data.get('status', 'unknown')}")
                    logger.info(f"  Host: {data.get('host', 'unknown')}")
                    logger.info(f"  Port: {data.get('port', 'unknown')}")
                    logger.info(f"  Server: {data.get('server', 'unknown')}")
                    return 0
                else:
                    logger.error(f"‚úó MCP server returned status {response.status}")
                    return 1
                    
        except urllib.error.URLError as e:
            logger.error(f"‚úó MCP server is not responding at {args.host}:{args.port}")
            logger.error(f"  Error: {e}")
            return 1
        except Exception as e:
            logger.error(f"‚úó Error checking MCP server status: {e}")
            return 1
    
    def run_mcp_user_info(self, args):
        """Get GitHub user information"""
        from ipfs_accelerate_py.mcp.tools.dashboard_data import get_user_info
        
        try:
            user_info = get_user_info()
            
            if args.json:
                print(json.dumps(user_info, indent=2))
            else:
                if user_info.get('authenticated'):
                    logger.info("‚úì GitHub User Information:")
                    logger.info(f"  Username: {user_info.get('username', 'Unknown')}")
                    if user_info.get('name'):
                        logger.info(f"  Name: {user_info.get('name')}")
                    if user_info.get('email'):
                        logger.info(f"  Email: {user_info.get('email')}")
                    logger.info(f"  Token Type: {user_info.get('token_type', 'unknown')}")
                    if user_info.get('public_repos') is not None:
                        logger.info(f"  Public Repos: {user_info.get('public_repos')}")
                else:
                    logger.warning("‚úó Not authenticated with GitHub")
                    if user_info.get('error'):
                        logger.warning(f"  Error: {user_info['error']}")
            return 0
        except Exception as e:
            logger.error(f"Error getting user info: {e}")
            return 1
    
    def run_mcp_cache_stats(self, args):
        """Get cache statistics"""
        from ipfs_accelerate_py.mcp.tools.dashboard_data import get_cache_stats
        
        try:
            cache_stats = get_cache_stats()
            
            if args.json:
                print(json.dumps(cache_stats, indent=2))
            else:
                if cache_stats.get('available'):
                    logger.info("‚úì GitHub API Cache Statistics:")
                    logger.info(f"  Total Entries: {cache_stats.get('total_entries', 0)}")
                    logger.info(f"  Cache Size: {cache_stats.get('total_size_mb', 0):.2f} MB")
                    logger.info(f"  Hit Rate: {cache_stats.get('hit_rate', 0)*100:.1f}%")
                    logger.info(f"  Total Hits: {cache_stats.get('total_hits', 0)}")
                    logger.info(f"  Total Misses: {cache_stats.get('total_misses', 0)}")
                    logger.info(f"  P2P Enabled: {'Yes' if cache_stats.get('p2p_enabled') else 'No'}")
                    if cache_stats.get('p2p_enabled'):
                        logger.info(f"  P2P Peers: {cache_stats.get('p2p_peers', 0)}")
                else:
                    logger.warning("‚úó Cache not available")
                    if cache_stats.get('error'):
                        logger.warning(f"  Error: {cache_stats['error']}")
            return 0
        except Exception as e:
            logger.error(f"Error getting cache stats: {e}")
            return 1
    
    def run_mcp_peer_status(self, args):
        """Get P2P peer system status"""
        from ipfs_accelerate_py.mcp.tools.dashboard_data import get_peer_status
        
        try:
            peer_status = get_peer_status()
            
            if args.json:
                print(json.dumps(peer_status, indent=2))
            else:
                logger.info("P2P Peer System Status:")
                logger.info(f"  Enabled: {'Yes' if peer_status.get('enabled') else 'No'}")
                logger.info(f"  Active: {'Yes' if peer_status.get('active') else 'No'}")
                logger.info(f"  Peer Count: {peer_status.get('peer_count', 0)}")
                
                if peer_status.get('peers'):
                    logger.info("  Connected Peers:")
                    for peer in peer_status['peers']:
                        logger.info(f"    - {peer.get('peer_id')} ({peer.get('runner_name')})")
                elif not peer_status.get('enabled'):
                    logger.info("  (P2P cache sharing is not enabled)")
            return 0
        except Exception as e:
            logger.error(f"Error getting peer status: {e}")
            return 1
    
    def run_mcp_metrics(self, args):
        """Get system metrics"""
        from ipfs_accelerate_py.mcp.tools.dashboard_data import get_system_metrics
        
        try:
            metrics = get_system_metrics()
            
            if args.json:
                print(json.dumps(metrics, indent=2))
            else:
                logger.info("‚úì System Metrics:")
                logger.info(f"  CPU Usage: {metrics.get('cpu_percent', 0)}%")
                logger.info(f"  Memory Usage: {metrics.get('memory_percent', 0)}%")
                logger.info(f"  Memory Used: {metrics.get('memory_used_gb', 0):.2f} GB / {metrics.get('memory_total_gb', 0):.2f} GB")
                logger.info(f"  Uptime: {metrics.get('uptime', 'unknown')}")
                logger.info(f"  Active Connections: {metrics.get('active_connections', 0)}")
                logger.info(f"  Process ID: {metrics.get('pid', 0)}")
            return 0
        except Exception as e:
            logger.error(f"Error getting system metrics: {e}")
            return 1
    
    def run_mcp_logs(self, args):
        """Get system logs"""
        from ipfs_accelerate_py.logs import SystemLogs
        
        try:
            logs_manager = SystemLogs(args.service)
            
            if args.stats:
                # Show log statistics
                stats = logs_manager.get_stats()
                if args.json:
                    print(json.dumps(stats, indent=2))
                else:
                    logger.info(f"\nüìä Log Statistics (last hour):")
                    logger.info(f"Total logs: {stats['total']}")
                    logger.info(f"\nBy level:")
                    for level, count in stats['by_level'].items():
                        if count > 0:
                            logger.info(f"  {level}: {count}")
                return 0
            
            if args.errors:
                # Show recent errors
                logs = logs_manager.get_recent_errors()
                if args.json:
                    print(json.dumps(logs, indent=2))
                else:
                    logger.info(f"\nüö® Recent Errors (last 24 hours): {len(logs)} found\n")
                    for log in logs:
                        logger.info(f"[{log['timestamp']}] {log['level']}: {log['message']}")
                return 0
            
            # Get logs
            # Ensure level is a string or None, not a list
            level = args.level
            if isinstance(level, list):
                level = level[0] if level else None
            
            logs = logs_manager.get_logs(
                lines=args.lines,
                since=args.since,
                level=level,
                follow=args.follow
            )
            
            if args.json:
                print(json.dumps(logs, indent=2))
            else:
                if not args.follow:
                    logger.info(f"\nüìù System Logs ({len(logs)} entries):\n")
                for log in logs:
                    level_emoji = {
                        'ERROR': '‚ùå',
                        'CRITICAL': 'üî•',
                        'WARNING': '‚ö†Ô∏è',
                        'INFO': '‚ÑπÔ∏è',
                        'DEBUG': 'üîç'
                    }.get(log['level'], 'üìù')
                    logger.info(f"{level_emoji} [{log['timestamp']}] {log['level']}: {log['message']}")
            
            return 0
        except Exception as e:
            logger.error(f"Error getting system logs: {e}")
            return 1
    
    def _start_integrated_mcp_server(self, args):
        """Start the integrated MCP server with dashboard, model manager, and queue monitoring"""
        import asyncio
        import threading
        from http.server import HTTPServer, BaseHTTPRequestHandler
        import json
        
        logger.info(f"Starting integrated MCP server on port {args.port}")
        logger.info("Integrated components: MCP Server, Web Dashboard, Model Manager, Queue Monitor")
        
        try:
            # Create the integrated dashboard handler
            class IntegratedMCPHandler(BaseHTTPRequestHandler):
                def do_GET(self):
                    if self.path == '/' or self.path == '/dashboard':
                        self._serve_dashboard()
                    elif self.path == '/health':
                        # Simple health check endpoint
                        try:
                            self.send_response(200)
                            self.send_header('Content-type', 'application/json')
                            self.end_headers()
                            # Report the actual bound host/port
                            try:
                                bound_host = self.server.server_address[0]
                                bound_port = getattr(self.server, 'server_port', None)
                            except Exception:
                                bound_host = args.host
                                bound_port = args.port
                            payload = {
                                "status": "ok",
                                "server": "IPFS Accelerate MCP (integrated)",
                                "host": bound_host,
                                "port": bound_port
                            }
                            self.wfile.write(json.dumps(payload).encode())
                        except Exception:
                            self.send_response(500)
                            self.end_headers()
                    elif self.path == '/favicon.ico':
                        # Avoid 404 for favicon requests
                        self.send_response(204)
                        self.end_headers()
                    elif self.path.startswith('/api/mcp/models/'):
                        self._handle_model_api()
                    elif self.path.startswith('/api/mcp/'):
                        self._handle_mcp_api()
                    elif self.path.startswith('/api/models/'):
                        self._handle_model_api()
                    elif self.path.startswith('/api/queue/'):
                        self._handle_queue_api()
                    elif self.path.startswith('/static/'):
                        self._serve_static()
                    else:
                        self.send_response(404)
                        self.end_headers()
                
                def do_POST(self):
                    if self.path.startswith('/api/'):
                        self._handle_post_api()
                    else:
                        self.send_response(404)
                        self.end_headers()
            
            def _serve_dashboard(self):
                """Serve the integrated dashboard"""
                try:
                    template_path = os.path.join(os.path.dirname(__file__), 'templates', 'dashboard.html')
                    with open(template_path, 'r', encoding='utf-8') as f:
                        dashboard_html = f.read()
                    
                    self.send_response(200)
                    self.send_header('Content-type', 'text/html')
                    self.end_headers()
                    self.wfile.write(dashboard_html.encode())
                except Exception as e:
                    # Fallback to a basic HTML page if template not found
                    self.send_response(200)
                    self.send_header('Content-type', 'text/html')
                    self.end_headers()
                    fallback_html = """<!DOCTYPE html>
<html><head><title>MCP Dashboard</title></head>
<body><h1>MCP Server Dashboard</h1><p>Template loading error: """ + str(e) + """</p></body></html>"""
                    self.wfile.write(fallback_html.encode())
            
            def _handle_mcp_api(self):
                """Handle MCP-related API calls"""
                self.send_response(200)
                self.send_header('Content-type', 'application/json')
                self.end_headers()
                
                # Mock MCP status for now
                response = {
                    "status": "running",
                    "server": "IPFS Accelerate MCP",
                    "port": args.port,
                    "components": ["mcp_server", "dashboard", "model_manager", "queue_monitor"]
                }
                self.wfile.write(json.dumps(response).encode())
            
            def _handle_model_api(self):
                """Handle model manager API calls"""
                try:
                    from urllib.parse import urlparse, parse_qs
                    parsed_url = urlparse(self.path)
                    query_params = parse_qs(parsed_url.query)
                    
                    # Handle different model API endpoints
                    if '/search' in self.path:
                        self._handle_model_search(query_params)
                    elif '/stats' in self.path:
                        self._handle_model_stats()
                    elif '/test' in self.path:
                        self._handle_model_test(query_params)
                    elif '/details' in self.path:
                        # Extract model ID from path like /api/mcp/models/Falconsai/nsfw_image_detection/details
                        model_id = self.path.split('/models/')[-1].replace('/details', '')
                        self._handle_model_details(model_id)
                    else:
                        # Default model listing
                        self.send_response(200)
                        self.send_header('Content-type', 'application/json')
                        self.end_headers()
                        response = {"models": [], "status": "Model manager API"}
                        self.wfile.write(json.dumps(response).encode())
                        
                except Exception as e:
                    self.send_response(500)
                    self.send_header('Content-type', 'application/json')
                    self.end_headers()
                    response = {"error": str(e)}
                    self.wfile.write(json.dumps(response).encode())
            
            def _handle_model_search(self, query_params):
                """Handle HuggingFace model search"""
                query = query_params.get('q', [''])[0].lower()  # Changed from 'query' to 'q'
                task = query_params.get('task', [''])[0]
                hardware = query_params.get('hardware', [''])[0]  # Changed from 'size' to 'hardware'
                
                # Enhanced fallback database with realistic models
                model_database = [
                    {
                        "id": "microsoft/DialoGPT-large",
                        "title": "DialoGPT Large",
                        "description": "Large-scale conversational response generation model trained on 147M dialogues",
                        "task": "text-generation",
                        "downloads": 125000,
                        "likes": 2300,
                        "size": "large",
                        "architecture": "GPT-2",
                        "parameters": "774M",
                        "memory_gb": 1.4,
                        "throughput": 45,
                        "tags": ["conversational", "dialogue", "pytorch"]
                    },
                    {
                        "id": "microsoft/DialoGPT-medium",
                        "title": "DialoGPT Medium", 
                        "description": "Medium-scale conversational response generation model",
                        "task": "text-generation",
                        "downloads": 89000,
                        "likes": 1800,
                        "size": "medium",
                        "architecture": "GPT-2",
                        "parameters": "354M",
                        "memory_gb": 0.7,
                        "throughput": 62,
                        "tags": ["conversational", "dialogue", "pytorch"]
                    },
                    {
                        "id": "meta-llama/Llama-2-7b-chat-hf",
                        "title": "Llama 2 7B Chat",
                        "description": "Fine-tuned version of Llama 2 7B for chat conversations",
                        "task": "text-generation", 
                        "downloads": 1800000,
                        "likes": 45000,
                        "size": "large",
                        "architecture": "LLaMA",
                        "parameters": "7B",
                        "memory_gb": 13.5,
                        "throughput": 28,
                        "tags": ["llama", "chat", "conversational"]
                    },
                    {
                        "id": "meta-llama/Llama-2-13b-chat-hf",
                        "title": "Llama 2 13B Chat",
                        "description": "Fine-tuned version of Llama 2 13B for chat conversations",
                        "task": "text-generation",
                        "downloads": 950000,
                        "likes": 25000,
                        "size": "large",
                        "architecture": "LLaMA",
                        "parameters": "13B",
                        "memory_gb": 25.0,
                        "throughput": 18,
                        "tags": ["llama", "chat", "conversational"]
                    },
                    {
                        "id": "codellama/CodeLlama-7b-Python-hf",
                        "title": "Code Llama 7B Python",
                        "description": "Code Llama model fine-tuned for Python code generation",
                        "task": "code-generation",
                        "downloads": 850000,
                        "likes": 12000,
                        "size": "large",
                        "architecture": "LLaMA",
                        "parameters": "7B",
                        "memory_gb": 13.5,
                        "throughput": 32,
                        "tags": ["llama", "code", "python"]
                    },
                    {
                        "id": "bert-base-uncased",
                        "title": "BERT Base Uncased",
                        "description": "Base BERT model, uncased version for text understanding",
                        "task": "text-classification",
                        "downloads": 2100000,
                        "likes": 25000,
                        "size": "medium",
                        "architecture": "BERT",
                        "parameters": "110M",
                        "memory_gb": 0.4,
                        "throughput": 120,
                        "tags": ["bert", "base", "uncased"]
                    },
                    {
                        "id": "distilbert-base-uncased",
                        "title": "DistilBERT Base Uncased",
                        "description": "Distilled version of BERT base model, faster inference",
                        "task": "text-classification",
                        "downloads": 1500000,
                        "likes": 18000,
                        "size": "small",
                        "architecture": "DistilBERT",
                        "parameters": "66M",
                        "memory_gb": 0.3,
                        "throughput": 180,
                        "tags": ["distilbert", "base", "uncased"]
                    },
                    {
                        "id": "gpt2",
                        "title": "GPT-2",
                        "description": "OpenAI's GPT-2 model for text generation",
                        "task": "text-generation",
                        "downloads": 3200000,
                        "likes": 35000,
                        "size": "medium",
                        "architecture": "GPT-2",
                        "parameters": "124M",
                        "memory_gb": 0.5,
                        "throughput": 85,
                        "tags": ["gpt2", "openai", "generation"]
                    },
                    {
                        "id": "gpt2-medium",
                        "title": "GPT-2 Medium",
                        "description": "Medium version of OpenAI's GPT-2 model",
                        "task": "text-generation", 
                        "downloads": 1900000,
                        "likes": 22000,
                        "size": "medium",
                        "architecture": "GPT-2",
                        "parameters": "354M",
                        "memory_gb": 1.4,
                        "throughput": 53,
                        "tags": ["gpt2", "openai", "generation"]
                    },
                    {
                        "id": "gpt2-large",
                        "title": "GPT-2 Large",
                        "description": "Large version of OpenAI's GPT-2 model",
                        "task": "text-generation",
                        "downloads": 1200000,
                        "likes": 18000,
                        "size": "large",
                        "architecture": "GPT-2",
                        "parameters": "774M",
                        "memory_gb": 3.2,
                        "throughput": 35,
                        "tags": ["gpt2", "openai", "generation"]
                    }
                ]
                
                # Filter models based on search criteria
                filtered_models = []
                for model in model_database:
                    # Search in model ID, title, and description
                    search_text = f"{model['id']} {model['title']} {model['description']}".lower()
                    
                    # Check if query matches
                    query_match = not query or query in search_text
                    
                    # Check task filter
                    task_match = not task or task == 'all' or model['task'] == task
                    
                    # Check hardware filter (simplified check)
                    hardware_match = True
                    if hardware and hardware != 'all':
                        if hardware == 'cpu':
                            hardware_match = model['size'] in ['small', 'medium']  # CPU can handle smaller models
                        elif hardware == 'gpu':
                            hardware_match = True  # GPU can handle all models
                    
                    if query_match and task_match and hardware_match:
                        # Transform to expected format
                        formatted_model = {
                            "model_id": model['id'],
                            "model_info": {
                                "model_name": model['title'],
                                "description": model['description'],
                                "pipeline_tag": model['task'],
                                "downloads": model['downloads'],
                                "likes": model.get('likes', 0),
                                "architecture": model.get('architecture', 'Unknown')
                            },
                            "performance": {
                                "parameters": model.get('parameters', ''),
                                "memory_gb": model.get('memory_gb', 1.0),
                                "throughput_tokens_per_sec": model.get('throughput', 50)
                            },
                            "compatibility": {
                                "supports_cpu": True,
                                "supports_gpu": model['size'] != 'large',
                                "supports_mps": True,
                                "min_ram_gb": 2 if model['size'] == 'small' else 4 if model['size'] == 'medium' else 8,
                                "recommended_hardware": "GPU" if model['size'] == 'large' else "CPU"
                            }
                        }
                        filtered_models.append(formatted_model)
                
                self.send_response(200)
                self.send_header('Content-type', 'application/json')
                self.end_headers()
                
                response = {
                    "results": filtered_models,
                    "total": len(filtered_models),
                    "query": query,
                    "fallback": True,
                    "message": "Using integrated fallback model database"
                }
                self.wfile.write(json.dumps(response).encode())
            
            def _handle_model_stats(self):
                """Handle model statistics"""
                self.send_response(200)
                self.send_header('Content-type', 'application/json')
                self.end_headers()
                
                # Mock statistics based on the model database
                stats = {
                    "total_cached_models": 10,
                    "models_with_performance": 10,
                    "models_with_compatibility": 10,
                    "architecture_distribution": {
                        "GPT-2": 4,
                        "LLaMA": 3,
                        "BERT": 1,
                        "DistilBERT": 1,
                        "Unknown": 1
                    },
                    "task_distribution": {
                        "text-generation": 7,
                        "text-classification": 2,
                        "code-generation": 1
                    },
                    "popular_models": [
                        {"model_id": "gpt2", "downloads": 3200000},
                        {"model_id": "bert-base-uncased", "downloads": 2100000},
                        {"model_id": "gpt2-medium", "downloads": 1900000},
                        {"model_id": "meta-llama/Llama-2-7b-chat-hf", "downloads": 1800000},
                        {"model_id": "distilbert-base-uncased", "downloads": 1500000}
                    ],
                    "fallback": True,
                    "message": "Using integrated fallback statistics"
                }
                self.wfile.write(json.dumps(stats).encode())
            
            def _handle_model_test(self, query_params):
                """Handle model compatibility testing"""
                model_id = query_params.get('model', [''])[0]
                platforms = query_params.get('platforms', ['cpu'])[0].split(',')
                batch_size = int(query_params.get('batch_size', ['1'])[0])
                seq_length = int(query_params.get('seq_length', ['512'])[0])
                precision = query_params.get('precision', ['FP32'])[0]
                
                if not model_id:
                    self.send_response(400)
                    self.send_header('Content-type', 'application/json')
                    self.end_headers()
                    response = {"error": "Model ID is required"}
                    self.wfile.write(json.dumps(response).encode())
                    return
                
                # Generate realistic test results
                results = []
                for platform in platforms:
                    platform = platform.strip().lower()
                    
                    # Simulate different performance characteristics per platform
                    if platform == 'cpu':
                        memory_gb = 2.1 + (batch_size * seq_length / 10000)
                        latency_ms = 150 + (seq_length / 10)
                        status = 'compatible'
                        notes = 'Good CPU performance, recommended for development'
                    elif platform == 'cuda':
                        memory_gb = 1.4 + (batch_size * seq_length / 15000)
                        latency_ms = 25 + (seq_length / 50)
                        status = 'optimal'
                        notes = 'Excellent GPU acceleration, recommended for production'
                    elif platform == 'rocm':
                        memory_gb = 1.6 + (batch_size * seq_length / 12000)
                        latency_ms = 35 + (seq_length / 40)
                        status = 'compatible'
                        notes = 'Good AMD GPU performance'
                    elif platform == 'openvino':
                        memory_gb = 1.8 + (batch_size * seq_length / 14000)
                        latency_ms = 45 + (seq_length / 30)
                        status = 'compatible'
                        notes = 'Optimized for Intel hardware'
                    elif platform == 'mps':
                        memory_gb = 1.5 + (batch_size * seq_length / 16000)
                        latency_ms = 30 + (seq_length / 45)
                        status = 'optimal'
                        notes = 'Excellent Apple Silicon performance'
                    else:
                        memory_gb = 2.5
                        latency_ms = 200
                        status = 'limited'
                        notes = f'Limited support for {platform}'
                    
                    results.append({
                        'platform': platform.upper(),
                        'status': status,
                        'memory': f'{memory_gb:.1f} GB',
                        'performance': f'{int(latency_ms)}ms/token',
                        'batch_size': batch_size,
                        'seq_length': seq_length,
                        'precision': precision,
                        'notes': notes,
                        'test_time': '2.3s'
                    })
                
                self.send_response(200)
                self.send_header('Content-type', 'application/json')
                self.end_headers()
                
                response = {
                    "model_id": model_id,
                    "results": results,
                    "timestamp": time.time()
                }
                self.wfile.write(json.dumps(response).encode())
            
            def _handle_model_details(self, model_id):
                """Handle model details API request"""
                try:
                    from urllib.parse import unquote
                    from ipfs_accelerate_py.huggingface_hub_scanner import HuggingFaceHubScanner
                    
                    # Decode URL-encoded model ID
                    model_id = unquote(model_id)
                    
                    # Get HuggingFaceHubScanner instance
                    scanner = HuggingFaceHubScanner()
                    
                    # Check cache first
                    if model_id in scanner.model_cache:
                        model_info = scanner.model_cache[model_id]
                        self.send_response(200)
                        self.send_header('Content-type', 'application/json')
                        self.end_headers()
                        response = {
                            'status': 'success',
                            'model': model_info.to_dict()
                        }
                        self.wfile.write(json.dumps(response).encode())
                        return
                    
                    # Fetch from API if not in cache
                    search_results = scanner.search_models(model_id, limit=1)
                    if search_results:
                        self.send_response(200)
                        self.send_header('Content-type', 'application/json')
                        self.end_headers()
                        response = {
                            'status': 'success',
                            'model': search_results[0].to_dict()
                        }
                        self.wfile.write(json.dumps(response).encode())
                        return
                    
                    # Model not found
                    self.send_response(404)
                    self.send_header('Content-type', 'application/json')
                    self.end_headers()
                    response = {
                        'status': 'error',
                        'message': f'Model {model_id} not found'
                    }
                    self.wfile.write(json.dumps(response).encode())
                    
                except Exception as e:
                    self.send_response(500)
                    self.send_header('Content-type', 'application/json')
                    self.end_headers()
                    response = {
                        'status': 'error',
                        'message': str(e)
                    }
                    self.wfile.write(json.dumps(response).encode())
            
            def _handle_queue_api(self):
                """Handle queue monitoring API calls"""
                self.send_response(200)
                self.send_header('Content-type', 'application/json')
                self.end_headers()
                
                # Mock queue status
                response = {
                    "queue_status": "active",
                    "pending_jobs": 0,
                    "completed_jobs": 0,
                    "failed_jobs": 0,
                    "workers": 1
                }
                self.wfile.write(json.dumps(response).encode())
            
            def _serve_static(self):
                """Serve static files (CSS, JS, images)"""
                try:
                    # Extract the file path from the URL
                    file_path = self.path[8:]  # Remove '/static/'
                    static_file_path = os.path.join(os.path.dirname(__file__), 'static', file_path)
                    
                    if os.path.exists(static_file_path) and os.path.isfile(static_file_path):
                        # Determine content type based on file extension
                        content_type = 'text/plain'
                        if file_path.endswith('.css'):
                            content_type = 'text/css'
                        elif file_path.endswith('.js'):
                            content_type = 'application/javascript'
                        elif file_path.endswith('.png'):
                            content_type = 'image/png'
                        elif file_path.endswith('.jpg') or file_path.endswith('.jpeg'):
                            content_type = 'image/jpeg'
                        elif file_path.endswith('.gif'):
                            content_type = 'image/gif'
                        elif file_path.endswith('.svg'):
                            content_type = 'image/svg+xml'
                        
                        # Read and serve the file
                        with open(static_file_path, 'rb') as f:
                            content = f.read()
                        
                        self.send_response(200)
                        self.send_header('Content-type', content_type)
                        self.send_header('Content-Length', str(len(content)))
                        self.end_headers()
                        self.wfile.write(content)
                    else:
                        # File not found
                        self.send_response(404)
                        self.send_header('Content-type', 'text/html')
                        self.end_headers()
                        error_html = f"<html><body><h1>404 Not Found</h1><p>Static file not found: {file_path}</p></body></html>"
                        self.wfile.write(error_html.encode())
                        
                except Exception as e:
                    self.send_response(500)
                    self.send_header('Content-type', 'text/html')
                    self.end_headers()
                    error_html = f"<html><body><h1>500 Server Error</h1><p>Error serving static file: {str(e)}</p></body></html>"
                    self.wfile.write(error_html.encode())
            
            def _handle_post_api(self):
                """Handle POST API requests"""
                try:
                    content_length = int(self.headers.get('Content-Length', 0))
                    post_data = self.rfile.read(content_length)
                    data = json.loads(post_data.decode('utf-8')) if post_data else {}
                    
                    # Route based on the path
                    if '/api/mcp/models/download' in self.path or '/api/models/download' in self.path:
                        self._handle_model_download(data)
                    elif '/api/mcp/models/test' in self.path or '/api/models/test' in self.path:
                        self._handle_model_test_post(data)
                    else:
                        # Default stub response for unimplemented endpoints
                        self.send_response(200)
                        self.send_header('Content-type', 'application/json')
                        self.end_headers()
                        response = {"status": "received", "message": "API endpoint not yet implemented"}
                        self.wfile.write(json.dumps(response).encode())
                        
                except Exception as e:
                    self.send_response(500)
                    self.send_header('Content-type', 'application/json')
                    self.end_headers()
                    response = {"status": "error", "message": str(e)}
                    self.wfile.write(json.dumps(response).encode())
            
            def _handle_model_download(self, data):
                """Handle model download POST request"""
                try:
                    model_id = data.get('model_id')
                    if not model_id:
                        self.send_response(400)
                        self.send_header('Content-type', 'application/json')
                        self.end_headers()
                        response = {"status": "error", "message": "model_id is required"}
                        self.wfile.write(json.dumps(response).encode())
                        return
                    
                    logger.info(f"Download request for model: {model_id}")
                    
                    # Try to use the HuggingFace scanner
                    try:
                        from ipfs_accelerate_py.huggingface_hub_scanner import HuggingFaceHubScanner
                        scanner = HuggingFaceHubScanner(cache_dir="./mcp_model_cache")
                        result = scanner.download_model(model_id)
                        
                        self.send_response(200)
                        self.send_header('Content-type', 'application/json')
                        self.end_headers()
                        self.wfile.write(json.dumps(result).encode())
                        
                    except ImportError as e:
                        logger.warning(f"HuggingFaceHubScanner not available: {e}")
                        # Fallback to simulated download
                        self.send_response(200)
                        self.send_header('Content-type', 'application/json')
                        self.end_headers()
                        response = {
                            "status": "success",
                            "model_id": model_id,
                            "download_path": f"./models/{model_id}",
                            "message": f"Model {model_id} download initiated (simulated)"
                        }
                        self.wfile.write(json.dumps(response).encode())
                        
                except Exception as e:
                    logger.error(f"Error handling download: {e}")
                    self.send_response(500)
                    self.send_header('Content-type', 'application/json')
                    self.end_headers()
                    response = {"status": "error", "message": str(e)}
                    self.wfile.write(json.dumps(response).encode())
            
            def _handle_model_test_post(self, data):
                """Handle model test POST request"""
                try:
                    model_id = data.get('model_id')
                    hardware = data.get('hardware', 'cpu')
                    test_prompt = data.get('test_prompt', 'Hello, world!')
                    
                    if not model_id:
                        self.send_response(400)
                        self.send_header('Content-type', 'application/json')
                        self.end_headers()
                        response = {"status": "error", "message": "model_id is required"}
                        self.wfile.write(json.dumps(response).encode())
                        return
                    
                    logger.info(f"Test request for model: {model_id} on {hardware}")
                    
                    # For now, return a stub response for inference
                    # Real implementation would load and run the model
                    self.send_response(200)
                    self.send_header('Content-type', 'application/json')
                    self.end_headers()
                    response = {
                        "status": "success",
                        "model_id": model_id,
                        "hardware": hardware,
                        "test_prompt": test_prompt,
                        "result": "Model inference not yet implemented. Download functionality is available.",
                        "message": "Test completed (inference stub)"
                    }
                    self.wfile.write(json.dumps(response).encode())
                    
                except Exception as e:
                    logger.error(f"Error handling test: {e}")
                    self.send_response(500)
                    self.send_header('Content-type', 'application/json')
                    self.end_headers()
                    response = {"status": "error", "message": str(e)}
                    self.wfile.write(json.dumps(response).encode())
            
            # Bind helper functions as methods on the handler class
            IntegratedMCPHandler._serve_dashboard = _serve_dashboard
            IntegratedMCPHandler._handle_mcp_api = _handle_mcp_api
            IntegratedMCPHandler._handle_model_api = _handle_model_api
            IntegratedMCPHandler._handle_model_search = _handle_model_search
            IntegratedMCPHandler._handle_model_stats = _handle_model_stats
            IntegratedMCPHandler._handle_model_test = _handle_model_test
            IntegratedMCPHandler._handle_queue_api = _handle_queue_api
            IntegratedMCPHandler._serve_static = _serve_static
            IntegratedMCPHandler._handle_post_api = _handle_post_api
            IntegratedMCPHandler._handle_model_download = _handle_model_download
            IntegratedMCPHandler._handle_model_test_post = _handle_model_test_post

            # Bind and start the integrated HTTP server
            try:
                server = HTTPServer((args.host, args.port), IntegratedMCPHandler)
                bound_port = args.port
            except OSError as e:
                # Address in use: try next 10 ports
                if getattr(e, 'errno', None) == 98:
                    server = None
                    for p in range(args.port + 1, args.port + 11):
                        try:
                            server = HTTPServer((args.host, p), IntegratedMCPHandler)
                            bound_port = p
                            logger.warning(f"Port {args.port} in use. Falling back to port {p}.")
                            break
                        except OSError:
                            continue
                    if server is None:
                        raise
                else:
                    raise

            logger.info(f"Integrated MCP Server + Dashboard started at http://{args.host}:{bound_port}")
            logger.info(f"Dashboard accessible at http://{args.host}:{bound_port}/dashboard")

            # Start GitHub Actions autoscaler in background thread
            # Only attempt if not in a container environment (gh CLI typically not in containers)
            autoscaler_thread = None
            autoscaler_instance = None
            in_container = os.path.exists('/.dockerenv') or os.path.exists('/run/.containerenv')
            
            if not getattr(args, 'disable_autoscaler', False) and not in_container:
                try:
                    from github_autoscaler import GitHubRunnerAutoscaler
                    
                    # Check if GitHub CLI is authenticated
                    from ipfs_accelerate_py.github_cli import GitHubCLI
                    gh = GitHubCLI()
                    auth_status = gh.get_auth_status()
                    
                    if auth_status.get("authenticated"):
                        logger.info("Starting GitHub Actions autoscaler in background...")
                        autoscaler_instance = GitHubRunnerAutoscaler(
                            owner=getattr(args, 'autoscaler_owner', None),
                            poll_interval=getattr(args, 'autoscaler_interval', 120),  # Increased from 60s to 120s
                            since_days=getattr(args, 'autoscaler_since_days', 1),
                            max_runners=getattr(args, 'autoscaler_max_runners', None),
                            filter_by_arch=True
                        )
                        
                        def run_autoscaler():
                            try:
                                autoscaler_instance.start(setup_signals=False)
                            except Exception as e:
                                logger.error(f"Autoscaler error: {e}", exc_info=True)
                        
                        autoscaler_thread = threading.Thread(target=run_autoscaler, daemon=True)
                        autoscaler_thread.start()
                        logger.info("‚úì GitHub Actions autoscaler started")
                    else:
                        logger.warning(f"GitHub CLI not authenticated - autoscaler disabled (user: {auth_status.get('username', 'none')})")
                except ImportError as e:
                    logger.warning(f"GitHub autoscaler not available: {e}")
                except Exception as e:
                    logger.error(f"Could not start autoscaler: {e}", exc_info=True)

            if getattr(args, 'open_browser', False):
                import webbrowser
                webbrowser.open(f"http://{args.host}:{bound_port}")

            try:
                server.serve_forever()
            except KeyboardInterrupt:
                logger.info("Server shutdown requested")
                if autoscaler_instance:
                    logger.info("Stopping autoscaler...")
                    autoscaler_instance.stop()
                server.shutdown()
                return 0
            except Exception as e:
                logger.error(f"Server error: {e}")
                return 1
            
        except Exception as e:
            logger.error(f"Error creating advanced dashboard: {e}")
            raise
    
    def run_p2p_status(self, args):
        """Get P2P scheduler status"""
        try:
            from ipfs_accelerate_py.p2p_workflow_scheduler import P2PWorkflowScheduler
            from ipfs_accelerate_py.mcp.tools.p2p_workflow_tools import get_scheduler
            
            scheduler = get_scheduler()
            if scheduler is None:
                logger.error("‚úó P2P scheduler not available")
                return 1
            
            status = scheduler.get_status()
            
            if args.json:
                print(json.dumps(status, indent=2))
            else:
                logger.info("‚úì P2P Workflow Scheduler Status:")
                logger.info(f"  Peer ID: {status['peer_id']}")
                logger.info(f"  Pending Tasks: {status['pending_tasks']}")
                logger.info(f"  Assigned Tasks: {status['assigned_tasks']}")
                logger.info(f"  Completed Tasks: {status['completed_tasks']}")
                logger.info(f"  Queue Size: {status['queue_size']}")
                logger.info(f"  Known Peers: {status['known_peers']}")
                logger.info(f"  Merkle Clock Hash: {status['merkle_clock']['merkle_root']}")
            
            return 0
            
        except Exception as e:
            logger.error(f"Error getting P2P scheduler status: {e}")
            return 1
    
    def run_p2p_submit(self, args):
        """Submit a task to P2P scheduler"""
        try:
            from ipfs_accelerate_py.mcp.tools.p2p_workflow_tools import get_scheduler
            from ipfs_accelerate_py.p2p_workflow_scheduler import P2PTask, WorkflowTag
            
            scheduler = get_scheduler()
            if scheduler is None:
                logger.error("‚úó P2P scheduler not available")
                return 1
            
            # Convert string tags to WorkflowTag enums
            workflow_tags = []
            for tag_str in args.tags:
                try:
                    enum_name = tag_str.upper().replace('-', '_')
                    workflow_tags.append(WorkflowTag[enum_name])
                except (KeyError, AttributeError):
                    logger.warning(f"Unknown tag: {tag_str}, skipping")
            
            # Create and submit task
            task = P2PTask(
                task_id=args.task_id,
                workflow_id=args.workflow_id,
                name=args.name,
                tags=workflow_tags,
                priority=args.priority,
                created_at=time.time()
            )
            
            success = scheduler.submit_task(task)
            
            if success:
                logger.info("‚úì Task submitted successfully")
                logger.info(f"  Task ID: {task.task_id}")
                logger.info(f"  Task Hash: {task.task_hash}")
                logger.info(f"  Priority: {task.priority}")
                return 0
            else:
                logger.error("‚úó Failed to submit task")
                return 1
                
        except Exception as e:
            logger.error(f"Error submitting task: {e}")
            return 1
    
    def run_p2p_next(self, args):
        """Get next task to execute"""
        try:
            from ipfs_accelerate_py.mcp.tools.p2p_workflow_tools import get_scheduler
            
            scheduler = get_scheduler()
            if scheduler is None:
                logger.error("‚úó P2P scheduler not available")
                return 1
            
            task = scheduler.get_next_task()
            
            if task is None:
                if args.json:
                    print(json.dumps({"task": None, "message": "No tasks available"}))
                else:
                    logger.info("No tasks available for this peer")
                return 0
            
            task_info = {
                "task_id": task.task_id,
                "workflow_id": task.workflow_id,
                "name": task.name,
                "tags": [tag.value for tag in task.tags],
                "priority": task.priority,
                "task_hash": task.task_hash,
                "assigned_peer": task.assigned_peer
            }
            
            if args.json:
                print(json.dumps({"task": task_info}, indent=2))
            else:
                logger.info("‚úì Next task:")
                logger.info(f"  Task ID: {task.task_id}")
                logger.info(f"  Workflow ID: {task.workflow_id}")
                logger.info(f"  Name: {task.name}")
                logger.info(f"  Tags: {', '.join(tag.value for tag in task.tags)}")
                logger.info(f"  Priority: {task.priority}")
                logger.info(f"  Assigned Peer: {task.assigned_peer}")
            
            return 0
            
        except Exception as e:
            logger.error(f"Error getting next task: {e}")
            return 1
    
    def run_p2p_complete(self, args):
        """Mark task as complete"""
        try:
            from ipfs_accelerate_py.mcp.tools.p2p_workflow_tools import get_scheduler
            
            scheduler = get_scheduler()
            if scheduler is None:
                logger.error("‚úó P2P scheduler not available")
                return 1
            
            success = scheduler.mark_task_complete(args.task_id)
            
            if success:
                logger.info(f"‚úì Task {args.task_id} marked complete")
                return 0
            else:
                logger.error(f"‚úó Task {args.task_id} not found")
                return 1
                
        except Exception as e:
            logger.error(f"Error marking task complete: {e}")
            return 1
    
    def run_p2p_check_tags(self, args):
        """Check if tags should bypass GitHub"""
        try:
            from ipfs_accelerate_py.mcp.tools.p2p_workflow_tools import get_scheduler
            from ipfs_accelerate_py.p2p_workflow_scheduler import WorkflowTag
            
            scheduler = get_scheduler()
            if scheduler is None:
                logger.error("‚úó P2P scheduler not available")
                return 1
            
            # Convert string tags to WorkflowTag enums
            workflow_tags = []
            for tag_str in args.tags:
                try:
                    enum_name = tag_str.upper().replace('-', '_')
                    workflow_tags.append(WorkflowTag[enum_name])
                except (KeyError, AttributeError):
                    pass
            
            should_bypass = scheduler.should_bypass_github(workflow_tags)
            is_p2p_only = scheduler.is_p2p_only(workflow_tags)
            
            result = {
                "should_bypass_github": should_bypass,
                "is_p2p_only": is_p2p_only,
                "tags": args.tags
            }
            
            if args.json:
                print(json.dumps(result, indent=2))
            else:
                logger.info("‚úì Tag Check Results:")
                logger.info(f"  Tags: {', '.join(args.tags)}")
                logger.info(f"  Should Bypass GitHub: {'Yes' if should_bypass else 'No'}")
                logger.info(f"  P2P Only: {'Yes' if is_p2p_only else 'No'}")
            
            return 0
            
        except Exception as e:
            logger.error(f"Error checking tags: {e}")
            return 1
    
    def run_p2p_clock(self, args):
        """Get merkle clock state"""
        try:
            from ipfs_accelerate_py.mcp.tools.p2p_workflow_tools import get_scheduler
            
            scheduler = get_scheduler()
            if scheduler is None:
                logger.error("‚úó P2P scheduler not available")
                return 1
            
            clock_data = scheduler.merkle_clock.to_dict()
            
            if args.json:
                print(json.dumps(clock_data, indent=2))
            else:
                logger.info("‚úì Merkle Clock State:")
                logger.info(f"  Node ID: {clock_data['node_id']}")
                logger.info(f"  Merkle Root: {clock_data['merkle_root']}")
                logger.info(f"  Vector Clock: {clock_data['vector']}")
            
            return 0
            
        except Exception as e:
            logger.error(f"Error getting clock state: {e}")
            return 1


def main():
    """Main entry point for the CLI"""
    try:
        # Create argument parser
        parser = argparse.ArgumentParser(
            description="IPFS Accelerate CLI - Unified interface for AI inference and IPFS operations",
            formatter_class=argparse.RawDescriptionHelpFormatter,
            epilog="""
Examples:
  ipfs-accelerate mcp start --dashboard --open-browser
  ipfs-accelerate mcp status
  ipfs-accelerate inference generate --prompt "Hello world"
  ipfs-accelerate models list --output-json
  ipfs-accelerate queue status
  ipfs-accelerate network status
            """
        )
        
        # Add global arguments
        parser.add_argument('--output-json', action='store_true', help='Output results in JSON format')
        parser.add_argument('--debug', action='store_true', help='Enable debug logging')
        
        # Create subparsers for different command categories
        subparsers = parser.add_subparsers(dest='command', help='Available commands')
        
        # MCP commands
        mcp_parser = subparsers.add_parser('mcp', help='MCP server management')
        mcp_subparsers = mcp_parser.add_subparsers(dest='mcp_command', help='MCP commands')
        
        # MCP start command
        start_parser = mcp_subparsers.add_parser('start', help='Start MCP server')
        start_parser.add_argument('--name', default='ipfs-accelerate', help='Server name')
        start_parser.add_argument('--host', default='0.0.0.0', help='Host to bind to (default: 0.0.0.0)')
        start_parser.add_argument('--port', type=int, default=9000, help='Port to bind to (default: 9000)')
        start_parser.add_argument('--dashboard', action='store_true', help='Enable web dashboard')
        start_parser.add_argument('--open-browser', action='store_true', help='Open browser automatically')
        start_parser.add_argument('--keep-running', action='store_true', help='Keep server running')
        
        # GitHub Actions autoscaler options
        start_parser.add_argument('--disable-autoscaler', action='store_true', 
                                 help='Disable GitHub Actions autoscaler')
        start_parser.add_argument('--autoscaler-owner', type=str,
                                 help='GitHub owner/org to monitor for autoscaler')
        start_parser.add_argument('--autoscaler-interval', type=int, default=60,
                                 help='Autoscaler poll interval in seconds (default: 60)')
        start_parser.add_argument('--autoscaler-since-days', type=int, default=1,
                                 help='Monitor repos updated in last N days (default: 1)')
        start_parser.add_argument('--autoscaler-max-runners', type=int,
                                 help='Max runners for autoscaler (default: system cores)')
        
        # MCP dashboard command
        dashboard_parser = mcp_subparsers.add_parser('dashboard', help='Start dashboard only')
        dashboard_parser.add_argument('--host', default='0.0.0.0', help='Host to bind to (default: 0.0.0.0)')
        dashboard_parser.add_argument('--port', type=int, default=9000, help='Port to bind to (default: 9000)')
        dashboard_parser.add_argument('--open-browser', action='store_true', help='Open browser automatically')
        
        # MCP status command
        status_parser = mcp_subparsers.add_parser('status', help='Check MCP server status')
        status_parser.add_argument('--host', default='0.0.0.0', help='Server host (default: 0.0.0.0)')
        status_parser.add_argument('--port', type=int, default=9000, help='Server port (default: 9000)')
        
        # Dashboard data commands
        user_info_parser = mcp_subparsers.add_parser('user-info', help='Get GitHub user information')
        user_info_parser.add_argument('--json', action='store_true', help='Output as JSON')
        
        cache_stats_parser = mcp_subparsers.add_parser('cache-stats', help='Get cache statistics')
        cache_stats_parser.add_argument('--json', action='store_true', help='Output as JSON')
        
        peer_status_parser = mcp_subparsers.add_parser('peer-status', help='Get P2P peer system status')
        peer_status_parser.add_argument('--json', action='store_true', help='Output as JSON')
        
        metrics_parser = mcp_subparsers.add_parser('metrics', help='Get system metrics')
        metrics_parser.add_argument('--json', action='store_true', help='Output as JSON')
        
        # Logs command
        logs_parser = mcp_subparsers.add_parser('logs', help='Get system logs')
        logs_parser.add_argument('--service', default='ipfs-accelerate', help='Service name (default: ipfs-accelerate)')
        logs_parser.add_argument('--lines', '-n', type=int, default=100, help='Number of lines (default: 100)')
        logs_parser.add_argument('--since', help='Show logs since time (e.g., "1 hour ago")')
        logs_parser.add_argument('--level', choices=['DEBUG', 'INFO', 'WARNING', 'ERROR', 'CRITICAL'], 
                               help='Filter by log level')
        logs_parser.add_argument('--follow', '-f', action='store_true', help='Follow log output')
        logs_parser.add_argument('--errors', action='store_true', help='Show only errors from last 24 hours')
        logs_parser.add_argument('--stats', action='store_true', help='Show log statistics')
        logs_parser.add_argument('--json', action='store_true', help='Output as JSON')
        
        # P2P Workflow commands
        p2p_parser = subparsers.add_parser('p2p-workflow', help='P2P workflow scheduler management')
        p2p_subparsers = p2p_parser.add_subparsers(dest='p2p_command', help='P2P workflow commands')
        
        # P2P status command
        p2p_status_parser = p2p_subparsers.add_parser('status', help='Get P2P scheduler status')
        p2p_status_parser.add_argument('--json', action='store_true', help='Output as JSON')
        
        # P2P submit task command
        p2p_submit_parser = p2p_subparsers.add_parser('submit', help='Submit a task to P2P scheduler')
        p2p_submit_parser.add_argument('--task-id', required=True, help='Task ID')
        p2p_submit_parser.add_argument('--workflow-id', required=True, help='Workflow ID')
        p2p_submit_parser.add_argument('--name', required=True, help='Task name')
        p2p_submit_parser.add_argument('--tags', nargs='+', default=['p2p-eligible'], 
                                      help='Task tags (e.g., p2p-only, code-generation, web-scraping)')
        p2p_submit_parser.add_argument('--priority', type=int, default=5, 
                                      help='Task priority 1-10 (default: 5)')
        
        # P2P get next task command
        p2p_next_parser = p2p_subparsers.add_parser('next', help='Get next task to execute')
        p2p_next_parser.add_argument('--json', action='store_true', help='Output as JSON')
        
        # P2P mark task complete command
        p2p_complete_parser = p2p_subparsers.add_parser('complete', help='Mark task as complete')
        p2p_complete_parser.add_argument('--task-id', required=True, help='Task ID')
        
        # P2P check tags command
        p2p_check_parser = p2p_subparsers.add_parser('check-tags', help='Check if tags should bypass GitHub')
        p2p_check_parser.add_argument('--tags', nargs='+', required=True, help='Tags to check')
        p2p_check_parser.add_argument('--json', action='store_true', help='Output as JSON')
        
        # P2P clock command
        p2p_clock_parser = p2p_subparsers.add_parser('clock', help='Get merkle clock state')
        p2p_clock_parser.add_argument('--json', action='store_true', help='Output as JSON')
        
        # Parse arguments
        args = parser.parse_args()
        
        # Set debug logging if requested
        if args.debug:
            logging.getLogger().setLevel(logging.DEBUG)
            logger.setLevel(logging.DEBUG)
        
        # Handle commands
        if not args.command:
            parser.print_help()
            return 0
            
        cli = IPFSAccelerateCLI()
        
        if args.command == 'mcp':
            if args.mcp_command == 'start':
                return cli.run_mcp_start(args)
            elif args.mcp_command == 'dashboard':
                return cli.run_mcp_dashboard(args)
            elif args.mcp_command == 'status':
                return cli.run_mcp_status(args)
            elif args.mcp_command == 'user-info':
                return cli.run_mcp_user_info(args)
            elif args.mcp_command == 'cache-stats':
                return cli.run_mcp_cache_stats(args)
            elif args.mcp_command == 'peer-status':
                return cli.run_mcp_peer_status(args)
            elif args.mcp_command == 'metrics':
                return cli.run_mcp_metrics(args)
            elif args.mcp_command == 'logs':
                return cli.run_mcp_logs(args)
            else:
                mcp_parser.print_help()
                return 1
        elif args.command == 'p2p-workflow':
            if args.p2p_command == 'status':
                return cli.run_p2p_status(args)
            elif args.p2p_command == 'submit':
                return cli.run_p2p_submit(args)
            elif args.p2p_command == 'next':
                return cli.run_p2p_next(args)
            elif args.p2p_command == 'complete':
                return cli.run_p2p_complete(args)
            elif args.p2p_command == 'check-tags':
                return cli.run_p2p_check_tags(args)
            elif args.p2p_command == 'clock':
                return cli.run_p2p_clock(args)
            else:
                p2p_parser.print_help()
                return 1
        else:
            parser.print_help()
            return 1
            
    except KeyboardInterrupt:
        logger.info("CLI interrupted by user")
        return 0
    except Exception as e:
        logger.error(f"CLI error: {e}")
        return 1


if __name__ == "__main__":
    import sys
    sys.exit(main())
