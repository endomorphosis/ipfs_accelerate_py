# Web Deployment Example for Hugging Face Models

This document provides a complete example of deploying a Hugging Face model to web browsers using the WebNN and WebGPU backends generated by the integrated skillset generator.

## Prerequisites

- A generated skillset implementation (e.g., `hf_bert.py`)
- Node.js 14+ (for transformers.js)
- Python 3.8+ with PyTorch and ONNX
- A modern web browser with WebNN or WebGPU support

## Step 1: Generate the Implementation

First, generate a skillset implementation with web backend support:

```bash
# Generate tests for the model
python generators/test_generators/merged_test_generator.py --generate bert

# Run tests to collect insights
python test/skills/test_hf_bert.py

# Generate implementation with web support
python generators/skill_generators/integrated_skillset_generator.py --model bert --run-tests
```

## Step 2: Export the Model for Web

The generated implementation includes methods for exporting to web formats. Here's a Python script to export a BERT model:

```python
#!/usr/bin/env python3
# export_bert_web.py

import os
import sys
from pathlib import Path

# Add parent directory to path for imports
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# Import the generated skillset
from generated_skillsets.hf_bert import hf_bert

def main():
    # Initialize the model
    bert = hf_bert()
    
    # Export paths
    webnn_dir = Path("web_models/webnn/bert")
    webgpu_dir = Path("web_models/webgpu/bert")
    
    # Create output directories
    webnn_dir.mkdir(parents=True, exist_ok=True)
    webgpu_dir.mkdir(parents=True, exist_ok=True)
    
    print("Exporting BERT for WebNN...")
    # Export for WebNN (via ONNX)
    bert.export_for_webnn(
        model_name="bert-base-uncased",
        output_dir=webnn_dir,
        precision="fp16",  # Use fp16 for better performance
        optimize=True
    )
    
    print("Exporting BERT for WebGPU/transformers.js...")
    # Export for WebGPU/transformers.js
    bert.export_for_webgpu(
        model_name="bert-base-uncased",
        output_dir=webgpu_dir,
        include_tokenizer=True
    )
    
    print(f"Export complete. Models saved to:")
    print(f" - WebNN: {webnn_dir}")
    print(f" - WebGPU: {webgpu_dir}")

if __name__ == "__main__":
    main()
```

## Step 3: Create a Web Application

### WebNN Example

Create an HTML file for the WebNN implementation:

```html
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BERT with WebNN</title>
    <script src="https://cdn.jsdelivr.net/npm/onnxruntime-web/dist/ort.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/webnn-polyfill/dist/webnn-polyfill.js"></script>
</head>
<body>
    <h1>BERT Text Classification with WebNN</h1>
    
    <div class="container">
        <textarea id="input-text" rows="4" cols="50" placeholder="Enter text to classify..."></textarea>
        <button id="classify-btn">Classify</button>
        
        <div id="results" class="results">
            <h3>Results:</h3>
            <pre id="output"></pre>
        </div>
    </div>

    <script>
        // Load the model
        async function loadModel() {
            try {
                // Check for WebNN support
                if (!navigator.ml || !navigator.ml.getNeuralNetworkContext) {
                    console.log("WebNN not available, using polyfill");
                    // Polyfill will be used automatically
                }
                
                const context = await navigator.ml.getNeuralNetworkContext();
                const model = await fetch('web_models/webnn/bert/model.onnx')
                    .then(response => response.arrayBuffer());
                
                // Load tokenizer vocabulary
                const vocab = await fetch('web_models/webnn/bert/vocab.json')
                    .then(response => response.json());
                
                // Compile the model for WebNN
                const compiledModel = await context.compile({
                    operations: [
                        {
                            type: 'graph',
                            inputs: ['input_ids', 'attention_mask', 'token_type_ids'],
                            outputs: ['logits'],
                            operands: [...new Uint8Array(model)]
                        }
                    ]
                });
                
                return {
                    model: compiledModel,
                    vocab: vocab
                };
            } catch (error) {
                console.error("Error loading model:", error);
                document.getElementById('output').textContent = 
                    `Error loading model: ${error.message}`;
                throw error;
            }
        }
        
        // Tokenize input text
        function tokenize(text, vocab, maxLength = 128) {
            const tokens = text.toLowerCase().split(/\s+/);
            const tokenIds = tokens.map(token => vocab[token] || vocab['[UNK]']);
            
            // Truncate or pad to maxLength
            const paddedIds = tokenIds.slice(0, maxLength);
            while (paddedIds.length < maxLength) {
                paddedIds.push(0); // Padding token
            }
            
            // Create attention mask (1 for real tokens, 0 for padding)
            const attentionMask = tokenIds.map((_, i) => i < tokenIds.length ? 1 : 0);
            
            // Create token type ids (all 0 for single sentence)
            const tokenTypeIds = new Array(maxLength).fill(0);
            
            return {
                input_ids: paddedIds,
                attention_mask: attentionMask,
                token_type_ids: tokenTypeIds
            };
        }
        
        // Run inference
        async function runInference(model, vocab, text) {
            try {
                // Tokenize the input
                const tokenized = tokenize(text, vocab);
                
                // Prepare inputs for the model
                const inputs = {
                    'input_ids': new Float32Array(tokenized.input_ids),
                    'attention_mask': new Float32Array(tokenized.attention_mask),
                    'token_type_ids': new Float32Array(tokenized.token_type_ids)
                };
                
                // Run the model
                const outputs = await model.compute(inputs);
                const logits = outputs['logits'];
                
                // Process the output (example for binary classification)
                const score = logits[0];
                return {
                    positive: score > 0 ? score : 0,
                    negative: score < 0 ? -score : 0,
                    raw: score
                };
            } catch (error) {
                console.error("Error during inference:", error);
                throw error;
            }
        }
        
        // Initialize the application
        async function init() {
            try {
                // Load the model
                const { model, vocab } = await loadModel();
                console.log("Model loaded successfully");
                
                // Set up the classify button
                document.getElementById('classify-btn').addEventListener('click', async () => {
                    const text = document.getElementById('input-text').value;
                    if (!text) {
                        alert("Please enter some text");
                        return;
                    }
                    
                    // Show loading state
                    const outputElement = document.getElementById('output');
                    outputElement.textContent = "Classifying...";
                    
                    try {
                        // Run inference
                        const result = await runInference(model, vocab, text);
                        
                        // Display the result
                        outputElement.textContent = JSON.stringify(result, null, 2);
                    } catch (error) {
                        outputElement.textContent = `Error: ${error.message}`;
                    }
                });
            } catch (error) {
                console.error("Initialization error:", error);
            }
        }
        
        // Start the application when the page loads
        window.addEventListener('DOMContentLoaded', init);
    </script>
</body>
</html>
```

### WebGPU/transformers.js Example

Create an HTML file for the WebGPU/transformers.js implementation:

```html
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BERT with WebGPU/transformers.js</title>
    <script src="https://cdn.jsdelivr.net/npm/@xenova/transformers@2.6.0"></script>
</head>
<body>
    <h1>BERT Text Classification with WebGPU/transformers.js</h1>
    
    <div class="container">
        <textarea id="input-text" rows="4" cols="50" placeholder="Enter text to classify..."></textarea>
        <button id="classify-btn">Classify</button>
        
        <div id="results" class="results">
            <h3>Results:</h3>
            <pre id="output"></pre>
        </div>
    </div>

    <script>
        // Import from transformers.js
        const { pipeline, env } = window.transformers;
        
        // Configure transformers.js to use WebGPU if available
        env.useBrowserCache = true;
        env.useWebGPU = true;
        
        // Store the classifier globally
        let classifier;
        
        // Initialize the classifier
        async function initClassifier() {
            try {
                // Either load from local path or Hugging Face Hub
                const modelPath = 'web_models/webgpu/bert';
                
                // Create a text classification pipeline
                classifier = await pipeline('text-classification', modelPath, {
                    quantized: false, // Use full precision
                    revision: 'main'
                });
                
                console.log("Model loaded successfully");
                return classifier;
            } catch (error) {
                console.error("Error loading model:", error);
                document.getElementById('output').textContent = 
                    `Error loading model: ${error.message}`;
                throw error;
            }
        }
        
        // Run classification
        async function classifyText(text) {
            if (!classifier) {
                await initClassifier();
            }
            
            try {
                // Run classification
                const result = await classifier(text);
                return result;
            } catch (error) {
                console.error("Classification error:", error);
                throw error;
            }
        }
        
        // Initialize the application
        async function init() {
            try {
                // Initialize the classifier in the background
                initClassifier();
                
                // Set up the classify button
                document.getElementById('classify-btn').addEventListener('click', async () => {
                    const text = document.getElementById('input-text').value;
                    if (!text) {
                        alert("Please enter some text");
                        return;
                    }
                    
                    // Show loading state
                    const outputElement = document.getElementById('output');
                    outputElement.textContent = "Classifying...";
                    
                    try {
                        // Run classification
                        const result = await classifyText(text);
                        
                        // Display the result
                        outputElement.textContent = JSON.stringify(result, null, 2);
                    } catch (error) {
                        outputElement.textContent = `Error: ${error.message}`;
                    }
                });
            } catch (error) {
                console.error("Initialization error:", error);
            }
        }
        
        // Start the application when the page loads
        window.addEventListener('DOMContentLoaded', init);
    </script>
</body>
</html>
```

## Step 4: Serve the Web Application

Create a simple server to host the web application:

```bash
# Install a simple HTTP server
npm install -g http-server

# Serve the application
http-server . -p 8080
```

Now you can access your application at http://localhost:8080

## Performance Optimization Tips

### WebNN Optimization

1. **Quantization**: Use int8 quantization for faster inference:
   ```python
   bert.export_for_webnn(
       model_name="bert-base-uncased",
       output_dir=webnn_dir,
       precision="int8",
       optimize=True
   )
   ```

2. **Model Pruning**: Use smaller models like MobileBERT or TinyBERT:
   ```python
   bert.export_for_webnn(
       model_name="google/mobilebert-uncased",
       output_dir=webnn_dir
   )
   ```

3. **Operator Fusion**: Enable operator fusion for WebNN optimization:
   ```python
   bert.export_for_webnn(
       model_name="bert-base-uncased",
       output_dir=webnn_dir,
       operator_fusion=True
   )
   ```

### WebGPU/transformers.js Optimization

1. **Caching**: Enable model caching for faster loading:
   ```javascript
   env.useBrowserCache = true;
   env.cacheVersion = '1.0'; // Update when model changes
   ```

2. **Batching**: Batch multiple inputs for better throughput:
   ```javascript
   const results = await classifier(['Text 1', 'Text 2', 'Text 3']);
   ```

3. **Quantization**: Use 8-bit quantization for faster inference:
   ```javascript
   classifier = await pipeline('text-classification', modelPath, {
       quantized: true
   });
   ```

## Browser Compatibility

| Browser | WebNN Support | WebGPU Support | Notes |
|---------|---------------|----------------|-------|
| Chrome 113+ | ✅ | ✅ | Best performance for both backends |
| Edge 113+ | ✅ | ✅ | Based on Chromium, similar to Chrome |
| Safari 16.4+ | ❌ (Polyfill) | ✅ | Native WebGPU, polyfill for WebNN |
| Firefox 115+ | ❌ (Polyfill) | ✅ (Experimental) | Enable WebGPU in about:config |
| Opera 99+ | ✅ | ✅ | Based on Chromium |
| Mobile Chrome | ✅ | ✅ | Performance varies by device |
| Mobile Safari | ❌ (Polyfill) | ✅ | iOS 16.4+ for WebGPU |

## Troubleshooting

### Common Issues

1. **WebNN Not Available**: If you see "WebNN not available, using polyfill", the browser doesn't support WebNN natively. The polyfill will provide compatibility but with lower performance.

2. **WebGPU Not Available**: If transformers.js fails to use WebGPU, it automatically falls back to WebGL or CPU. Check browser compatibility and ensure hardware acceleration is enabled.

3. **Memory Issues**: Large models may exceed browser memory limits. Try:
   - Using smaller models
   - Enabling quantization
   - Reducing batch size
   - Breaking input into smaller chunks

4. **CORS Errors**: When loading models, you may encounter CORS issues. Ensure your server sets proper CORS headers:
   ```
   Access-Control-Allow-Origin: *
   ```

5. **Model Loading Timeout**: Large models may time out during loading. Increase the timeout:
   ```javascript
   env.timeout = 120000; // 2 minutes
   ```

## Next Steps

- **Progressive Enhancement**: Implement fallback for browsers without WebNN/WebGPU
- **Worker Threads**: Move inference to a Web Worker for better UI responsiveness
- **SharedArrayBuffer**: Use shared buffers for more efficient memory handling
- **Service Worker**: Cache models using Service Workers for offline operation
- **Streaming Inference**: Implement streaming for long inputs
- **Advanced Optimization**: Apply model distillation or pruning for smaller models