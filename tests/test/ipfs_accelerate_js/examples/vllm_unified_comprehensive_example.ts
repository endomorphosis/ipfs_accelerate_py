/**
 * VLLM Unified API Backend Comprehensive Example
 * 
 * This example demonstrates the full range of capabilities offered by the VLLM Unified API backend,
 * including text generation, chat interfaces, streaming, batch processing, LoRA adapter management,
 * quantization settings, Docker container management, and advanced error handling.
 * 
 * VLLM is a high-performance inference engine for large language models that provides:
 * - Continuous batching for efficient processing
 * - PagedAttention for memory-efficient KV cache management
 * - Tensor parallelism for distributed inference across multiple GPUs
 * - Quantization support (AWQ, SqueezeLLM, etc.)
 * - Efficient streaming for responsive UIs
 * 
 * The Unified API backend adds enhanced functionality:
 * - Dual-mode operation (API and container modes)
 * - Circuit breaker pattern for resilience
 * - Resource pooling and connection management
 * - Advanced error handling and monitoring
 * - Performance benchmarking and optimization tools
 * - Docker container management and health monitoring
 * 
 * To run this example:
 * 1. Build the SDK: npm run build
 * 2. Run the example: node dist/examples/vllm_unified_comprehensive_example.js
 * 
 * If you don't have a running VLLM server, the example will use mock implementations
 * for demonstration purposes.
 */

import { VllmUnified } from '../src/api_backends/vllm_unified/vllm_unified';
import { 
  VllmRequest, 
  VllmUnifiedResponse, 
  VllmLoraAdapter, 
  VllmQuantizationConfig, 
  VllmContainerConfig,
  VllmContainerStatus,
  VllmContainerMetrics
} from '../src/api_backends/vllm_unified/types';
import { Message, ChatCompletionResponse } from '../src/api_backends/types';
import { EventEmitter } from 'events';

// API URLs and model configuration
const DEFAULT_API_URL = process.env.VLLM_API_URL || 'http://localhost:8000';
const DEFAULT_MODEL = process.env.VLLM_MODEL || 'meta-llama/Llama-2-7b-chat-hf';
const SMALL_MODEL = 'facebook/opt-125m'; // For quick, lightweight examples
const MAX_TOKENS = 100;

/**
 * Sleep utility for demonstration purposes
 */
const sleep = (ms: number) => new Promise(resolve => setTimeout(resolve, ms));

/**
 * Format the output of an example section
 */
const formatExampleOutput = (title: string, output: any) => {
  console.log(`\n${'='.repeat(80)}`);
  console.log(`${title}`);
  console.log(`${'='.repeat(80)}`);
  
  if (typeof output === 'string') {
    console.log(output);
  } else {
    console.log(JSON.stringify(output, null, 2));
  }
};

/**
 * Mock implementation for examples where a real VLLM server is not available
 */
class MockVllmImplementation {
  // Mock implementations of API methods
  static mockTextGeneration(prompt: string): string {
    if (prompt.includes('error')) {
      throw new Error('Simulated error in text generation');
    }
    
    const responses = {
      'dragons': 'Once upon a time, in a land far away, there lived a magnificent dragon with scales that shimmered like emeralds in the sunlight. The dragon, named Emeralda, was known for her wisdom and kindness, unlike the fearsome reputation most dragons had.',
      'science': 'Science is the systematic study of the natural world through observation, experimentation, and the formulation of theories. It is a method of inquiry that has led to countless discoveries and technological advancements throughout human history.',
      'quantum': 'Quantum computing leverages the principles of quantum mechanics to process information in ways that classical computers cannot. Instead of using bits that are either 0 or 1, quantum computers use quantum bits or "qubits" that can exist in multiple states simultaneously.',
      'AI': 'Artificial Intelligence (AI) refers to computer systems designed to perform tasks that typically require human intelligence. These tasks include learning, reasoning, problem-solving, perception, and language understanding.'
    };
    
    // Generate a response based on keywords in the prompt
    for (const [keyword, response] of Object.entries(responses)) {
      if (prompt.toLowerCase().includes(keyword.toLowerCase())) {
        return response;
      }
    }
    
    // Default response if no keywords match
    return `This is a mock response to: "${prompt}". In a production environment, this would be generated by the VLLM server.`;
  }
  
  static mockChat(messages: Message[]): string {
    const lastUserMessage = messages.filter(m => m.role === 'user').pop();
    
    if (!lastUserMessage) {
      return "I don't see a user message to respond to.";
    }
    
    return MockVllmImplementation.mockTextGeneration(lastUserMessage.content);
  }
  
  static mockLoraAdapters(): VllmLoraAdapter[] {
    return [
      { id: 'adapter1', name: 'Fiction Writing', base_model: 'llama-7b', size_mb: 42, active: true },
      { id: 'adapter2', name: 'Medical Knowledge', base_model: 'llama-7b', size_mb: 38, active: false },
      { id: 'adapter3', name: 'Code Generation', base_model: 'llama-13b', size_mb: 51, active: false }
    ];
  }
  
  static mockContainerMetrics(): VllmContainerMetrics {
    return {
      cpu_usage: 35.2,
      memory_usage: 4200,
      gpu_memory_usage: 7850,
      requests_processed: 1250,
      tokens_generated: 187500,
      average_latency: 128.5,
      throughput: 75.3
    };
  }
}

/**
 * Production-ready LLM System with comprehensive features
 */
class ProductionLlmSystem extends EventEmitter {
  private vllm: VllmUnified;
  private active: boolean = false;
  private metrics: any = {
    requests: 0,
    success: 0,
    errors: 0,
    latency: []
  };
  private mockMode: boolean = true;

  constructor(config: any = {}) {
    super();
    
    // Initialize with container support and circuit breaker
    this.vllm = new VllmUnified({}, {
      vllm_container_enabled: config.containerEnabled ?? true,
      vllm_container_gpu: config.containerGpu ?? true,
      vllm_api_url: config.apiUrl ?? 'http://localhost:8000',
      vllm_model: config.model ?? 'meta-llama/Llama-2-7b-chat-hf',
      maxRetries: config.maxRetries ?? 3,
      timeout: config.timeout ?? 60000,
      circuitBreaker: {
        failureThreshold: config.failureThreshold ?? 5,
        resetTimeout: config.resetTimeout ?? 30000
      }
    });
    
    this.mockMode = config.mockMode ?? true;
    
    // Set up event listeners
    this.vllm.on('container_started', this.handleContainerStarted.bind(this));
    this.vllm.on('container_error', this.handleContainerError.bind(this));
    this.vllm.on('circuit_open', this.handleCircuitOpen.bind(this));
    this.vllm.on('circuit_closed', this.handleCircuitClosed.bind(this));
  }
  
  // Start the system
  async start() {
    this.active = true;
    
    if (!this.mockMode) {
      try {
        await this.vllm.startContainer();
        this.vllm.startContainerMonitoring({ interval: 5000 });
        
        // Validate container capabilities
        const capabilities = await this.vllm.validateContainerCapabilities();
        if (!capabilities.api_accessible) {
          throw new Error('API not accessible after container start');
        }
      } catch (error) {
        console.warn('Container startup failed, falling back to mock mode:', error);
        this.mockMode = true;
      }
    }
    
    this.emit('system_ready');
    return true;
  }
  
  // Stop the system
  async stop() {
    this.active = false;
    
    if (!this.mockMode) {
      this.vllm.stopContainerMonitoring();
      await this.vllm.stopContainer();
    }
    
    this.emit('system_stopped');
    return true;
  }
  
  // Generate text with robust error handling
  async generateText(prompt: string, options: any = {}) {
    if (!this.active) {
      throw new Error('System not active');
    }
    
    const startTime = Date.now();
    this.metrics.requests++;
    
    try {
      let result;
      
      if (this.mockMode) {
        // Use mock implementation
        const mockText = MockVllmImplementation.mockTextGeneration(prompt);
        result = {
          text: mockText,
          metadata: {
            finish_reason: 'length',
            model: options.model || 'meta-llama/Llama-2-7b-chat-hf',
            usage: {
              prompt_tokens: prompt.split(' ').length,
              completion_tokens: mockText.split(' ').length,
              total_tokens: prompt.split(' ').length + mockText.split(' ').length
            }
          }
        };
      } else {
        // Use actual VLLM server
        result = await this.vllm.makeRequest(
          'http://localhost:8000',
          prompt,
          options.model || 'meta-llama/Llama-2-7b-chat-hf',
          options
        );
      }
      
      // Record metrics
      this.metrics.success++;
      this.metrics.latency.push(Date.now() - startTime);
      
      return result;
    } catch (error) {
      this.metrics.errors++;
      this.emit('request_error', { prompt, error });
      throw error;
    }
  }
  
  // Chat completion with robust error handling
  async chat(messages: Message[], options: any = {}) {
    if (!this.active) {
      throw new Error('System not active');
    }
    
    const startTime = Date.now();
    this.metrics.requests++;
    
    try {
      let result;
      
      if (this.mockMode) {
        // Use mock implementation
        const mockResponse = MockVllmImplementation.mockChat(messages);
        result = {
          content: mockResponse,
          role: 'assistant',
          model: options.model || 'meta-llama/Llama-2-7b-chat-hf'
        };
      } else {
        // Use actual VLLM server
        result = await this.vllm.chat(messages, options);
      }
      
      // Record metrics
      this.metrics.success++;
      this.metrics.latency.push(Date.now() - startTime);
      
      return result;
    } catch (error) {
      this.metrics.errors++;
      this.emit('request_error', { messages, error });
      throw error;
    }
  }
  
  // Get LoRA adapters
  async getLoraAdapters() {
    if (this.mockMode) {
      return MockVllmImplementation.mockLoraAdapters();
    } else {
      return await this.vllm.listLoraAdapters('http://localhost:8000');
    }
  }
  
  // Get container metrics
  async getContainerMetrics() {
    if (this.mockMode) {
      return MockVllmImplementation.mockContainerMetrics();
    } else {
      return await this.vllm.getContainerMetrics();
    }
  }
  
  // Get system metrics
  getMetrics() {
    const avgLatency = this.metrics.latency.length > 0
      ? this.metrics.latency.reduce((a: number, b: number) => a + b, 0) / this.metrics.latency.length
      : 0;
    
    return {
      requests: this.metrics.requests,
      success: this.metrics.success,
      errors: this.metrics.errors,
      error_rate: this.metrics.requests > 0 
        ? (this.metrics.errors / this.metrics.requests) * 100 
        : 0,
      avg_latency: avgLatency,
      container_status: this.mockMode ? 'mock' : this.vllm.getContainerStatus(),
      active: this.active,
      mock_mode: this.mockMode
    };
  }
  
  // Event handlers
  private handleContainerStarted(containerId: string) {
    this.emit('container_started', containerId);
  }
  
  private handleContainerError(error: Error) {
    this.emit('container_error', error);
    if (this.active && !this.mockMode) {
      // Attempt recovery
      setTimeout(() => {
        this.vllm.restartContainer().catch(e => {
          this.emit('recovery_failed', e);
        });
      }, 5000);
    }
  }
  
  private handleCircuitOpen() {
    this.emit('circuit_open');
  }
  
  private handleCircuitClosed() {
    this.emit('circuit_closed');
  }
}

/**
 * Main demo function that showcases all the VLLM Unified backend capabilities
 */
async function runVllmUnifiedDemo() {
  console.log('Starting VLLM Unified API Backend Comprehensive Example...');
  console.log('This example demonstrates all capabilities of the VLLM Unified backend.');
  console.log('Using mock mode for demonstration purposes.');
  
  try {
    // Create the production LLM system
    const llmSystem = new ProductionLlmSystem({
      mockMode: true, // Use mock mode for demonstration
      model: SMALL_MODEL
    });
    
    // Set up event listeners
    llmSystem.on('system_ready', () => {
      console.log('✅ LLM system is ready');
    });
    
    llmSystem.on('request_error', ({ error }: {error: Error}) => {
      console.error('❌ Request error:', error.message);
    });
    
    // Start the system
    await llmSystem.start();
    
    // 1. Basic Text Generation
    const textResult = await llmSystem.generateText(
      'Explain quantum computing in simple terms.',
      { temperature: 0.7, max_tokens: MAX_TOKENS }
    );
    formatExampleOutput('1. Basic Text Generation', 
      `Prompt: "Explain quantum computing in simple terms."\n\nResponse: "${textResult.text}"`
    );
    
    // 2. Chat Completion
    const chatResult = await llmSystem.chat(
      [
        { role: 'system', content: 'You are a helpful assistant specialized in science.' },
        { role: 'user', content: 'What are the main challenges in AI safety?' }
      ],
      { temperature: 0.7, max_tokens: MAX_TOKENS }
    );
    formatExampleOutput('2. Chat Completion',
      `System: "You are a helpful assistant specialized in science."\nUser: "What are the main challenges in AI safety?"\n\nAssistant: "${chatResult.content}"`
    );
    
    // 3. LoRA Adapters
    const loraAdapters = await llmSystem.getLoraAdapters();
    formatExampleOutput('3. LoRA Adapters', loraAdapters);
    
    // 4. Container Metrics
    const containerMetrics = await llmSystem.getContainerMetrics();
    formatExampleOutput('4. Container Metrics', containerMetrics);
    
    // 5. System Metrics
    const metrics = llmSystem.getMetrics();
    formatExampleOutput('5. System Metrics', metrics);
    
    // 6. Batch Processing Simulation
    const prompts = [
      'What is the capital of France?',
      'What is the capital of Italy?',
      'What is the capital of Germany?'
    ];
    
    const batchResults = [];
    for (const prompt of prompts) {
      const result = await llmSystem.generateText(prompt, { temperature: 0.1 });
      batchResults.push(result.text);
    }
    
    const batchOutput = prompts.map((prompt, i) => 
      `Prompt: "${prompt}"\nResponse: "${batchResults[i]}"`
    ).join('\n\n');
    
    formatExampleOutput('6. Batch Processing', batchOutput);
    
    // 7. Error Handling Demonstration
    try {
      await llmSystem.generateText('This will cause an error', { model: 'non-existent-model' });
    } catch (error: any) {
      formatExampleOutput('7. Error Handling', `Error handled: ${error.message}`);
    }
    
    // 8. Production System Demonstration
    formatExampleOutput('8. Production System Features', `
The VLLM Unified backend includes production-ready features:

✅ Automatic container management
✅ High availability with failover
✅ Rate limiting and request queuing
✅ Circuit breaker pattern for resilience
✅ Exponential backoff for retries
✅ Detailed monitoring and metrics
✅ LoRA adapter management
✅ Quantization control
✅ Batch processing
✅ Streaming support

The ProductionLlmSystem class demonstrates how to build a robust LLM system using the VLLM Unified backend.
It includes event handling, metrics collection, and automatic recovery from failures.
`);
    
    // Stop the system
    await llmSystem.stop();
    
    console.log(`\n${'='.repeat(80)}`);
    console.log('VLLM Unified API Backend Comprehensive Example Completed');
    console.log(`${'='.repeat(80)}`);
    
  } catch (error) {
    console.error('Demo error:', error);
  }
}

// Run the demo
runVllmUnifiedDemo().catch(console.error);

/**
 * This example demonstrates the comprehensive capabilities of the VLLM Unified API backend.
 * In a real-world scenario, the mockMode would be set to false, and the system would connect
 * to an actual VLLM server or manage Docker containers automatically.
 * 
 * For more advanced use cases, refer to the documentation at:
 * /docs/api_backends/VLLM_UNIFIED_USAGE.md
 */