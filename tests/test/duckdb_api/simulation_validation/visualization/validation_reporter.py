#!/usr/bin/env python3
"""
Validation Reporter implementation for the Simulation Accuracy and Validation Framework.

This module provides a concrete implementation of the ValidationReporter interface
that generates reports and visualizations of validation results, with support for
multiple formats, embedded visualizations, executive summaries, and detailed
technical analysis.
"""

import os
import logging
import json
import csv
from typing import Dict, List, Any, Optional, Union, Tuple, Set
from pathlib import Path
import datetime
import uuid
import statistics
import math
import re
import tempfile
import shutil
import base64
from collections import defaultdict

# Configure logging
logging.basicConfig(level=logging.INFO,
                    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger("validation_reporter")

# Import base classes
from duckdb_api.simulation_validation.core.base import (
    ValidationResult,
    ValidationReporter
)

# Optional imports for enhanced reporting
try:
    import numpy as np
    import scipy.stats
    NUMPY_AVAILABLE = True
except ImportError:
    logger.warning("NumPy and/or SciPy not available. Statistical analysis features will be limited.")
    NUMPY_AVAILABLE = False

try:
    import matplotlib
    matplotlib.use('Agg')  # Use non-interactive backend
    import matplotlib.pyplot as plt
    import matplotlib.dates as mdates
    MATPLOTLIB_AVAILABLE = True
except ImportError:
    logger.warning("Matplotlib not available. Static visualization features will be limited.")
    MATPLOTLIB_AVAILABLE = False

try:
    import plotly.graph_objects as go
    import plotly.express as px
    import plotly.io as pio
    from plotly.subplots import make_subplots
    PLOTLY_AVAILABLE = True
except ImportError:
    logger.warning("Plotly not available. Interactive visualization features will be limited.")
    PLOTLY_AVAILABLE = False


class ValidationReporterImpl(ValidationReporter):
    """
    Enhanced implementation of a validation reporter that generates comprehensive reports and visualizations.
    
    This reporter supports HTML, Markdown, JSON, CSV, and PDF formats for reports,
    and can include interactive visualizations, executive summaries, detailed
    technical analyses, and comparative reports for tracking improvements over time.
    """
    
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        """
        Initialize the validation reporter.
        
        Args:
            config: Configuration options for the reporter
        """
        self.config = config or {}
        
        # Default configuration
        self.default_config = {
            "report_formats": ["html", "markdown", "json", "csv", "pdf"],
            "include_visualizations": True,
            "visualization_types": [
                "error_distribution", 
                "trend_chart", 
                "metric_heatmap",
                "statistical_analysis",
                "confidence_intervals",
                "prediction_vs_actual",
                "regression_analysis",
                "calibration_effectiveness",
                "drift_detection",
                "parameter_sensitivity"
            ],
            "max_results_per_page": 20,
            "output_directory": "output",
            "report_title_template": "Simulation Validation Report - {timestamp}",
            "css_style_path": None,
            "html_template_path": None,
            "executive_summary": True,
            "technical_details": True,
            "statistical_confidence_level": 0.95,  # 95% confidence level
            "visualization_width": 800,
            "visualization_height": 500,
            "interactive_visualizations": True,
            "report_sections": [
                "executive_summary",
                "overview",
                "hardware_comparison",
                "model_comparison",
                "metric_analysis",
                "statistical_analysis",
                "detailed_results",
                "recommendations",
                "appendix"
            ],
            "cache_visualizations": True,
            "cache_directory": ".visualization_cache",
            "visualization_format": "png",  # or svg
            "auto_refresh_interval": None,  # For HTML reports
            "comparative_analysis": True,
            "report_theme": "light",  # or dark
            "report_watermark": None,
            "report_footer": "Generated by Simulation Accuracy and Validation Framework",
            "report_logo_path": None,
            "report_author": "Simulation Validation System",
            "pdf_engine": "weasyprint"  # or "wkhtmltopdf"
        }
        
        # Apply default config values if not specified
        for key, value in self.default_config.items():
            if key not in self.config:
                self.config[key] = value
                
        # Initialize visualization cache if enabled
        if self.config["cache_visualizations"]:
            self._initialize_visualization_cache()
            
        # Visualization cache to prevent regenerating the same charts
        self.visualization_cache = {}
            
    def _initialize_visualization_cache(self):
        """Initialize the visualization cache directory."""
        cache_dir = self.config["cache_directory"]
        if not os.path.exists(cache_dir):
            try:
                os.makedirs(cache_dir, exist_ok=True)
                logger.info(f"Created visualization cache directory: {cache_dir}")
            except Exception as e:
                logger.warning(f"Could not create visualization cache directory: {e}")
                self.config["cache_visualizations"] = False
                
    def _get_cache_key(self, visualization_type: str, params: Dict[str, Any]) -> str:
        """
        Generate a cache key for a visualization based on its type and parameters.
        
        Args:
            visualization_type: Type of visualization
            params: Parameters used to generate the visualization
            
        Returns:
            Cache key as a string
        """
        # Convert parameters to a sorted, stringified representation
        param_str = json.dumps(params, sort_keys=True)
        # Generate a hash of the parameters
        param_hash = hash(param_str) % 10000000  # Limit to 7 digits
        # Combine visualization type and parameter hash
        return f"{visualization_type}_{param_hash}"
    
    def _cache_visualization(self, cache_key: str, content: str, format: str) -> None:
        """
        Cache a visualization.
        
        Args:
            cache_key: Cache key for the visualization
            content: Visualization content (HTML, SVG, or base64-encoded image)
            format: Format of the visualization (html, svg, png, etc.)
        """
        if not self.config["cache_visualizations"]:
            return
            
        try:
            cache_dir = self.config["cache_directory"]
            cache_path = os.path.join(cache_dir, f"{cache_key}.{format}")
            
            with open(cache_path, 'w' if format in ['html', 'svg'] else 'wb') as f:
                if format in ['html', 'svg']:
                    f.write(content)
                else:
                    # Assume base64-encoded image
                    f.write(base64.b64decode(content))
                    
            # Also store in memory cache
            self.visualization_cache[cache_key] = {
                'content': content,
                'format': format,
                'timestamp': datetime.datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.warning(f"Could not cache visualization: {e}")
    
    def _get_cached_visualization(self, cache_key: str, format: str) -> Optional[str]:
        """
        Retrieve a cached visualization.
        
        Args:
            cache_key: Cache key for the visualization
            format: Format of the visualization (html, svg, png, etc.)
            
        Returns:
            Cached visualization content or None if not found
        """
        if not self.config["cache_visualizations"]:
            return None
            
        # Check memory cache first
        if cache_key in self.visualization_cache:
            cached = self.visualization_cache[cache_key]
            if cached['format'] == format:
                return cached['content']
        
        # Check file cache
        try:
            cache_dir = self.config["cache_directory"]
            cache_path = os.path.join(cache_dir, f"{cache_key}.{format}")
            
            if os.path.exists(cache_path):
                with open(cache_path, 'r' if format in ['html', 'svg'] else 'rb') as f:
                    if format in ['html', 'svg']:
                        content = f.read()
                    else:
                        # Encode image as base64
                        content = base64.b64encode(f.read()).decode('utf-8')
                        
                # Store in memory cache
                self.visualization_cache[cache_key] = {
                    'content': content,
                    'format': format,
                    'timestamp': datetime.datetime.now().isoformat()
                }
                
                return content
        except Exception as e:
            logger.warning(f"Could not retrieve cached visualization: {e}")
            
        return None
        
    def _create_error_distribution_chart(
        self, 
        validation_results: List[ValidationResult],
        metric_name: Optional[str] = None,
        interactive: Optional[bool] = None
    ) -> str:
        """
        Create an error distribution chart.
        
        Args:
            validation_results: List of validation results
            metric_name: Name of the metric to visualize (or None for all metrics)
            interactive: Whether to create an interactive visualization
            
        Returns:
            Visualization content (HTML or base64-encoded image)
        """
        # Determine if visualization should be interactive
        if interactive is None:
            interactive = self.config["interactive_visualizations"]
            
        # Use the cache key for this visualization
        cache_params = {
            'validation_result_ids': [id(vr) for vr in validation_results],
            'metric_name': metric_name,
            'interactive': interactive
        }
        cache_key = self._get_cache_key('error_distribution', cache_params)
        
        # Check if cached version exists
        format = 'html' if interactive else self.config["visualization_format"]
        cached = self._get_cached_visualization(cache_key, format)
        if cached:
            return cached
            
        # Create the visualization
        if interactive and PLOTLY_AVAILABLE:
            return self._create_interactive_error_distribution(validation_results, metric_name)
        elif MATPLOTLIB_AVAILABLE:
            return self._create_static_error_distribution(validation_results, metric_name)
        else:
            logger.warning("Cannot create error distribution chart: No visualization library available")
            return ""
            
    def _create_interactive_error_distribution(
        self,
        validation_results: List[ValidationResult],
        metric_name: Optional[str] = None
    ) -> str:
        """
        Create an interactive error distribution chart using Plotly.
        
        Args:
            validation_results: List of validation results
            metric_name: Name of the metric to visualize (or None for all metrics)
            
        Returns:
            HTML content for the interactive visualization
        """
        if not PLOTLY_AVAILABLE:
            return ""
            
        # Extract error values
        error_values = []
        metric_names = []
        hardware_ids = []
        model_ids = []
        
        for vr in validation_results:
            hw_id = vr.hardware_result.hardware_id
            model_id = vr.hardware_result.model_id
            
            if metric_name is None:
                # Include all metrics
                for metric, comparison in vr.metrics_comparison.items():
                    if "mape" in comparison:
                        error_values.append(comparison["mape"])
                        metric_names.append(metric)
                        hardware_ids.append(hw_id)
                        model_ids.append(model_id)
            elif metric_name in vr.metrics_comparison:
                # Include only the specified metric
                comparison = vr.metrics_comparison[metric_name]
                if "mape" in comparison:
                    error_values.append(comparison["mape"])
                    metric_names.append(metric_name)
                    hardware_ids.append(hw_id)
                    model_ids.append(model_id)
        
        if not error_values:
            logger.warning("No error values to visualize")
            return ""
            
        # Create a DataFrame for Plotly Express
        data = {
            "Error (MAPE)": error_values,
            "Metric": metric_names,
            "Hardware": hardware_ids,
            "Model": model_ids
        }
        
        try:
            import pandas as pd
            df = pd.DataFrame(data)
            
            # Create the visualization
            fig = px.histogram(
                df, 
                x="Error (MAPE)", 
                color="Metric" if metric_name is None else None,
                facet_row="Hardware" if len(set(hardware_ids)) > 1 else None,
                facet_col="Model" if len(set(model_ids)) > 1 else None,
                title="Error Distribution (MAPE)",
                labels={"Error (MAPE)": "Mean Absolute Percentage Error (%)"},
                opacity=0.7,
                marginal="box",
                template="plotly_white" if self.config["report_theme"] == "light" else "plotly_dark"
            )
            
            # Add statistics as annotations
            if NUMPY_AVAILABLE:
                mean_error = np.mean(error_values)
                median_error = np.median(error_values)
                std_error = np.std(error_values)
                
                stats_text = (
                    f"Mean: {mean_error:.2f}%<br>"
                    f"Median: {median_error:.2f}%<br>"
                    f"Std Dev: {std_error:.2f}%"
                )
                
                fig.add_annotation(
                    x=0.02,
                    y=0.98,
                    xref="paper",
                    yref="paper",
                    text=stats_text,
                    showarrow=False,
                    font=dict(size=12),
                    bgcolor="rgba(255, 255, 255, 0.8)",
                    bordercolor="rgba(0, 0, 0, 0.2)",
                    borderwidth=1,
                    borderpad=4
                )
            
            # Customize layout
            fig.update_layout(
                height=self.config["visualization_height"],
                width=self.config["visualization_width"],
                autosize=True,
                margin=dict(l=50, r=50, t=80, b=50)
            )
            
            # Generate HTML
            html = fig.to_html(full_html=False, include_plotlyjs='cdn')
            
            # Cache the visualization
            cache_params = {
                'validation_result_ids': [id(vr) for vr in validation_results],
                'metric_name': metric_name,
                'interactive': True
            }
            cache_key = self._get_cache_key('error_distribution', cache_params)
            self._cache_visualization(cache_key, html, 'html')
            
            return html
            
        except ImportError:
            logger.warning("Pandas not available for interactive error distribution")
            return ""
        except Exception as e:
            logger.error(f"Error creating interactive error distribution: {e}")
            return ""
            
    def _create_static_error_distribution(
        self,
        validation_results: List[ValidationResult],
        metric_name: Optional[str] = None
    ) -> str:
        """
        Create a static error distribution chart using Matplotlib.
        
        Args:
            validation_results: List of validation results
            metric_name: Name of the metric to visualize (or None for all metrics)
            
        Returns:
            Base64-encoded image of the visualization
        """
        if not MATPLOTLIB_AVAILABLE:
            return ""
            
        # Extract error values
        error_data = defaultdict(list)
        
        for vr in validation_results:
            if metric_name is None:
                # Include all metrics
                for metric, comparison in vr.metrics_comparison.items():
                    if "mape" in comparison:
                        error_data[metric].append(comparison["mape"])
            elif metric_name in vr.metrics_comparison:
                # Include only the specified metric
                comparison = vr.metrics_comparison[metric_name]
                if "mape" in comparison:
                    error_data[metric_name].append(comparison["mape"])
        
        if not error_data:
            logger.warning("No error values to visualize")
            return ""
            
        try:
            # Set up the figure
            fig, ax = plt.subplots(
                figsize=(
                    self.config["visualization_width"] / 100,
                    self.config["visualization_height"] / 100
                )
            )
            
            # Set colors and style
            plt.style.use('seaborn-v0_8-whitegrid' if self.config["report_theme"] == "light" else 'dark_background')
            
            # Plot histograms
            n_metrics = len(error_data)
            if n_metrics > 1:
                # Multiple metrics - create a histogram for each
                for i, (metric, values) in enumerate(error_data.items()):
                    ax.hist(
                        values, 
                        alpha=0.7, 
                        bins=min(20, len(values)), 
                        label=metric,
                        edgecolor='black'
                    )
                ax.legend()
            else:
                # Single metric - create a single histogram
                metric, values = next(iter(error_data.items()))
                ax.hist(
                    values, 
                    alpha=0.7, 
                    bins=min(20, len(values)),
                    edgecolor='black'
                )
                
                # Add statistics as text
                if NUMPY_AVAILABLE and values:
                    mean_error = np.mean(values)
                    median_error = np.median(values)
                    std_error = np.std(values)
                    
                    stats_text = (
                        f"Mean: {mean_error:.2f}%\n"
                        f"Median: {median_error:.2f}%\n"
                        f"Std Dev: {std_error:.2f}%"
                    )
                    
                    # Add text box in top left corner
                    props = dict(boxstyle='round', facecolor='white', alpha=0.8)
                    ax.text(
                        0.05, 0.95, stats_text, 
                        transform=ax.transAxes, 
                        fontsize=10,
                        verticalalignment='top', 
                        bbox=props
                    )
            
            # Customize plot
            ax.set_xlabel('Mean Absolute Percentage Error (%)')
            ax.set_ylabel('Frequency')
            ax.set_title(
                f"Error Distribution - {'All Metrics' if metric_name is None else metric_name}"
            )
            
            # Save to a BytesIO object
            import io
            buf = io.BytesIO()
            plt.tight_layout()
            plt.savefig(buf, format=self.config["visualization_format"], dpi=100)
            plt.close(fig)
            
            # Convert to base64
            image_base64 = base64.b64encode(buf.getvalue()).decode('utf-8')
            
            # Cache the visualization
            cache_params = {
                'validation_result_ids': [id(vr) for vr in validation_results],
                'metric_name': metric_name,
                'interactive': False
            }
            cache_key = self._get_cache_key('error_distribution', cache_params)
            self._cache_visualization(cache_key, image_base64, self.config["visualization_format"])
            
            return image_base64
            
        except Exception as e:
            logger.error(f"Error creating static error distribution: {e}")
            return ""
            
    def _generate_executive_summary(self, validation_results: List[ValidationResult]) -> Dict[str, Any]:
        """
        Generate an executive summary of validation results.
        
        Args:
            validation_results: List of validation results
            
        Returns:
            Dictionary with executive summary data
        """
        if not validation_results:
            return {}
            
        # Group validation results by hardware and model
        hardware_model_results = defaultdict(list)
        hardware_types = set()
        model_types = set()
        
        for vr in validation_results:
            hw_id = vr.hardware_result.hardware_id
            model_id = vr.hardware_result.model_id
            
            hardware_types.add(hw_id)
            model_types.add(model_id)
            
            hardware_model_results[(hw_id, model_id)].append(vr)
        
        # Calculate overall metrics
        total_results = len(validation_results)
        total_hardware = len(hardware_types)
        total_models = len(model_types)
        
        # Calculate overall MAPE
        all_mape_values = []
        metric_mape_values = defaultdict(list)
        
        for vr in validation_results:
            for metric_name, comparison in vr.metrics_comparison.items():
                if "mape" in comparison:
                    mape = comparison["mape"]
                    all_mape_values.append(mape)
                    metric_mape_values[metric_name].append(mape)
        
        if all_mape_values:
            overall_mape = sum(all_mape_values) / len(all_mape_values)
            
            # Calculate metric-specific MAPEs
            metric_mapes = {}
            for metric_name, mape_values in metric_mape_values.items():
                if mape_values:
                    metric_mapes[metric_name] = sum(mape_values) / len(mape_values)
                    
            # Find best and worst metrics
            if metric_mapes:
                best_metric = min(metric_mapes.items(), key=lambda x: x[1])
                worst_metric = max(metric_mapes.items(), key=lambda x: x[1])
            else:
                best_metric = None
                worst_metric = None
        else:
            overall_mape = None
            metric_mapes = {}
            best_metric = None
            worst_metric = None
        
        # Find best and worst hardware-model combinations
        hw_model_mapes = {}
        for (hw_id, model_id), results in hardware_model_results.items():
            result_mape_values = []
            for vr in results:
                for comparison in vr.metrics_comparison.values():
                    if "mape" in comparison:
                        result_mape_values.append(comparison["mape"])
            
            if result_mape_values:
                hw_model_mapes[(hw_id, model_id)] = sum(result_mape_values) / len(result_mape_values)
        
        if hw_model_mapes:
            best_hw_model = min(hw_model_mapes.items(), key=lambda x: x[1])
            worst_hw_model = max(hw_model_mapes.items(), key=lambda x: x[1])
        else:
            best_hw_model = None
            worst_hw_model = None
            
        # Generate summary text
        status = self._interpret_mape(overall_mape)
        
        # Calculate additional statistical metrics
        if all_mape_values and NUMPY_AVAILABLE:
            avg_mape = np.mean(all_mape_values)
            median_mape = np.median(all_mape_values)
            std_dev = np.std(all_mape_values)
            
            if len(all_mape_values) > 1:
                confidence_level = self.config["statistical_confidence_level"]
                confidence_interval = scipy.stats.t.interval(
                    confidence_level,
                    len(all_mape_values) - 1, 
                    loc=avg_mape, 
                    scale=scipy.stats.sem(all_mape_values)
                )
            else:
                confidence_interval = (avg_mape, avg_mape)
                
            statistical_metrics = {
                "mean": avg_mape,
                "median": median_mape,
                "std_dev": std_dev,
                "confidence_interval": confidence_interval,
                "confidence_level": self.config["statistical_confidence_level"],
                "min": min(all_mape_values),
                "max": max(all_mape_values),
                "p25": np.percentile(all_mape_values, 25),
                "p75": np.percentile(all_mape_values, 75)
            }
        else:
            statistical_metrics = {}
            
        # Generate recommendations
        recommendations = []
        
        if worst_metric and worst_metric[1] > 15:
            recommendations.append({
                "type": "metric_improvement",
                "target": worst_metric[0],
                "current_mape": worst_metric[1],
                "recommendation": f"Consider calibration for {worst_metric[0]}"
            })
            
        if worst_hw_model and worst_hw_model[1] > 15:
            hw, model = worst_hw_model[0]
            recommendations.append({
                "type": "hardware_model_calibration",
                "target_hardware": hw,
                "target_model": model,
                "current_mape": worst_hw_model[1],
                "recommendation": f"Prioritize calibration for {model} on {hw}"
            })
            
        # Check for potential drift
        drift_detected = False
        if len(validation_results) > 5:
            # Sort by timestamp
            sorted_results = sorted(validation_results, key=lambda vr: vr.validation_timestamp)
            
            # Split into old and new results
            split_index = len(sorted_results) // 2
            old_results = sorted_results[:split_index]
            new_results = sorted_results[split_index:]
            
            # Calculate MAPEs for old and new results
            old_mape_values = []
            new_mape_values = []
            
            for vr in old_results:
                for comparison in vr.metrics_comparison.values():
                    if "mape" in comparison:
                        old_mape_values.append(comparison["mape"])
                        
            for vr in new_results:
                for comparison in vr.metrics_comparison.values():
                    if "mape" in comparison:
                        new_mape_values.append(comparison["mape"])
                        
            if old_mape_values and new_mape_values:
                old_avg_mape = sum(old_mape_values) / len(old_mape_values)
                new_avg_mape = sum(new_mape_values) / len(new_mape_values)
                
                # Check for significant drift (>10% change)
                if abs(new_avg_mape - old_avg_mape) / old_avg_mape > 0.1:
                    drift_detected = True
                    recommendations.append({
                        "type": "drift_detection",
                        "old_mape": old_avg_mape,
                        "new_mape": new_avg_mape,
                        "change_percent": (new_avg_mape - old_avg_mape) / old_avg_mape * 100,
                        "recommendation": "Potential drift detected, consider running drift detection"
                    })
        
        # Prepare executive summary
        executive_summary = {
            "total_results": total_results,
            "total_hardware": total_hardware,
            "total_models": total_models,
            "overall_mape": overall_mape,
            "overall_status": status,
            "metric_mapes": metric_mapes,
            "best_metric": best_metric,
            "worst_metric": worst_metric,
            "best_hardware_model": best_hw_model,
            "worst_hardware_model": worst_hw_model,
            "drift_detected": drift_detected,
            "recommendations": recommendations,
            "statistical_metrics": statistical_metrics
        }
        
        return executive_summary
        
    def _create_statistical_analysis_visualization(
        self,
        validation_results: List[ValidationResult],
        metric_name: Optional[str] = None,
        interactive: Optional[bool] = None
    ) -> str:
        """
        Create a statistical analysis visualization.
        
        Args:
            validation_results: List of validation results
            metric_name: Name of the metric to visualize (or None for overall analysis)
            interactive: Whether to create an interactive visualization
            
        Returns:
            Visualization content (HTML or base64-encoded image)
        """
        # Determine if visualization should be interactive
        if interactive is None:
            interactive = self.config["interactive_visualizations"]
            
        # Use the cache key for this visualization
        cache_params = {
            'validation_result_ids': [id(vr) for vr in validation_results],
            'metric_name': metric_name,
            'interactive': interactive
        }
        cache_key = self._get_cache_key('statistical_analysis', cache_params)
        
        # Check if cached version exists
        format = 'html' if interactive else self.config["visualization_format"]
        cached = self._get_cached_visualization(cache_key, format)
        if cached:
            return cached
            
        # Create the visualization
        if not NUMPY_AVAILABLE:
            logger.warning("NumPy not available for statistical analysis")
            return ""
            
        # Extract error values
        if metric_name is None:
            # All metrics
            error_values = []
            for vr in validation_results:
                for comparison in vr.metrics_comparison.values():
                    if "mape" in comparison:
                        error_values.append(comparison["mape"])
        else:
            # Specific metric
            error_values = []
            for vr in validation_results:
                if metric_name in vr.metrics_comparison:
                    comparison = vr.metrics_comparison[metric_name]
                    if "mape" in comparison:
                        error_values.append(comparison["mape"])
        
        if not error_values:
            logger.warning("No error values to analyze")
            return ""
            
        # Calculate statistical metrics
        mean_error = np.mean(error_values)
        median_error = np.median(error_values)
        std_error = np.std(error_values)
        min_error = min(error_values)
        max_error = max(error_values)
        q1_error = np.percentile(error_values, 25)
        q3_error = np.percentile(error_values, 75)
        
        # Calculate confidence interval
        if len(error_values) > 1:
            confidence_level = self.config["statistical_confidence_level"]
            ci = scipy.stats.t.interval(
                confidence_level,
                len(error_values) - 1, 
                loc=mean_error, 
                scale=scipy.stats.sem(error_values)
            )
        else:
            ci = (mean_error, mean_error)
            
        if interactive and PLOTLY_AVAILABLE:
            return self._create_interactive_statistical_analysis(
                error_values,
                mean_error,
                median_error,
                std_error,
                min_error,
                max_error,
                q1_error,
                q3_error,
                ci,
                metric_name,
                cache_key
            )
        elif MATPLOTLIB_AVAILABLE:
            return self._create_static_statistical_analysis(
                error_values,
                mean_error,
                median_error,
                std_error,
                min_error,
                max_error,
                q1_error,
                q3_error,
                ci,
                metric_name,
                cache_key
            )
        else:
            logger.warning("Cannot create statistical analysis: No visualization library available")
            return ""
            
    def _create_interactive_statistical_analysis(
        self,
        error_values: List[float],
        mean_error: float,
        median_error: float,
        std_error: float,
        min_error: float,
        max_error: float,
        q1_error: float,
        q3_error: float,
        confidence_interval: Tuple[float, float],
        metric_name: Optional[str] = None,
        cache_key: Optional[str] = None
    ) -> str:
        """
        Create an interactive statistical analysis visualization using Plotly.
        
        Args:
            error_values: List of error values
            mean_error: Mean error
            median_error: Median error
            std_error: Standard deviation of errors
            min_error: Minimum error
            max_error: Maximum error
            q1_error: First quartile (25th percentile) error
            q3_error: Third quartile (75th percentile) error
            confidence_interval: Tuple of (lower, upper) confidence interval bounds
            metric_name: Name of the metric being analyzed
            cache_key: Cache key for the visualization
            
        Returns:
            HTML content for the interactive visualization
        """
        if not PLOTLY_AVAILABLE:
            return ""
            
        try:
            # Create a subplot with 2 rows and 1 column
            fig = make_subplots(
                rows=2, 
                cols=1,
                row_heights=[0.7, 0.3],
                subplot_titles=(
                    f"Error Distribution - {'All Metrics' if metric_name is None else metric_name}",
                    "Statistical Summary"
                )
            )
            
            # Add histogram to the first row
            fig.add_trace(
                go.Histogram(
                    x=error_values,
                    opacity=0.7,
                    name="Error Distribution",
                    marker=dict(
                        color='rgba(50, 130, 184, 0.7)',
                        line=dict(color='rgba(50, 130, 184, 1.0)', width=1)
                    ),
                    xbins=dict(size=(max_error - min_error) / 20)
                ),
                row=1, col=1
            )
            
            # Add mean line
            fig.add_trace(
                go.Scatter(
                    x=[mean_error, mean_error],
                    y=[0, len(error_values) / 4],  # Adjust height as needed
                    mode="lines",
                    line=dict(color="red", width=2, dash="dash"),
                    name="Mean"
                ),
                row=1, col=1
            )
            
            # Add median line
            fig.add_trace(
                go.Scatter(
                    x=[median_error, median_error],
                    y=[0, len(error_values) / 4],  # Adjust height as needed
                    mode="lines",
                    line=dict(color="green", width=2, dash="dash"),
                    name="Median"
                ),
                row=1, col=1
            )
            
            # Add confidence interval
            fig.add_trace(
                go.Scatter(
                    x=[confidence_interval[0], confidence_interval[1]],
                    y=[len(error_values) / 8, len(error_values) / 8],  # Adjust height as needed
                    mode="lines",
                    line=dict(color="purple", width=6),
                    name=f"{self.config['statistical_confidence_level'] * 100:.0f}% Confidence Interval"
                ),
                row=1, col=1
            )
            
            # Add box plot to the second row
            fig.add_trace(
                go.Box(
                    x=error_values,
                    name="Error Distribution",
                    marker=dict(color='rgba(50, 130, 184, 0.7)'),
                    boxpoints='outliers'
                ),
                row=2, col=1
            )
            
            # Create a table of statistics
            stats_table = go.Table(
                header=dict(
                    values=["Statistic", "Value"],
                    fill_color='rgba(50, 130, 184, 0.7)',
                    align='center',
                    font=dict(color='white', size=12)
                ),
                cells=dict(
                    values=[
                        [
                            "Mean", "Median", "Std Dev", 
                            "Min", "Max", "25th Percentile", "75th Percentile",
                            f"{self.config['statistical_confidence_level'] * 100:.0f}% CI (Lower)",
                            f"{self.config['statistical_confidence_level'] * 100:.0f}% CI (Upper)"
                        ],
                        [
                            f"{mean_error:.2f}%", f"{median_error:.2f}%", f"{std_error:.2f}%",
                            f"{min_error:.2f}%", f"{max_error:.2f}%", 
                            f"{q1_error:.2f}%", f"{q3_error:.2f}%",
                            f"{confidence_interval[0]:.2f}%", f"{confidence_interval[1]:.2f}%"
                        ]
                    ],
                    fill_color='rgba(245, 246, 249, 1)',
                    align='center'
                )
            )
            
            # Update layout
            fig.update_layout(
                height=self.config["visualization_height"],
                width=self.config["visualization_width"],
                title=f"Statistical Analysis - {'All Metrics' if metric_name is None else metric_name}",
                showlegend=True,
                template="plotly_white" if self.config["report_theme"] == "light" else "plotly_dark",
                margin=dict(l=50, r=50, t=100, b=50)
            )
            
            # Update x-axis labels
            fig.update_xaxes(title_text="Mean Absolute Percentage Error (%)", row=1, col=1)
            fig.update_xaxes(title_text="Mean Absolute Percentage Error (%)", row=2, col=1)
            
            # Update y-axis labels
            fig.update_yaxes(title_text="Frequency", row=1, col=1)
            
            # Generate HTML
            html = fig.to_html(full_html=False, include_plotlyjs='cdn')
            
            # Add table after the figure
            table_html = go.Figure(stats_table).to_html(full_html=False, include_plotlyjs=False)
            html = f"{html}<br/>{table_html}"
            
            # Cache the visualization
            if cache_key:
                self._cache_visualization(cache_key, html, 'html')
            
            return html
            
        except Exception as e:
            logger.error(f"Error creating interactive statistical analysis: {e}")
            return ""
            
    def _create_static_statistical_analysis(
        self,
        error_values: List[float],
        mean_error: float,
        median_error: float,
        std_error: float,
        min_error: float,
        max_error: float,
        q1_error: float,
        q3_error: float,
        confidence_interval: Tuple[float, float],
        metric_name: Optional[str] = None,
        cache_key: Optional[str] = None
    ) -> str:
        """
        Create a static statistical analysis visualization using Matplotlib.
        
        Args:
            error_values: List of error values
            mean_error: Mean error
            median_error: Median error
            std_error: Standard deviation of errors
            min_error: Minimum error
            max_error: Maximum error
            q1_error: First quartile (25th percentile) error
            q3_error: Third quartile (75th percentile) error
            confidence_interval: Tuple of (lower, upper) confidence interval bounds
            metric_name: Name of the metric being analyzed
            cache_key: Cache key for the visualization
            
        Returns:
            Base64-encoded image of the visualization
        """
        if not MATPLOTLIB_AVAILABLE:
            return ""
            
        try:
            # Create figure with 2 rows
            fig, (ax1, ax2) = plt.subplots(
                2, 1,
                figsize=(
                    self.config["visualization_width"] / 100,
                    self.config["visualization_height"] / 100
                ),
                gridspec_kw={'height_ratios': [3, 1]}
            )
            
            # Set colors and style
            plt.style.use('seaborn-v0_8-whitegrid' if self.config["report_theme"] == "light" else 'dark_background')
            
            # Plot histogram in the first row
            n, bins, patches = ax1.hist(
                error_values,
                bins=min(20, len(error_values)),
                alpha=0.7,
                edgecolor='black',
                color='skyblue'
            )
            
            # Add mean line
            ax1.axvline(
                mean_error,
                color='red',
                linestyle='dashed',
                linewidth=2,
                label=f'Mean: {mean_error:.2f}%'
            )
            
            # Add median line
            ax1.axvline(
                median_error,
                color='green',
                linestyle='dashed',
                linewidth=2,
                label=f'Median: {median_error:.2f}%'
            )
            
            # Add confidence interval
            ax1.plot(
                [confidence_interval[0], confidence_interval[1]],
                [max(n) * 0.2, max(n) * 0.2],
                color='purple',
                linewidth=4,
                label=f'{self.config["statistical_confidence_level"] * 100:.0f}% CI'
            )
            
            # Plot box plot in the second row
            ax2.boxplot(
                error_values,
                vert=False,
                widths=0.7,
                showmeans=True,
                meanline=True,
                patch_artist=True,
                boxprops=dict(facecolor='skyblue', alpha=0.7)
            )
            
            # Set titles and labels
            fig.suptitle(f"Statistical Analysis - {'All Metrics' if metric_name is None else metric_name}")
            ax1.set_title("Error Distribution")
            ax1.set_xlabel("Mean Absolute Percentage Error (%)")
            ax1.set_ylabel("Frequency")
            ax1.legend()
            
            ax2.set_title("Box Plot")
            ax2.set_xlabel("Mean Absolute Percentage Error (%)")
            
            # Add statistics as table
            table_data = [
                ["Mean", f"{mean_error:.2f}%"],
                ["Median", f"{median_error:.2f}%"],
                ["Std Dev", f"{std_error:.2f}%"],
                ["Min", f"{min_error:.2f}%"],
                ["Max", f"{max_error:.2f}%"],
                ["25th Percentile", f"{q1_error:.2f}%"],
                ["75th Percentile", f"{q3_error:.2f}%"],
                [f"{self.config['statistical_confidence_level'] * 100:.0f}% CI", f"({confidence_interval[0]:.2f}%, {confidence_interval[1]:.2f}%)"]
            ]
            
            # Create a new axis for the table
            ax3 = fig.add_axes([0.15, 0.01, 0.7, 0.15])  # [left, bottom, width, height]
            ax3.axis('off')
            
            # Create table
            table = ax3.table(
                cellText=table_data,
                colWidths=[0.3, 0.3],
                cellLoc='center',
                loc='center'
            )
            
            # Style the table
            table.auto_set_font_size(False)
            table.set_fontsize(9)
            table.scale(1, 1.2)
            
            # Adjust layout
            plt.tight_layout()
            plt.subplots_adjust(bottom=0.2)  # Make room for the table
            
            # Save to a BytesIO object
            import io
            buf = io.BytesIO()
            plt.savefig(buf, format=self.config["visualization_format"], dpi=100)
            plt.close(fig)
            
            # Convert to base64
            image_base64 = base64.b64encode(buf.getvalue()).decode('utf-8')
            
            # Cache the visualization
            if cache_key:
                self._cache_visualization(cache_key, image_base64, self.config["visualization_format"])
            
            return image_base64
            
        except Exception as e:
            logger.error(f"Error creating static statistical analysis: {e}")
            return ""
                
    def generate_report(
        self,
        validation_results: List[ValidationResult],
        format: str = "html",
        include_visualizations: bool = True,
        include_executive_summary: Optional[bool] = None,
        include_statistical_analysis: Optional[bool] = None,
        include_recommendations: Optional[bool] = None,
        include_sections: Optional[List[str]] = None,
        custom_title: Optional[str] = None,
        hardware_filter: Optional[str] = None,
        model_filter: Optional[str] = None,
        date_range: Optional[Tuple[str, str]] = None,
        output_type: str = "string"
    ) -> str:
        """
        Generate a comprehensive validation report.
        
        Args:
            validation_results: List of validation results
            format: Output format (html, markdown, json, csv, pdf)
            include_visualizations: Whether to include visualizations
            include_executive_summary: Whether to include executive summary (default: from config)
            include_statistical_analysis: Whether to include statistical analysis (default: from config)
            include_recommendations: Whether to include recommendations (default: from config)
            include_sections: List of specific sections to include (default: all sections from config)
            custom_title: Custom title for the report (default: from config template)
            hardware_filter: Filter results by hardware ID
            model_filter: Filter results by model ID
            date_range: Filter results by date range (start_date, end_date)
            output_type: Type of output - "string" or "file" (for PDF)
            
        Returns:
            Report content as a string
        """
        if not validation_results:
            return "No validation results to report"
        
        # Apply filters if specified
        filtered_results = validation_results
        
        if hardware_filter:
            filtered_results = [
                vr for vr in filtered_results 
                if vr.hardware_result.hardware_id == hardware_filter
            ]
            
        if model_filter:
            filtered_results = [
                vr for vr in filtered_results 
                if vr.hardware_result.model_id == model_filter
            ]
            
        if date_range:
            start_date, end_date = date_range
            if start_date:
                filtered_results = [
                    vr for vr in filtered_results 
                    if vr.validation_timestamp >= start_date
                ]
            if end_date:
                filtered_results = [
                    vr for vr in filtered_results 
                    if vr.validation_timestamp <= end_date
                ]
                
        if not filtered_results:
            return "No validation results match the specified filters"
            
        # Set default values from config if not specified
        if include_executive_summary is None:
            include_executive_summary = self.config["executive_summary"]
            
        if include_statistical_analysis is None:
            include_statistical_analysis = self.config["technical_details"]
            
        if include_recommendations is None:
            include_recommendations = True
            
        if include_sections is None:
            include_sections = self.config["report_sections"]
            
        # Check if the format is supported
        if format not in self.config["report_formats"]:
            logger.warning(f"Unsupported format: {format}. Defaulting to HTML.")
            format = "html"
        
        # Generate executive summary if requested
        executive_summary = None
        if include_executive_summary and "executive_summary" in include_sections:
            executive_summary = self._generate_executive_summary(filtered_results)
        
        # Common report data
        report_data = self._prepare_report_data(filtered_results)
        
        # Add executive summary and configuration to report data
        report_data["executive_summary"] = executive_summary
        report_data["include_sections"] = include_sections
        report_data["include_visualizations"] = include_visualizations
        report_data["include_statistical_analysis"] = include_statistical_analysis
        report_data["include_recommendations"] = include_recommendations
        report_data["custom_title"] = custom_title
        
        # Generate all required visualizations if needed
        visualizations = {}
        
        if include_visualizations:
            # Create visualizations based on configuration
            if "error_distribution" in self.config["visualization_types"]:
                visualizations["error_distribution"] = self._create_error_distribution_chart(
                    filtered_results,
                    interactive=(format == "html")
                )
                
            if include_statistical_analysis and "statistical_analysis" in self.config["visualization_types"]:
                visualizations["statistical_analysis"] = self._create_statistical_analysis_visualization(
                    filtered_results,
                    interactive=(format == "html")
                )
                
            # Add more visualizations as needed
            # ...
            
        # Add visualizations to report data
        report_data["visualizations"] = visualizations
        
        # Generate report based on format
        if format == "html":
            return self._generate_enhanced_html_report(report_data)
        elif format == "markdown":
            return self._generate_enhanced_markdown_report(report_data)
        elif format == "json":
            return self._generate_enhanced_json_report(report_data)
        elif format == "csv":
            return self._generate_csv_report(report_data)
        elif format == "pdf":
            return self._generate_pdf_report(report_data, output_type)
        else:
            return self._generate_enhanced_text_report(report_data)
            
    def _generate_enhanced_html_report(self, report_data: Dict[str, Any]) -> str:
        """
        Generate an enhanced HTML report with executive summary, visualizations,
        and statistical analysis.
        
        Args:
            report_data: Report data including validation results and visualizations
            
        Returns:
            HTML report as a string
        """
        # Determine if we should include executive summary, visualizations, etc.
        include_sections = report_data.get("include_sections", self.config["report_sections"])
        include_exec_summary = "executive_summary" in include_sections and report_data.get("executive_summary")
        include_visualizations = report_data.get("include_visualizations", True)
        include_statistical_analysis = report_data.get("include_statistical_analysis", True)
        include_recommendations = report_data.get("include_recommendations", True)
        
        # Get visualizations
        visualizations = report_data.get("visualizations", {})
        
        # Set up HTML template with modern styling
        html_template = """
        <!DOCTYPE html>
        <html lang="en">
        <head>
            <meta charset="UTF-8">
            <meta name="viewport" content="width=device-width, initial-scale=1.0">
            <title>{title}</title>
            <style>
                :root {{
                    --primary-color: #3377B0;
                    --secondary-color: #F5F5F5;
                    --accent-color: #4CAF50;
                    --warning-color: #FFC107;
                    --danger-color: #F44336;
                    --text-color: #333333;
                    --background-color: #FFFFFF;
                    --border-color: #DDDDDD;
                    --hover-color: #f0f8ff;
                }}
                
                body {{
                    font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
                    line-height: the 1.6;
                    margin: 0;
                    padding: 0;
                    color: var(--text-color);
                    background-color: var(--background-color);
                }}
                
                .container {{
                    width: 100%;
                    max-width: 1400px;
                    margin: 0 auto;
                    padding: 20px;
                }}
                
                header {{
                    background-color: var(--primary-color);
                    color: white;
                    padding: 20px;
                    margin-bottom: 20px;
                    border-radius: 5px;
                    box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
                }}
                
                header h1 {{
                    margin: 0;
                    font-size: 24px;
                }}
                
                header p {{
                    margin: 5px 0 0;
                    font-size: 14px;
                    opacity: 0.9;
                }}
                
                nav {{
                    background-color: #f8f9fa;
                    padding: 10px 20px;
                    margin-bottom: 20px;
                    border-radius: 5px;
                    box-shadow: 0 1px 2px rgba(0, 0, 0, 0.05);
                }}
                
                nav a {{
                    display: inline-block;
                    padding: 5px 10px;
                    margin-right: 10px;
                    color: var(--primary-color);
                    text-decoration: none;
                    border-radius: 3px;
                }}
                
                nav a:hover {{
                    background-color: var(--hover-color);
                }}
                
                .section {{
                    background-color: white;
                    padding: 20px;
                    margin-bottom: 20px;
                    border-radius: 5px;
                    box-shadow: 0 1px 3px rgba(0, 0, 0, 0.1);
                }}
                
                .section-header {{
                    border-bottom: 1px solid var(--border-color);
                    padding-bottom: 10px;
                    margin-bottom: 15px;
                }}
                
                .section-header h2 {{
                    margin: 0;
                    font-size: 20px;
                    color: var(--primary-color);
                }}
                
                .highlight {{
                    font-weight: bold;
                    color: var(--primary-color);
                }}
                
                .card {{
                    background-color: var(--secondary-color);
                    padding: 15px;
                    margin-bottom: 15px;
                    border-radius: 5px;
                    border-left: 4px solid var(--primary-color);
                    box-shadow: 0 1px 2px rgba(0, 0, 0, 0.05);
                }}
                
                .card-title {{
                    margin-top: 0;
                    font-size: 16px;
                    color: var(--primary-color);
                }}
                
                .metrics-grid {{
                    display: grid;
                    grid-template-columns: repeat(auto-fill, minmax(200px, 1fr));
                    gap: 15px;
                    margin-bottom: 20px;
                }}
                
                .metric-card {{
                    background-color: white;
                    padding: 15px;
                    border-radius: 5px;
                    box-shadow: 0 1px 2px rgba(0, 0, 0, 0.05);
                    text-align: center;
                }}
                
                .metric-value {{
                    font-size: 24px;
                    font-weight: bold;
                    margin: 10px 0;
                    color: var(--primary-color);
                }}
                
                .metric-label {{
                    font-size: 14px;
                    color: #666;
                }}
                
                .status-excellent {{
                    color: #27ae60;
                    font-weight: bold;
                }}
                
                .status-good {{
                    color: #2ecc71;
                }}
                
                .status-acceptable {{
                    color: #f39c12;
                }}
                
                .status-problematic {{
                    color: #e67e22;
                }}
                
                .status-poor {{
                    color: #e74c3c;
                    font-weight: bold;
                }}
                
                table {{
                    width: 100%;
                    border-collapse: collapse;
                    margin-bottom: 20px;
                }}
                
                th, td {{
                    padding: 12px 15px;
                    text-align: left;
                    border-bottom: 1px solid var(--border-color);
                }}
                
                th {{
                    background-color: var(--secondary-color);
                    font-weight: bold;
                    color: var(--primary-color);
                }}
                
                tr:nth-child(even) {{
                    background-color: #f9f9f9;
                }}
                
                tr:hover {{
                    background-color: var(--hover-color);
                }}
                
                .visualizations {{
                    display: flex;
                    flex-direction: column;
                    gap: 20px;
                    margin-bottom: 20px;
                }}
                
                .visualization {{
                    background-color: white;
                    padding: 15px;
                    border-radius: 5px;
                    box-shadow: 0 1px 3px rgba(0, 0, 0, 0.1);
                }}
                
                .recommendation {{
                    background-color: #e8f4fd;
                    border-left: 4px solid var(--primary-color);
                    padding: 15px;
                    margin-bottom: 15px;
                    border-radius: 5px;
                }}
                
                .recommendation-title {{
                    font-weight: bold;
                    margin-bottom: 5px;
                    color: var(--primary-color);
                }}
                
                .recommendation-content {{
                    margin: 0;
                }}
                
                .recommendation.high-priority {{
                    background-color: #fff8e1;
                    border-left: 4px solid var(--warning-color);
                }}
                
                .recommendation.critical-priority {{
                    background-color: #ffebee;
                    border-left: 4px solid var(--danger-color);
                }}
                
                footer {{
                    background-color: var(--secondary-color);
                    padding: 15px;
                    margin-top: 30px;
                    text-align: center;
                    font-size: 14px;
                    color: #666;
                    border-top: 1px solid var(--border-color);
                }}
                
                .tab {{
                    overflow: hidden;
                    background-color: #f1f1f1;
                    border-radius: 5px 5px 0 0;
                }}
                
                .tab button {{
                    background-color: inherit;
                    float: left;
                    border: none;
                    outline: none;
                    cursor: pointer;
                    padding: 10px 16px;
                    transition: 0.3s;
                    font-size: 14px;
                }}
                
                .tab button:hover {{
                    background-color: #ddd;
                }}
                
                .tab button.active {{
                    background-color: var(--primary-color);
                    color: white;
                }}
                
                .tabcontent {{
                    display: none;
                    padding: 15px;
                    border: 1px solid #ccc;
                    border-top: none;
                    border-radius: 0 0 5px 5px;
                    animation: fadeEffect 1s;
                }}
                
                @keyframes fadeEffect {{
                    from {{opacity: 0;}}
                    to {{opacity: 1;}}
                }}
                
                @media print {{
                    header, nav, .no-print {{
                        display: none;
                    }}
                    
                    body, .container {{
                        width: 100%;
                        margin: 0;
                        padding: 0;
                    }}
                    
                    .section {{
                        page-break-inside: avoid;
                        box-shadow: none;
                        border: 1px solid #ddd;
                    }}
                }}
            </style>
            {auto_refresh}
        </head>
        <body>
            <div class="container">
                <header>
                    <h1>{title}</h1>
                    <p>Generated on: {timestamp}</p>
                </header>
                
                <nav id="report-nav">
                    {nav_links}
                </nav>
                
                {executive_summary_section}
                
                {overview_section}
                
                {hardware_comparison_section}
                
                {model_comparison_section}
                
                {metric_analysis_section}
                
                {statistical_analysis_section}
                
                {detailed_results_section}
                
                {recommendations_section}
                
                {appendix_section}
                
                <footer>
                    {footer}
                </footer>
            </div>
            
            <script>
                // Simple tab navigation
                function openSection(evt, sectionName) {{
                    var i, tabcontent, tablinks;
                    tabcontent = document.getElementsByClassName("tabcontent");
                    for (i = 0; i < tabcontent.length; i++) {{
                        tabcontent[i].style.display = "none";
                    }}
                    tablinks = document.getElementsByClassName("tablink");
                    for (i = 0; i < tablinks.length; i++) {{
                        tablinks[i].className = tablinks[i].className.replace(" active", "");
                    }}
                    document.getElementById(sectionName).style.display = "block";
                    evt.currentTarget.className += " active";
                }}
                
                // Get the element with id="defaultOpen" and click on it
                document.addEventListener('DOMContentLoaded', function() {{
                    // Get the first tab button and click it
                    if (document.getElementsByClassName('tablink').length > 0) {{
                        document.getElementsByClassName('tablink')[0].click();
                    }}
                    
                    // Set up smooth scrolling for navigation links
                    document.querySelectorAll('nav a').forEach(anchor => {{
                        anchor.addEventListener('click', function(e) {{
                            e.preventDefault();
                            
                            const targetId = this.getAttribute('href').substring(1);
                            const targetElement = document.getElementById(targetId);
                            
                            if (targetElement) {{
                                targetElement.scrollIntoView({{
                                    behavior: 'smooth'
                                }});
                            }}
                        }});
                    }});
                }});
            </script>
        </body>
        </html>
        """
        
        # Generate navigation links
        nav_links = []
        if include_exec_summary and "executive_summary" in include_sections:
            nav_links.append('<a href="#executive-summary">Executive Summary</a>')
        if "overview" in include_sections:
            nav_links.append('<a href="#overview">Overview</a>')
        if "hardware_comparison" in include_sections:
            nav_links.append('<a href="#hardware-comparison">Hardware Comparison</a>')
        if "model_comparison" in include_sections:
            nav_links.append('<a href="#model-comparison">Model Comparison</a>')
        if "metric_analysis" in include_sections:
            nav_links.append('<a href="#metric-analysis">Metric Analysis</a>')
        if include_statistical_analysis and "statistical_analysis" in include_sections:
            nav_links.append('<a href="#statistical-analysis">Statistical Analysis</a>')
        if "detailed_results" in include_sections:
            nav_links.append('<a href="#detailed-results">Detailed Results</a>')
        if include_recommendations and "recommendations" in include_sections:
            nav_links.append('<a href="#recommendations">Recommendations</a>')
        if "appendix" in include_sections:
            nav_links.append('<a href="#appendix">Appendix</a>')
            
        nav_links_html = '\n'.join(nav_links)
        
        # Generate executive summary section
        executive_summary_section = ""
        if include_exec_summary and "executive_summary" in include_sections:
            exec_summary = report_data["executive_summary"]
            
            # Key metrics cards
            metrics_grid = '<div class="metrics-grid">'
            
            # Total results card
            metrics_grid += f"""
            <div class="metric-card">
                <div class="metric-label">Total Results</div>
                <div class="metric-value">{exec_summary["total_results"]}</div>
            </div>
            """
            
            # Total hardware types card
            metrics_grid += f"""
            <div class="metric-card">
                <div class="metric-label">Hardware Types</div>
                <div class="metric-value">{exec_summary["total_hardware"]}</div>
            </div>
            """
            
            # Total model types card
            metrics_grid += f"""
            <div class="metric-card">
                <div class="metric-label">Model Types</div>
                <div class="metric-value">{exec_summary["total_models"]}</div>
            </div>
            """
            
            # Overall MAPE card
            if exec_summary["overall_mape"] is not None:
                status_class = self._status_class(exec_summary["overall_status"])
                metrics_grid += f"""
                <div class="metric-card">
                    <div class="metric-label">Overall MAPE</div>
                    <div class="metric-value status-{status_class}">{exec_summary["overall_mape"]:.2f}%</div>
                    <div>Status: <span class="status-{status_class}">{exec_summary["overall_status"]}</span></div>
                </div>
                """
            
            # Add more metrics cards as needed
            if "statistical_metrics" in exec_summary and exec_summary["statistical_metrics"]:
                sm = exec_summary["statistical_metrics"]
                
                # Median MAPE card
                if "median" in sm:
                    metrics_grid += f"""
                    <div class="metric-card">
                        <div class="metric-label">Median MAPE</div>
                        <div class="metric-value">{sm["median"]:.2f}%</div>
                    </div>
                    """
                
                # Standard deviation card
                if "std_dev" in sm:
                    metrics_grid += f"""
                    <div class="metric-card">
                        <div class="metric-label">Std Deviation</div>
                        <div class="metric-value">{sm["std_dev"]:.2f}%</div>
                    </div>
                    """
                    
                # Confidence interval card
                if "confidence_interval" in sm and "confidence_level" in sm:
                    ci = sm["confidence_interval"]
                    cl = sm["confidence_level"]
                    metrics_grid += f"""
                    <div class="metric-card">
                        <div class="metric-label">{cl * 100:.0f}% Confidence Interval</div>
                        <div class="metric-value">{ci[0]:.2f}% - {ci[1]:.2f}%</div>
                    </div>
                    """
            
            metrics_grid += '</div>'
            
            # Best and worst metrics
            best_worst_metrics = ""
            if exec_summary.get("best_metric") and exec_summary.get("worst_metric"):
                best_metric, best_value = exec_summary["best_metric"]
                worst_metric, worst_value = exec_summary["worst_metric"]
                
                best_worst_metrics = f"""
                <div class="card">
                    <h3 class="card-title">Best and Worst Metrics</h3>
                    <p><strong>Best performing metric:</strong> {best_metric} ({best_value:.2f}% MAPE)</p>
                    <p><strong>Worst performing metric:</strong> {worst_metric} ({worst_value:.2f}% MAPE)</p>
                </div>
                """
                
            # Best and worst hardware-model combinations
            best_worst_hw_model = ""
            if exec_summary.get("best_hardware_model") and exec_summary.get("worst_hardware_model"):
                best_hw_model, best_hw_value = exec_summary["best_hardware_model"]
                worst_hw_model, worst_hw_value = exec_summary["worst_hardware_model"]
                
                best_hw, best_model = best_hw_model
                worst_hw, worst_model = worst_hw_model
                
                best_worst_hw_model = f"""
                <div class="card">
                    <h3 class="card-title">Best and Worst Hardware-Model Combinations</h3>
                    <p><strong>Best combination:</strong> {best_model} on {best_hw} ({best_hw_value:.2f}% MAPE)</p>
                    <p><strong>Worst combination:</strong> {worst_model} on {worst_hw} ({worst_hw_value:.2f}% MAPE)</p>
                </div>
                """
            
            # Recommendations section
            recommendations_html = ""
            if include_recommendations and exec_summary.get("recommendations"):
                recommendations_html = "<h3>Key Recommendations</h3>"
                for rec in exec_summary["recommendations"]:
                    priority_class = ""
                    if "current_mape" in rec:
                        if rec["current_mape"] > 25:
                            priority_class = "critical-priority"
                        elif rec["current_mape"] > 15:
                            priority_class = "high-priority"
                    
                    recommendations_html += f"""
                    <div class="recommendation {priority_class}">
                        <div class="recommendation-title">{rec["type"].replace("_", " ").title()}</div>
                        <p class="recommendation-content">{rec["recommendation"]}</p>
                    </div>
                    """
            
            # Put it all together
            executive_summary_section = f"""
            <section id="executive-summary" class="section">
                <div class="section-header">
                    <h2>Executive Summary</h2>
                </div>
                
                {metrics_grid}
                
                {best_worst_metrics}
                
                {best_worst_hw_model}
                
                {recommendations_html}
            </section>
            """
        
        # Generate overview section
        overview_section = ""
        if "overview" in include_sections:
            total_results = report_data["total_results"]
            overall_mape = f"{report_data['overall_mape']:.2f}%" if report_data['overall_mape'] is not None else "N/A"
            overall_status = report_data['overall_status']
            status_class = self._status_class(overall_status)
            
            overview_section = f"""
            <section id="overview" class="section">
                <div class="section-header">
                    <h2>Overview</h2>
                </div>
                
                <p>This report analyzes simulation validation results, comparing simulation predictions with actual hardware measurements.</p>
                
                <div class="card">
                    <h3 class="card-title">Summary</h3>
                    <p><strong>Total validation results:</strong> {total_results}</p>
                    <p><strong>Overall MAPE:</strong> {overall_mape}</p>
                    <p><strong>Overall status:</strong> <span class="status-{status_class}">{overall_status}</span></p>
                </div>
                
                <div class="card">
                    <h3 class="card-title">What is MAPE?</h3>
                    <p>Mean Absolute Percentage Error (MAPE) measures the average percentage difference between simulated and actual values. Lower values indicate better simulation accuracy.</p>
                    <ul>
                        <li><span class="status-excellent">Excellent (< 5%)</span>: Simulation is highly accurate</li>
                        <li><span class="status-good">Good (5-10%)</span>: Simulation is very reliable</li>
                        <li><span class="status-acceptable">Acceptable (10-15%)</span>: Simulation is usable but could be improved</li>
                        <li><span class="status-problematic">Problematic (15-25%)</span>: Simulation needs calibration</li>
                        <li><span class="status-poor">Poor (> 25%)</span>: Simulation requires significant improvement</li>
                    </ul>
                </div>
            </section>
            """
        
        # Generate hardware comparison section
        hardware_comparison_section = ""
        if "hardware_comparison" in include_sections:
            # Hardware comparison table
            hardware_table = """
            <table>
                <tr>
                    <th>Hardware</th>
                    <th>Count</th>
                    <th>MAPE</th>
                    <th>Status</th>
                </tr>
            """
            
            for hw_id, stats in report_data["hardware_stats"].items():
                if "mean_mape" in stats:
                    status_class = self._status_class(stats["status"])
                    hardware_table += f"""
                    <tr>
                        <td>{hw_id}</td>
                        <td>{stats['count']}</td>
                        <td>{stats['mean_mape']:.2f}%</td>
                        <td class="status-{status_class}">{stats['status']}</td>
                    </tr>
                    """
            
            hardware_table += "</table>"
            
            # Include error distribution visualization if available
            hw_visualization = ""
            if include_visualizations and "error_distribution" in visualizations:
                error_dist = visualizations["error_distribution"]
                
                if error_dist:
                    # Check if it's an HTML or an image
                    if error_dist.startswith('<!DOCTYPE html') or error_dist.startswith('<html') or error_dist.startswith('<div'):
                        hw_visualization = f"""
                        <div class="visualization">
                            <h3>Error Distribution by Hardware</h3>
                            {error_dist}
                        </div>
                        """
                    else:
                        # Assume it's a base64-encoded image
                        hw_visualization = f"""
                        <div class="visualization">
                            <h3>Error Distribution by Hardware</h3>
                            <img src="data:image/{self.config['visualization_format']};base64,{error_dist}" alt="Error Distribution" style="max-width:100%;" />
                        </div>
                        """
            
            hardware_comparison_section = f"""
            <section id="hardware-comparison" class="section">
                <div class="section-header">
                    <h2>Hardware Comparison</h2>
                </div>
                
                <p>This section compares simulation accuracy across different hardware types.</p>
                
                {hardware_table}
                
                {hw_visualization}
            </section>
            """
        
        # Generate model comparison section
        model_comparison_section = ""
        if "model_comparison" in include_sections:
            # Model comparison table
            model_table = """
            <table>
                <tr>
                    <th>Model</th>
                    <th>Count</th>
                    <th>MAPE</th>
                    <th>Status</th>
                </tr>
            """
            
            for model_id, stats in report_data["model_stats"].items():
                if "mean_mape" in stats:
                    status_class = self._status_class(stats["status"])
                    model_table += f"""
                    <tr>
                        <td>{model_id}</td>
                        <td>{stats['count']}</td>
                        <td>{stats['mean_mape']:.2f}%</td>
                        <td class="status-{status_class}">{stats['status']}</td>
                    </tr>
                    """
            
            model_table += "</table>"
            
            # Include visualization if available
            # (No specific visualization for model comparison yet)
            
            model_comparison_section = f"""
            <section id="model-comparison" class="section">
                <div class="section-header">
                    <h2>Model Comparison</h2>
                </div>
                
                <p>This section compares simulation accuracy across different model types.</p>
                
                {model_table}
            </section>
            """
        
        # Generate hardware-model comparison section
        metric_analysis_section = ""
        if "metric_analysis" in include_sections:
            # Hardware-Model comparison table
            hw_model_table = """
            <table>
                <tr>
                    <th>Hardware</th>
                    <th>Model</th>
                    <th>Count</th>
                    <th>MAPE</th>
                    <th>Status</th>
                </tr>
            """
            
            for (hw_id, model_id), stats in report_data["hardware_model_stats"].items():
                if "mean_mape" in stats:
                    status_class = self._status_class(stats["status"])
                    hw_model_table += f"""
                    <tr>
                        <td>{hw_id}</td>
                        <td>{model_id}</td>
                        <td>{stats['count']}</td>
                        <td>{stats['mean_mape']:.2f}%</td>
                        <td class="status-{status_class}">{stats['status']}</td>
                    </tr>
                    """
            
            hw_model_table += "</table>"
            
            metric_analysis_section = f"""
            <section id="metric-analysis" class="section">
                <div class="section-header">
                    <h2>Metric Analysis</h2>
                </div>
                
                <p>This section shows validation results grouped by hardware and model combinations.</p>
                
                {hw_model_table}
            </section>
            """
        
        # Generate statistical analysis section
        statistical_analysis_section = ""
        if include_statistical_analysis and "statistical_analysis" in include_sections:
            # Include statistical analysis visualization if available
            stat_visualization = ""
            if include_visualizations and "statistical_analysis" in visualizations:
                stat_analysis = visualizations["statistical_analysis"]
                
                if stat_analysis:
                    # Check if it's an HTML or an image
                    if stat_analysis.startswith('<!DOCTYPE html') or stat_analysis.startswith('<html') or stat_analysis.startswith('<div'):
                        stat_visualization = f"""
                        <div class="visualization">
                            <h3>Statistical Analysis</h3>
                            {stat_analysis}
                        </div>
                        """
                    else:
                        # Assume it's a base64-encoded image
                        stat_visualization = f"""
                        <div class="visualization">
                            <h3>Statistical Analysis</h3>
                            <img src="data:image/{self.config['visualization_format']};base64,{stat_analysis}" alt="Statistical Analysis" style="max-width:100%;" />
                        </div>
                        """
            
            statistical_analysis_section = f"""
            <section id="statistical-analysis" class="section">
                <div class="section-header">
                    <h2>Statistical Analysis</h2>
                </div>
                
                <p>This section provides statistical analysis of the validation results, including confidence intervals and error distributions.</p>
                
                {stat_visualization}
            </section>
            """
        
        # Generate detailed results section
        detailed_results_section = ""
        if "detailed_results" in include_sections:
            # Detailed results table
            detailed_table = """
            <table>
                <tr>
                    <th>Hardware</th>
                    <th>Model</th>
                    <th>Batch Size</th>
                    <th>Precision</th>
                    <th>Throughput MAPE</th>
                    <th>Latency MAPE</th>
                    <th>Memory MAPE</th>
                    <th>Power MAPE</th>
                </tr>
            """
            
            max_results = self.config["max_results_per_page"]
            for i, val_result in enumerate(report_data["validation_results"]):
                if i >= max_results:
                    break
                    
                hw_result = val_result.hardware_result
                sim_result = val_result.simulation_result
                
                # Extract MAPE values for each metric
                throughput_mape = val_result.metrics_comparison.get("throughput_items_per_second", {}).get("mape", "N/A")
                latency_mape = val_result.metrics_comparison.get("average_latency_ms", {}).get("mape", "N/A")
                memory_mape = val_result.metrics_comparison.get("memory_peak_mb", {}).get("mape", "N/A")
                power_mape = val_result.metrics_comparison.get("power_consumption_w", {}).get("mape", "N/A")
                
                # Format MAPE values
                throughput_str = "N/A" if throughput_mape == "N/A" else f"{throughput_mape:.2f}%"
                latency_str = "N/A" if latency_mape == "N/A" else f"{latency_mape:.2f}%"
                memory_str = "N/A" if memory_mape == "N/A" else f"{memory_mape:.2f}%"
                power_str = "N/A" if power_mape == "N/A" else f"{power_mape:.2f}%"
                
                detailed_table += f"""
                <tr>
                    <td>{hw_result.hardware_id}</td>
                    <td>{hw_result.model_id}</td>
                    <td>{hw_result.batch_size}</td>
                    <td>{hw_result.precision}</td>
                    <td>{throughput_str}</td>
                    <td>{latency_str}</td>
                    <td>{memory_str}</td>
                    <td>{power_str}</td>
                </tr>
                """
            
            detailed_table += "</table>"
            
            detailed_results_section = f"""
            <section id="detailed-results" class="section">
                <div class="section-header">
                    <h2>Detailed Results</h2>
                </div>
                
                <p>This section shows detailed validation results for individual simulations.</p>
                <p>Showing up to {min(max_results, report_data["total_results"])} of {report_data["total_results"]} results</p>
                
                {detailed_table}
            </section>
            """
        
        # Generate recommendations section
        recommendations_section = ""
        if include_recommendations and "recommendations" in include_sections:
            recommendations_html = ""
            
            # Get recommendations from executive summary
            if report_data.get("executive_summary") and report_data["executive_summary"].get("recommendations"):
                for rec in report_data["executive_summary"]["recommendations"]:
                    priority_class = ""
                    if "current_mape" in rec:
                        if rec["current_mape"] > 25:
                            priority_class = "critical-priority"
                        elif rec["current_mape"] > 15:
                            priority_class = "high-priority"
                    
                    recommendations_html += f"""
                    <div class="recommendation {priority_class}">
                        <div class="recommendation-title">{rec["type"].replace("_", " ").title()}</div>
                        <p class="recommendation-content">{rec["recommendation"]}</p>
                    </div>
                    """
            
            # Add general recommendations based on overall MAPE
            overall_mape = report_data["overall_mape"]
            if overall_mape is not None:
                if overall_mape > 15:
                    recommendations_html += f"""
                    <div class="recommendation high-priority">
                        <div class="recommendation-title">General Calibration Needed</div>
                        <p class="recommendation-content">The overall MAPE of {overall_mape:.2f}% indicates that simulation accuracy needs improvement. Consider running calibration across all models and hardware types.</p>
                    </div>
                    """
                elif overall_mape > 10:
                    recommendations_html += f"""
                    <div class="recommendation">
                        <div class="recommendation-title">Consider Targeted Calibration</div>
                        <p class="recommendation-content">The overall MAPE of {overall_mape:.2f}% is acceptable but could be improved. Focus on calibrating the worst-performing combinations identified in this report.</p>
                    </div>
                    """
                else:
                    recommendations_html += f"""
                    <div class="recommendation">
                        <div class="recommendation-title">Maintain Current Performance</div>
                        <p class="recommendation-content">The overall MAPE of {overall_mape:.2f}% indicates good simulation accuracy. Continue monitoring for drift and consider further fine-tuning for critical workloads.</p>
                    </div>
                    """
            
            # Add recommendation to run drift detection
            recommendations_html += f"""
            <div class="recommendation">
                <div class="recommendation-title">Regular Drift Detection</div>
                <p class="recommendation-content">Run drift detection regularly to identify changes in simulation accuracy over time.</p>
            </div>
            """
            
            recommendations_section = f"""
            <section id="recommendations" class="section">
                <div class="section-header">
                    <h2>Recommendations</h2>
                </div>
                
                <p>Based on the validation results, the following recommendations are provided:</p>
                
                {recommendations_html}
            </section>
            """
        
        # Generate appendix section
        appendix_section = ""
        if "appendix" in include_sections:
            appendix_section = f"""
            <section id="appendix" class="section">
                <div class="section-header">
                    <h2>Appendix</h2>
                </div>
                
                <div class="card">
                    <h3 class="card-title">Report Methodology</h3>
                    <p>This report was generated using the Simulation Accuracy and Validation Framework. It compares simulation results with actual hardware measurements to assess simulation accuracy.</p>
                    <p>The primary metric used is Mean Absolute Percentage Error (MAPE), which measures the average percentage difference between simulated and actual values.</p>
                </div>
                
                <div class="card">
                    <h3 class="card-title">Report Configuration</h3>
                    <p><strong>Format:</strong> HTML</p>
                    <p><strong>Visualization Types:</strong> {', '.join(self.config['visualization_types'])}</p>
                    <p><strong>Generated At:</strong> {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
                </div>
            </section>
            """
        
        # Add auto-refresh if enabled
        auto_refresh = ""
        if self.config["auto_refresh_interval"]:
            interval = self.config["auto_refresh_interval"]
            auto_refresh = f'<meta http-equiv="refresh" content="{interval}">'
        
        # Get report title
        title = report_data.get("custom_title") or self.config["report_title_template"].format(
            timestamp=datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        )
        
        # Generate footer
        footer = self.config["report_footer"]
        
        # Render HTML template
        html_report = html_template.format(
            title=title,
            timestamp=datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            nav_links=nav_links_html,
            executive_summary_section=executive_summary_section,
            overview_section=overview_section,
            hardware_comparison_section=hardware_comparison_section,
            model_comparison_section=model_comparison_section,
            metric_analysis_section=metric_analysis_section,
            statistical_analysis_section=statistical_analysis_section,
            detailed_results_section=detailed_results_section,
            recommendations_section=recommendations_section,
            appendix_section=appendix_section,
            footer=footer,
            auto_refresh=auto_refresh
        )
        
        return html_report
            
    def _generate_enhanced_markdown_report(self, report_data: Dict[str, Any]) -> str:
        """
        Generate an enhanced Markdown report with executive summary and statistical analysis.
        
        Args:
            report_data: Report data including validation results
            
        Returns:
            Markdown report as a string
        """
        # Determine if we should include executive summary, visualizations, etc.
        include_sections = report_data.get("include_sections", self.config["report_sections"])
        include_exec_summary = "executive_summary" in include_sections and report_data.get("executive_summary")
        include_recommendations = report_data.get("include_recommendations", True)
        
        # Get report title
        title = report_data.get("custom_title") or self.config["report_title_template"].format(
            timestamp=datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        )
        
        # Start markdown content
        markdown = f"# {title}\n\n"
        markdown += f"Generated on: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n"
        
        # Add table of contents
        markdown += "## Table of Contents\n\n"
        
        if include_exec_summary and "executive_summary" in include_sections:
            markdown += "1. [Executive Summary](#executive-summary)\n"
        if "overview" in include_sections:
            markdown += "2. [Overview](#overview)\n"
        if "hardware_comparison" in include_sections:
            markdown += "3. [Hardware Comparison](#hardware-comparison)\n"
        if "model_comparison" in include_sections:
            markdown += "4. [Model Comparison](#model-comparison)\n"
        if "metric_analysis" in include_sections:
            markdown += "5. [Metric Analysis](#metric-analysis)\n"
        if "statistical_analysis" in include_sections:
            markdown += "6. [Statistical Analysis](#statistical-analysis)\n"
        if "detailed_results" in include_sections:
            markdown += "7. [Detailed Results](#detailed-results)\n"
        if include_recommendations and "recommendations" in include_sections:
            markdown += "8. [Recommendations](#recommendations)\n"
        
        markdown += "\n"
        
        # Executive Summary section
        if include_exec_summary and "executive_summary" in include_sections:
            exec_summary = report_data["executive_summary"]
            
            markdown += "## Executive Summary\n\n"
            
            # Key metrics
            markdown += "### Key Metrics\n\n"
            markdown += f"- **Total Results:** {exec_summary['total_results']}\n"
            markdown += f"- **Hardware Types:** {exec_summary['total_hardware']}\n"
            markdown += f"- **Model Types:** {exec_summary['total_models']}\n"
            
            if exec_summary["overall_mape"] is not None:
                markdown += f"- **Overall MAPE:** {exec_summary['overall_mape']:.2f}%\n"
                markdown += f"- **Status:** {exec_summary['overall_status']}\n"
            
            # Statistical metrics if available
            if "statistical_metrics" in exec_summary and exec_summary["statistical_metrics"]:
                sm = exec_summary["statistical_metrics"]
                markdown += "\n### Statistical Metrics\n\n"
                
                if "mean" in sm:
                    markdown += f"- **Mean MAPE:** {sm['mean']:.2f}%\n"
                if "median" in sm:
                    markdown += f"- **Median MAPE:** {sm['median']:.2f}%\n"
                if "std_dev" in sm:
                    markdown += f"- **Standard Deviation:** {sm['std_dev']:.2f}%\n"
                if "confidence_interval" in sm and "confidence_level" in sm:
                    ci = sm["confidence_interval"]
                    cl = sm["confidence_level"]
                    markdown += f"- **{cl * 100:.0f}% Confidence Interval:** {ci[0]:.2f}% - {ci[1]:.2f}%\n"
            
            # Best and worst metrics
            if exec_summary.get("best_metric") and exec_summary.get("worst_metric"):
                best_metric, best_value = exec_summary["best_metric"]
                worst_metric, worst_value = exec_summary["worst_metric"]
                
                markdown += "\n### Best and Worst Metrics\n\n"
                markdown += f"- **Best performing metric:** {best_metric} ({best_value:.2f}% MAPE)\n"
                markdown += f"- **Worst performing metric:** {worst_metric} ({worst_value:.2f}% MAPE)\n"
            
            # Best and worst hardware-model combinations
            if exec_summary.get("best_hardware_model") and exec_summary.get("worst_hardware_model"):
                best_hw_model, best_hw_value = exec_summary["best_hardware_model"]
                worst_hw_model, worst_hw_value = exec_summary["worst_hardware_model"]
                
                best_hw, best_model = best_hw_model
                worst_hw, worst_model = worst_hw_model
                
                markdown += "\n### Best and Worst Hardware-Model Combinations\n\n"
                markdown += f"- **Best combination:** {best_model} on {best_hw} ({best_hw_value:.2f}% MAPE)\n"
                markdown += f"- **Worst combination:** {worst_model} on {worst_hw} ({worst_hw_value:.2f}% MAPE)\n"
            
            # Recommendations
            if include_recommendations and exec_summary.get("recommendations"):
                markdown += "\n### Key Recommendations\n\n"
                
                for rec in exec_summary["recommendations"]:
                    markdown += f"- **{rec['type'].replace('_', ' ').title()}:** {rec['recommendation']}\n"
            
            markdown += "\n"
        
        # Overview section
        if "overview" in include_sections:
            total_results = report_data["total_results"]
            overall_mape = f"{report_data['overall_mape']:.2f}%" if report_data['overall_mape'] is not None else "N/A"
            overall_status = report_data['overall_status']
            
            markdown += "## Overview\n\n"
            markdown += "This report analyzes simulation validation results, comparing simulation predictions with actual hardware measurements.\n\n"
            
            markdown += "### Summary\n\n"
            markdown += f"- **Total validation results:** {total_results}\n"
            markdown += f"- **Overall MAPE:** {overall_mape}\n"
            markdown += f"- **Overall status:** {overall_status}\n\n"
            
            markdown += "### What is MAPE?\n\n"
            markdown += "Mean Absolute Percentage Error (MAPE) measures the average percentage difference between simulated and actual values. Lower values indicate better simulation accuracy.\n\n"
            markdown += "- **Excellent (< 5%):** Simulation is highly accurate\n"
            markdown += "- **Good (5-10%):** Simulation is very reliable\n"
            markdown += "- **Acceptable (10-15%):** Simulation is usable but could be improved\n"
            markdown += "- **Problematic (15-25%):** Simulation needs calibration\n"
            markdown += "- **Poor (> 25%):** Simulation requires significant improvement\n\n"
        
        # Hardware comparison section
        if "hardware_comparison" in include_sections:
            markdown += "## Hardware Comparison\n\n"
            markdown += "This section compares simulation accuracy across different hardware types.\n\n"
            
            # Hardware comparison table
            markdown += "| Hardware | Count | MAPE | Status |\n"
            markdown += "| --- | --- | --- | --- |\n"
            
            for hw_id, stats in report_data["hardware_stats"].items():
                if "mean_mape" in stats:
                    markdown += f"| {hw_id} | {stats['count']} | {stats['mean_mape']:.2f}% | {stats['status']} |\n"
            
            markdown += "\n"
        
        # Model comparison section
        if "model_comparison" in include_sections:
            markdown += "## Model Comparison\n\n"
            markdown += "This section compares simulation accuracy across different model types.\n\n"
            
            # Model comparison table
            markdown += "| Model | Count | MAPE | Status |\n"
            markdown += "| --- | --- | --- | --- |\n"
            
            for model_id, stats in report_data["model_stats"].items():
                if "mean_mape" in stats:
                    markdown += f"| {model_id} | {stats['count']} | {stats['mean_mape']:.2f}% | {stats['status']} |\n"
            
            markdown += "\n"
        
        # Metric analysis section
        if "metric_analysis" in include_sections:
            markdown += "## Metric Analysis\n\n"
            markdown += "This section shows validation results grouped by hardware and model combinations.\n\n"
            
            # Hardware-Model comparison table
            markdown += "| Hardware | Model | Count | MAPE | Status |\n"
            markdown += "| --- | --- | --- | --- | --- |\n"
            
            for (hw_id, model_id), stats in report_data["hardware_model_stats"].items():
                if "mean_mape" in stats:
                    markdown += f"| {hw_id} | {model_id} | {stats['count']} | {stats['mean_mape']:.2f}% | {stats['status']} |\n"
            
            markdown += "\n"
        
        # Statistical analysis section
        if "statistical_analysis" in include_sections:
            markdown += "## Statistical Analysis\n\n"
            markdown += "This section provides statistical analysis of the validation results, including confidence intervals and error distributions.\n\n"
            markdown += "_Note: Visualizations are not available in Markdown format. Please use HTML format to view visualizations._\n\n"
        
        # Detailed results section
        if "detailed_results" in include_sections:
            markdown += "## Detailed Results\n\n"
            markdown += "This section shows detailed validation results for individual simulations.\n\n"
            
            max_results = self.config["max_results_per_page"]
            markdown += f"Showing up to {min(max_results, report_data['total_results'])} of {report_data['total_results']} results\n\n"
            
            # Detailed results table
            markdown += "| Hardware | Model | Batch Size | Precision | Throughput MAPE | Latency MAPE | Memory MAPE | Power MAPE |\n"
            markdown += "| --- | --- | --- | --- | --- | --- | --- | --- |\n"
            
            for i, val_result in enumerate(report_data["validation_results"]):
                if i >= max_results:
                    break
                    
                hw_result = val_result.hardware_result
                sim_result = val_result.simulation_result
                
                # Extract MAPE values for each metric
                throughput_mape = val_result.metrics_comparison.get("throughput_items_per_second", {}).get("mape", "N/A")
                latency_mape = val_result.metrics_comparison.get("average_latency_ms", {}).get("mape", "N/A")
                memory_mape = val_result.metrics_comparison.get("memory_peak_mb", {}).get("mape", "N/A")
                power_mape = val_result.metrics_comparison.get("power_consumption_w", {}).get("mape", "N/A")
                
                # Format MAPE values
                throughput_str = "N/A" if throughput_mape == "N/A" else f"{throughput_mape:.2f}%"
                latency_str = "N/A" if latency_mape == "N/A" else f"{latency_mape:.2f}%"
                memory_str = "N/A" if memory_mape == "N/A" else f"{memory_mape:.2f}%"
                power_str = "N/A" if power_mape == "N/A" else f"{power_mape:.2f}%"
                
                markdown += f"| {hw_result.hardware_id} | {hw_result.model_id} | {hw_result.batch_size} | {hw_result.precision} | {throughput_str} | {latency_str} | {memory_str} | {power_str} |\n"
            
            markdown += "\n"
        
        # Recommendations section
        if include_recommendations and "recommendations" in include_sections:
            markdown += "## Recommendations\n\n"
            markdown += "Based on the validation results, the following recommendations are provided:\n\n"
            
            # Get recommendations from executive summary
            if report_data.get("executive_summary") and report_data["executive_summary"].get("recommendations"):
                for rec in report_data["executive_summary"]["recommendations"]:
                    markdown += f"### {rec['type'].replace('_', ' ').title()}\n\n"
                    markdown += f"{rec['recommendation']}\n\n"
            
            # Add general recommendations based on overall MAPE
            overall_mape = report_data["overall_mape"]
            if overall_mape is not None:
                if overall_mape > 15:
                    markdown += "### General Calibration Needed\n\n"
                    markdown += f"The overall MAPE of {overall_mape:.2f}% indicates that simulation accuracy needs improvement. Consider running calibration across all models and hardware types.\n\n"
                elif overall_mape > 10:
                    markdown += "### Consider Targeted Calibration\n\n"
                    markdown += f"The overall MAPE of {overall_mape:.2f}% is acceptable but could be improved. Focus on calibrating the worst-performing combinations identified in this report.\n\n"
                else:
                    markdown += "### Maintain Current Performance\n\n"
                    markdown += f"The overall MAPE of {overall_mape:.2f}% indicates good simulation accuracy. Continue monitoring for drift and consider further fine-tuning for critical workloads.\n\n"
            
            # Add recommendation to run drift detection
            markdown += "### Regular Drift Detection\n\n"
            markdown += "Run drift detection regularly to identify changes in simulation accuracy over time.\n\n"
        
        # Footer
        markdown += "---\n\n"
        markdown += f"*{self.config['report_footer']}*"
        
        return markdown

    def _generate_enhanced_json_report(self, report_data: Dict[str, Any]) -> str:
        """
        Generate an enhanced JSON report.
        
        Args:
            report_data: Report data including validation results
            
        Returns:
            JSON report as a string
        """
        # Create a simplified version of report data that can be serialized to JSON
        json_data = {
            "title": report_data.get("custom_title") or self.config["report_title_template"].format(
                timestamp=datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            ),
            "timestamp": datetime.datetime.now().isoformat(),
            "total_results": report_data["total_results"],
            "overall_metrics": {
                "mape": report_data["overall_mape"],
                "status": report_data["overall_status"]
            }
        }
        
        # Add executive summary if available
        if report_data.get("executive_summary"):
            exec_summary = report_data["executive_summary"]
            
            # Remove validation_result_ids to make serializable
            if "statistical_metrics" in exec_summary:
                sm = exec_summary["statistical_metrics"]
                if "confidence_interval" in sm and isinstance(sm["confidence_interval"], tuple):
                    sm["confidence_interval"] = list(sm["confidence_interval"])
            
            json_data["executive_summary"] = exec_summary
        
        # Hardware stats
        hardware_stats = {}
        for hw_id, stats in report_data["hardware_stats"].items():
            if "mean_mape" in stats:
                hardware_stats[hw_id] = {
                    "count": stats["count"],
                    "mean_mape": stats["mean_mape"],
                    "status": stats["status"]
                }
        json_data["hardware_stats"] = hardware_stats
        
        # Model stats
        model_stats = {}
        for model_id, stats in report_data["model_stats"].items():
            if "mean_mape" in stats:
                model_stats[model_id] = {
                    "count": stats["count"],
                    "mean_mape": stats["mean_mape"],
                    "status": stats["status"]
                }
        json_data["model_stats"] = model_stats
        
        # Hardware-Model stats
        hw_model_stats = {}
        for (hw_id, model_id), stats in report_data["hardware_model_stats"].items():
            if "mean_mape" in stats:
                key = f"{hw_id}__{model_id}"  # Use string key for JSON
                hw_model_stats[key] = {
                    "hardware_id": hw_id,
                    "model_id": model_id,
                    "count": stats["count"],
                    "mean_mape": stats["mean_mape"],
                    "status": stats["status"]
                }
        json_data["hardware_model_stats"] = hw_model_stats
        
        # Detailed results (limited)
        detailed_results = []
        max_results = self.config["max_results_per_page"]
        
        for i, val_result in enumerate(report_data["validation_results"]):
            if i >= max_results:
                break
                
            hw_result = val_result.hardware_result
            sim_result = val_result.simulation_result
            
            # Extract MAPE values for each metric
            throughput_mape = val_result.metrics_comparison.get("throughput_items_per_second", {}).get("mape")
            latency_mape = val_result.metrics_comparison.get("average_latency_ms", {}).get("mape")
            memory_mape = val_result.metrics_comparison.get("memory_peak_mb", {}).get("mape")
            power_mape = val_result.metrics_comparison.get("power_consumption_w", {}).get("mape")
            
            detailed_results.append({
                "hardware_id": hw_result.hardware_id,
                "model_id": hw_result.model_id,
                "batch_size": hw_result.batch_size,
                "precision": hw_result.precision,
                "mape_values": {
                    "throughput_items_per_second": throughput_mape,
                    "average_latency_ms": latency_mape,
                    "memory_peak_mb": memory_mape,
                    "power_consumption_w": power_mape
                }
            })
        
        json_data["detailed_results"] = detailed_results
        
        # Report metadata
        json_data["report_metadata"] = {
            "format": "json",
            "version": "1.0",
            "generation_time": datetime.datetime.now().isoformat(),
            "config": {
                "visualization_types": self.config["visualization_types"],
                "report_sections": self.config["report_sections"]
            }
        }
        
        # Convert to JSON string
        try:
            return json.dumps(json_data, indent=2)
        except Exception as e:
            logger.error(f"Error generating JSON report: {e}")
            return json.dumps({"error": f"Error generating JSON report: {e}"})
            
    def _generate_csv_report(self, report_data: Dict[str, Any]) -> str:
        """
        Generate a CSV report of validation results.
        
        Args:
            report_data: Report data including validation results
            
        Returns:
            CSV report as a string
        """
        import io
        import csv
        
        output = io.StringIO()
        writer = csv.writer(output)
        
        # Write header row
        writer.writerow([
            "Hardware ID", "Model ID", "Batch Size", "Precision",
            "Throughput MAPE (%)", "Latency MAPE (%)", "Memory MAPE (%)", "Power MAPE (%)",
            "Hardware Throughput", "Hardware Latency", "Hardware Memory", "Hardware Power",
            "Simulation Throughput", "Simulation Latency", "Simulation Memory", "Simulation Power",
            "Validation Timestamp"
        ])
        
        # Write data rows
        for val_result in report_data["validation_results"]:
            hw_result = val_result.hardware_result
            sim_result = val_result.simulation_result
            
            # Extract MAPE values
            throughput_mape = val_result.metrics_comparison.get("throughput_items_per_second", {}).get("mape", "")
            latency_mape = val_result.metrics_comparison.get("average_latency_ms", {}).get("mape", "")
            memory_mape = val_result.metrics_comparison.get("memory_peak_mb", {}).get("mape", "")
            power_mape = val_result.metrics_comparison.get("power_consumption_w", {}).get("mape", "")
            
            # Extract hardware metric values
            hw_throughput = hw_result.metrics.get("throughput_items_per_second", "")
            hw_latency = hw_result.metrics.get("average_latency_ms", "")
            hw_memory = hw_result.metrics.get("memory_peak_mb", "")
            hw_power = hw_result.metrics.get("power_consumption_w", "")
            
            # Extract simulation metric values
            sim_throughput = sim_result.metrics.get("throughput_items_per_second", "")
            sim_latency = sim_result.metrics.get("average_latency_ms", "")
            sim_memory = sim_result.metrics.get("memory_peak_mb", "")
            sim_power = sim_result.metrics.get("power_consumption_w", "")
            
            writer.writerow([
                hw_result.hardware_id,
                hw_result.model_id,
                hw_result.batch_size,
                hw_result.precision,
                f"{throughput_mape:.2f}" if isinstance(throughput_mape, (int, float)) else "",
                f"{latency_mape:.2f}" if isinstance(latency_mape, (int, float)) else "",
                f"{memory_mape:.2f}" if isinstance(memory_mape, (int, float)) else "",
                f"{power_mape:.2f}" if isinstance(power_mape, (int, float)) else "",
                hw_throughput,
                hw_latency,
                hw_memory,
                hw_power,
                sim_throughput,
                sim_latency,
                sim_memory,
                sim_power,
                val_result.validation_timestamp
            ])
        
        # Get CSV content as string
        csv_content = output.getvalue()
        output.close()
        
        return csv_content
            
    def _generate_pdf_report(self, report_data: Dict[str, Any], output_type: str = "string") -> str:
        """
        Generate a PDF report based on HTML content.
        
        Args:
            report_data: Report data including validation results
            output_type: Type of output - "string" (returns PDF bytes as base64) or "file" (returns file path)
            
        Returns:
            PDF content as base64 string or file path depending on output_type
        """
        # Generate HTML report
        html_content = self._generate_enhanced_html_report(report_data)
        
        # Try to use one of the PDF conversion engines (if available)
        pdf_engine = self.config["pdf_engine"]
        
        if pdf_engine == "weasyprint":
            try:
                import weasyprint
                import io
                import tempfile
                
                # Create a temporary file for the HTML
                with tempfile.NamedTemporaryFile(mode='w', suffix='.html', delete=False) as f:
                    f.write(html_content)
                    html_path = f.name
                
                # Use weasyprint to convert HTML to PDF
                pdf = weasyprint.HTML(filename=html_path).write_pdf()
                
                # Clean up temp file
                import os
                os.unlink(html_path)
                
                if output_type == "string":
                    # Return as base64 string
                    return base64.b64encode(pdf).decode('utf-8')
                else:
                    # Save to a temporary file and return the path
                    with tempfile.NamedTemporaryFile(mode='wb', suffix='.pdf', delete=False) as f:
                        f.write(pdf)
                        return f.name
                        
            except ImportError:
                logger.warning("weasyprint not installed, falling back to HTML report")
        
        elif pdf_engine == "wkhtmltopdf":
            try:
                import pdfkit
                import tempfile
                
                # Create a temporary file for the HTML
                with tempfile.NamedTemporaryFile(mode='w', suffix='.html', delete=False) as f:
                    f.write(html_content)
                    html_path = f.name
                
                # Create a temporary file for the PDF
                pdf_path = tempfile.mktemp(suffix='.pdf')
                
                # Convert HTML to PDF
                pdfkit.from_file(html_path, pdf_path)
                
                # Clean up HTML temp file
                import os
                os.unlink(html_path)
                
                if output_type == "string":
                    # Read PDF file and convert to base64
                    with open(pdf_path, 'rb') as f:
                        pdf_content = f.read()
                    
                    os.unlink(pdf_path)
                    return base64.b64encode(pdf_content).decode('utf-8')
                else:
                    # Return the PDF file path
                    return pdf_path
                    
            except ImportError:
                logger.warning("pdfkit not installed, falling back to HTML report")
        
        # If PDF conversion failed, return HTML instead
        logger.warning(f"PDF engine {pdf_engine} failed, returning HTML instead")
        return html_content
            
    def _generate_enhanced_text_report(self, report_data: Dict[str, Any]) -> str:
        """
        Generate an enhanced plain text report.
        
        Args:
            report_data: Report data including validation results
            
        Returns:
            Text report as a string
        """
        # Determine if we should include executive summary
        include_sections = report_data.get("include_sections", self.config["report_sections"])
        include_exec_summary = "executive_summary" in include_sections and report_data.get("executive_summary")
        
        # Get report title
        title = report_data.get("custom_title") or self.config["report_title_template"].format(
            timestamp=datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        )
        
        # Start text content
        text = f"{title}\n"
        text += "=" * len(title) + "\n\n"
        text += f"Generated on: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n"
        
        # Executive Summary section
        if include_exec_summary:
            exec_summary = report_data["executive_summary"]
            
            text += "EXECUTIVE SUMMARY\n"
            text += "=================\n\n"
            
            # Key metrics
            text += "Key Metrics:\n"
            text += f"- Total Results: {exec_summary['total_results']}\n"
            text += f"- Hardware Types: {exec_summary['total_hardware']}\n"
            text += f"- Model Types: {exec_summary['total_models']}\n"
            
            if exec_summary["overall_mape"] is not None:
                text += f"- Overall MAPE: {exec_summary['overall_mape']:.2f}%\n"
                text += f"- Status: {exec_summary['overall_status']}\n"
            
            text += "\n"
            
            # Best and worst metrics
            if exec_summary.get("best_metric") and exec_summary.get("worst_metric"):
                best_metric, best_value = exec_summary["best_metric"]
                worst_metric, worst_value = exec_summary["worst_metric"]
                
                text += "Best and Worst Metrics:\n"
                text += f"- Best performing metric: {best_metric} ({best_value:.2f}% MAPE)\n"
                text += f"- Worst performing metric: {worst_metric} ({worst_value:.2f}% MAPE)\n\n"
            
            # Recommendations
            if exec_summary.get("recommendations"):
                text += "Key Recommendations:\n"
                
                for rec in exec_summary["recommendations"]:
                    text += f"- {rec['type'].replace('_', ' ').title()}: {rec['recommendation']}\n"
                
                text += "\n"
        
        # Overview section
        if "overview" in include_sections:
            text += "OVERVIEW\n"
            text += "========\n\n"
            
            total_results = report_data["total_results"]
            overall_mape = f"{report_data['overall_mape']:.2f}%" if report_data['overall_mape'] is not None else "N/A"
            overall_status = report_data['overall_status']
            
            text += f"Total validation results: {total_results}\n"
            text += f"Overall MAPE: {overall_mape}\n"
            text += f"Overall status: {overall_status}\n\n"
        
        # Hardware comparison section
        if "hardware_comparison" in include_sections:
            text += "HARDWARE COMPARISON\n"
            text += "===================\n\n"
            
            col_widths = {
                "hardware": 25,
                "count": 10,
                "mape": 10,
                "status": 15
            }
            
            # Header
            text += f"{'Hardware':<{col_widths['hardware']}} {'Count':<{col_widths['count']}} {'MAPE':<{col_widths['mape']}} {'Status':<{col_widths['status']}}\n"
            text += "-" * (sum(col_widths.values()) + len(col_widths) - 1) + "\n"
            
            # Data rows
            for hw_id, stats in report_data["hardware_stats"].items():
                if "mean_mape" in stats:
                    text += f"{hw_id:<{col_widths['hardware']}} {stats['count']:<{col_widths['count']}} {stats['mean_mape']:.2f}%{'':<{col_widths['mape']-6}} {stats['status']:<{col_widths['status']}}\n"
            
            text += "\n"
        
        # Model comparison section
        if "model_comparison" in include_sections:
            text += "MODEL COMPARISON\n"
            text += "================\n\n"
            
            col_widths = {
                "model": 30,
                "count": 10,
                "mape": 10,
                "status": 15
            }
            
            # Header
            text += f"{'Model':<{col_widths['model']}} {'Count':<{col_widths['count']}} {'MAPE':<{col_widths['mape']}} {'Status':<{col_widths['status']}}\n"
            text += "-" * (sum(col_widths.values()) + len(col_widths) - 1) + "\n"
            
            # Data rows
            for model_id, stats in report_data["model_stats"].items():
                if "mean_mape" in stats:
                    text += f"{model_id:<{col_widths['model']}} {stats['count']:<{col_widths['count']}} {stats['mean_mape']:.2f}%{'':<{col_widths['mape']-6}} {stats['status']:<{col_widths['status']}}\n"
            
            text += "\n"
        
        # Detailed results section
        if "detailed_results" in include_sections:
            text += "DETAILED RESULTS\n"
            text += "================\n\n"
            
            max_results = self.config["max_results_per_page"]
            text += f"Showing up to {min(max_results, report_data['total_results'])} of {report_data['total_results']} results\n\n"
            
            col_widths = {
                "hardware": 20,
                "model": 20,
                "batch": 10,
                "precision": 10,
                "throughput": 15,
                "latency": 15,
                "memory": 15,
                "power": 15
            }
            
            # Header
            text += f"{'Hardware':<{col_widths['hardware']}} {'Model':<{col_widths['model']}} {'Batch':<{col_widths['batch']}} {'Precision':<{col_widths['precision']}} "
            text += f"{'Throughput':<{col_widths['throughput']}} {'Latency':<{col_widths['latency']}} {'Memory':<{col_widths['memory']}} {'Power':<{col_widths['power']}}\n"
            text += "-" * (sum(col_widths.values()) + len(col_widths) - 1) + "\n"
            
            # Data rows
            for i, val_result in enumerate(report_data["validation_results"]):
                if i >= max_results:
                    break
                    
                hw_result = val_result.hardware_result
                sim_result = val_result.simulation_result
                
                # Extract MAPE values for each metric
                throughput_mape = val_result.metrics_comparison.get("throughput_items_per_second", {}).get("mape", "N/A")
                latency_mape = val_result.metrics_comparison.get("average_latency_ms", {}).get("mape", "N/A")
                memory_mape = val_result.metrics_comparison.get("memory_peak_mb", {}).get("mape", "N/A")
                power_mape = val_result.metrics_comparison.get("power_consumption_w", {}).get("mape", "N/A")
                
                # Format MAPE values
                throughput_str = "N/A" if throughput_mape == "N/A" else f"{throughput_mape:.2f}%"
                latency_str = "N/A" if latency_mape == "N/A" else f"{latency_mape:.2f}%"
                memory_str = "N/A" if memory_mape == "N/A" else f"{memory_mape:.2f}%"
                power_str = "N/A" if power_mape == "N/A" else f"{power_mape:.2f}%"
                
                text += f"{hw_result.hardware_id:<{col_widths['hardware']}} {hw_result.model_id:<{col_widths['model']}} {hw_result.batch_size:<{col_widths['batch']}} {hw_result.precision:<{col_widths['precision']}} "
                text += f"{throughput_str:<{col_widths['throughput']}} {latency_str:<{col_widths['latency']}} {memory_str:<{col_widths['memory']}} {power_str:<{col_widths['power']}}\n"
            
            text += "\n"
        
        # Footer
        text += "-" * 80 + "\n"
        text += f"Report generated by the Simulation Accuracy and Validation Framework"
        
        return text
    
    def export_report(
        self,
        validation_results: List[ValidationResult],
        output_path: str,
        format: str = "html",
        include_visualizations: bool = True,
        include_executive_summary: Optional[bool] = None,
        include_statistical_analysis: Optional[bool] = None,
        include_recommendations: Optional[bool] = None,
        include_sections: Optional[List[str]] = None,
        custom_title: Optional[str] = None,
        hardware_filter: Optional[str] = None,
        model_filter: Optional[str] = None,
        date_range: Optional[Tuple[str, str]] = None
    ) -> str:
        """
        Export a comprehensive validation report to a file.
        
        Args:
            validation_results: List of validation results
            output_path: Path to save the report
            format: Output format (html, markdown, json, csv, pdf)
            include_visualizations: Whether to include visualizations
            include_executive_summary: Whether to include executive summary (default: from config)
            include_statistical_analysis: Whether to include statistical analysis (default: from config)
            include_recommendations: Whether to include recommendations (default: from config)
            include_sections: List of specific sections to include (default: all sections from config)
            custom_title: Custom title for the report (default: from config template)
            hardware_filter: Filter results by hardware ID
            model_filter: Filter results by model ID
            date_range: Filter results by date range (start_date, end_date)
            
        Returns:
            Path to the saved report
        """
        # Generate enhanced report content
        report_content = self.generate_report(
            validation_results=validation_results,
            format=format,
            include_visualizations=include_visualizations,
            include_executive_summary=include_executive_summary,
            include_statistical_analysis=include_statistical_analysis,
            include_recommendations=include_recommendations,
            include_sections=include_sections,
            custom_title=custom_title,
            hardware_filter=hardware_filter,
            model_filter=model_filter,
            date_range=date_range,
            output_type="file" if format == "pdf" else "string"
        )
        
        # For PDF format with file output_type, report_content is already the path to the PDF file
        if format == "pdf" and not report_content.startswith("<!DOCTYPE html"):
            logger.info(f"PDF report saved to {report_content}")
            
            # If the output_path is different from the report_content path, rename the file
            if report_content != output_path:
                try:
                    # Create output directory if it doesn't exist
                    output_dir = os.path.dirname(output_path)
                    if output_dir and not os.path.exists(output_dir):
                        os.makedirs(output_dir, exist_ok=True)
                        
                    # Rename the file
                    import shutil
                    shutil.copy2(report_content, output_path)
                    os.unlink(report_content)  # Delete the temporary file
                    logger.info(f"PDF report copied from {report_content} to {output_path}")
                    return output_path
                except Exception as e:
                    logger.error(f"Error copying PDF report from {report_content} to {output_path}: {e}")
                    return report_content  # Return the temporary file path
            
            return report_content
        
        # For all other formats, create output directory if it doesn't exist
        output_dir = os.path.dirname(output_path)
        if output_dir and not os.path.exists(output_dir):
            os.makedirs(output_dir, exist_ok=True)
        
        # Write report to file with appropriate mode
        try:
            # Base64-encoded content (like for PDF) needs to be decoded first
            if format == "pdf" and report_content.startswith(("<!DOCTYPE html", "<html")):
                # Handle HTML fallback for PDF
                write_mode = 'w'
                decoded_content = report_content
            elif format == "pdf":
                # Handle base64-encoded PDF content
                write_mode = 'wb'
                try:
                    decoded_content = base64.b64decode(report_content)
                except Exception:
                    # If decoding fails, assume it's HTML fallback
                    write_mode = 'w'
                    decoded_content = report_content
            elif format == "csv":
                # CSV is text but may have special characters
                write_mode = 'w'
                decoded_content = report_content
            else:
                # HTML, Markdown, JSON, etc.
                write_mode = 'w'
                decoded_content = report_content
            
            with open(output_path, write_mode) as f:
                f.write(decoded_content)
                
            logger.info(f"Report saved to {output_path}")
            return output_path
            
        except Exception as e:
            logger.error(f"Error saving report to {output_path}: {e}")
            return f"Error saving report: {e}"
    
    def _prepare_report_data(self, validation_results: List[ValidationResult]) -> Dict[str, Any]:
        """
        Prepare data for the report.
        
        Args:
            validation_results: List of validation results
            
        Returns:
            Dictionary with report data
        """
        # Group validation results by hardware and model
        hardware_results = defaultdict(list)
        model_results = defaultdict(list)
        hardware_model_results = defaultdict(list)
        
        # Extract metrics and errors
        all_mape_values = []
        all_hw_types = set()
        all_model_types = set()
        all_metric_names = set()
        
        for vr in validation_results:
            hw_id = vr.hardware_result.hardware_id
            model_id = vr.hardware_result.model_id
            
            hardware_results[hw_id].append(vr)
            model_results[model_id].append(vr)
            hardware_model_results[(hw_id, model_id)].append(vr)
            
            all_hw_types.add(hw_id)
            all_model_types.add(model_id)
            
            # Extract metrics and errors
            for metric_name, comparison in vr.metrics_comparison.items():
                all_metric_names.add(metric_name)
                if "mape" in comparison:
                    all_mape_values.append(comparison["mape"])
        
        # Calculate overall metrics
        overall_mape = None
        if all_mape_values:
            overall_mape = sum(all_mape_values) / len(all_mape_values)
            
        overall_status = self._interpret_mape(overall_mape)
        
        # Calculate hardware-specific metrics
        hardware_stats = {}
        for hw_id, results in hardware_results.items():
            hw_mape_values = []
            for vr in results:
                for comparison in vr.metrics_comparison.values():
                    if "mape" in comparison:
                        hw_mape_values.append(comparison["mape"])
            
            if hw_mape_values:
                mean_mape = sum(hw_mape_values) / len(hw_mape_values)
                status = self._interpret_mape(mean_mape)
                
                hardware_stats[hw_id] = {
                    "count": len(results),
                    "mean_mape": mean_mape,
                    "status": status
                }
                
                if NUMPY_AVAILABLE and len(hw_mape_values) > 1:
                    hardware_stats[hw_id].update({
                        "std_dev": np.std(hw_mape_values),
                        "median": np.median(hw_mape_values),
                        "min": min(hw_mape_values),
                        "max": max(hw_mape_values)
                    })
        
        # Calculate model-specific metrics
        model_stats = {}
        for model_id, results in model_results.items():
            model_mape_values = []
            for vr in results:
                for comparison in vr.metrics_comparison.values():
                    if "mape" in comparison:
                        model_mape_values.append(comparison["mape"])
            
            if model_mape_values:
                mean_mape = sum(model_mape_values) / len(model_mape_values)
                status = self._interpret_mape(mean_mape)
                
                model_stats[model_id] = {
                    "count": len(results),
                    "mean_mape": mean_mape,
                    "status": status
                }
                
                if NUMPY_AVAILABLE and len(model_mape_values) > 1:
                    model_stats[model_id].update({
                        "std_dev": np.std(model_mape_values),
                        "median": np.median(model_mape_values),
                        "min": min(model_mape_values),
                        "max": max(model_mape_values)
                    })
        
        # Calculate metric-specific statistics
        metric_stats = {}
        for metric_name in all_metric_names:
            metric_mape_values = []
            for vr in validation_results:
                if metric_name in vr.metrics_comparison:
                    comparison = vr.metrics_comparison[metric_name]
                    if "mape" in comparison:
                        metric_mape_values.append(comparison["mape"])
            
            if metric_mape_values:
                mean_mape = sum(metric_mape_values) / len(metric_mape_values)
                status = self._interpret_mape(mean_mape)
                
                metric_stats[metric_name] = {
                    "count": len(metric_mape_values),
                    "mean_mape": mean_mape,
                    "status": status
                }
                
                if NUMPY_AVAILABLE and len(metric_mape_values) > 1:
                    metric_stats[metric_name].update({
                        "std_dev": np.std(metric_mape_values),
                        "median": np.median(metric_mape_values),
                        "min": min(metric_mape_values),
                        "max": max(metric_mape_values)
                    })
        
        # Calculate hardware-model combinations
        hardware_model_stats = {}
        for (hw_id, model_id), results in hardware_model_results.items():
            hm_mape_values = []
            for vr in results:
                for comparison in vr.metrics_comparison.values():
                    if "mape" in comparison:
                        hm_mape_values.append(comparison["mape"])
            
            if hm_mape_values:
                mean_mape = sum(hm_mape_values) / len(hm_mape_values)
                status = self._interpret_mape(mean_mape)
                
                hardware_model_stats[(hw_id, model_id)] = {
                    "count": len(results),
                    "mean_mape": mean_mape,
                    "status": status
                }
        
        # Prepare report data
        report_data = {
            "timestamp": datetime.datetime.now().isoformat(),
            "total_results": len(validation_results),
            "total_hardware_types": len(all_hw_types),
            "total_model_types": len(all_model_types),
            "overall_mape": overall_mape,
            "overall_status": overall_status,
            "hardware_stats": hardware_stats,
            "model_stats": model_stats,
            "metric_stats": metric_stats,
            "hardware_model_stats": hardware_model_stats,
            "validation_results": validation_results
        }
        
        return report_data
    
    def _generate_html_report(self, report_data: Dict[str, Any], include_visualizations: bool) -> str:
        """
        Generate an HTML report.
        
        Args:
            report_data: Report data
            include_visualizations: Whether to include visualizations
            
        Returns:
            HTML report as a string
        """
        # Basic HTML template
        html_template = """
        <!DOCTYPE html>
        <html lang="en">
        <head>
            <meta charset="UTF-8">
            <meta name="viewport" content="width=device-width, initial-scale=1.0">
            <title>{title}</title>
            <style>
                body {{
                    font-family: Arial, sans-serif;
                    line-height: 1.6;
                    margin: 0;
                    padding: 20px;
                    color: #333;
                }}
                h1, h2, h3, h4 {{
                    color: #2c3e50;
                }}
                table {{
                    border-collapse: collapse;
                    width: 100%;
                    margin-bottom: 20px;
                }}
                th, td {{
                    border: 1px solid #ddd;
                    padding: 8px;
                    text-align: left;
                }}
                th {{
                    background-color: #f2f2f2;
                }}
                tr:nth-child(even) {{
                    background-color: #f9f9f9;
                }}
                .status-excellent {{
                    color: #27ae60;
                    font-weight: bold;
                }}
                .status-good {{
                    color: #2ecc71;
                }}
                .status-acceptable {{
                    color: #f39c12;
                }}
                .status-problematic {{
                    color: #e67e22;
                }}
                .status-poor {{
                    color: #e74c3c;
                    font-weight: bold;
                }}
                .summary-box {{
                    background-color: #f8f9fa;
                    border: 1px solid #ddd;
                    border-radius: 4px;
                    padding: 15px;
                    margin-bottom: 20px;
                }}
                .metric-value {{
                    font-weight: bold;
                }}
                .visualization {{
                    margin-top: 20px;
                    margin-bottom: 20px;
                    border: 1px solid #ddd;
                    padding: 10px;
                    border-radius: 4px;
                }}
                .visualization-placeholder {{
                    background-color: #f8f9fa;
                    height: 300px;
                    display: flex;
                    align-items: center;
                    justify-content: center;
                    font-style: italic;
                    color: #777;
                }}
            </style>
        </head>
        <body>
            <h1>{title}</h1>
            <p>Generated on: {timestamp}</p>
            
            <div class="summary-box">
                <h2>Summary</h2>
                <p>Total validation results: <span class="metric-value">{total_results}</span></p>
                <p>Overall MAPE: <span class="metric-value">{overall_mape:.2f}%</span></p>
                <p>Overall status: <span class="status-{overall_status_class}">{overall_status}</span></p>
            </div>
            
            {hardware_summary}
            
            {model_summary}
            
            {hardware_model_summary}
            
            {visualizations}
            
            {detailed_results}
            
            <p><em>Report generated by the Simulation Accuracy and Validation Framework</em></p>
        </body>
        </html>
        """
        
        # Generate hardware summary
        hardware_summary = "<h2>Results by Hardware</h2>"
        hardware_summary += "<table>"
        hardware_summary += "<tr><th>Hardware</th><th>Count</th><th>MAPE</th><th>Status</th></tr>"
        
        for hw_id, stats in report_data["hardware_stats"].items():
            if "mean_mape" in stats:
                status_class = self._status_class(stats["status"])
                hardware_summary += f"<tr>"
                hardware_summary += f"<td>{hw_id}</td>"
                hardware_summary += f"<td>{stats['count']}</td>"
                hardware_summary += f"<td>{stats['mean_mape']:.2f}%</td>"
                hardware_summary += f"<td class='status-{status_class}'>{stats['status']}</td>"
                hardware_summary += f"</tr>"
        
        hardware_summary += "</table>"
        
        # Generate model summary
        model_summary = "<h2>Results by Model</h2>"
        model_summary += "<table>"
        model_summary += "<tr><th>Model</th><th>Count</th><th>MAPE</th><th>Status</th></tr>"
        
        for model_id, stats in report_data["model_stats"].items():
            if "mean_mape" in stats:
                status_class = self._status_class(stats["status"])
                model_summary += f"<tr>"
                model_summary += f"<td>{model_id}</td>"
                model_summary += f"<td>{stats['count']}</td>"
                model_summary += f"<td>{stats['mean_mape']:.2f}%</td>"
                model_summary += f"<td class='status-{status_class}'>{stats['status']}</td>"
                model_summary += f"</tr>"
        
        model_summary += "</table>"
        
        # Generate hardware-model summary
        hardware_model_summary = "<h2>Results by Hardware and Model</h2>"
        hardware_model_summary += "<table>"
        hardware_model_summary += "<tr><th>Hardware</th><th>Model</th><th>Count</th><th>MAPE</th><th>Status</th></tr>"
        
        for (hw_id, model_id), stats in report_data["hardware_model_stats"].items():
            if "mean_mape" in stats:
                status_class = self._status_class(stats["status"])
                hardware_model_summary += f"<tr>"
                hardware_model_summary += f"<td>{hw_id}</td>"
                hardware_model_summary += f"<td>{model_id}</td>"
                hardware_model_summary += f"<td>{stats['count']}</td>"
                hardware_model_summary += f"<td>{stats['mean_mape']:.2f}%</td>"
                hardware_model_summary += f"<td class='status-{status_class}'>{stats['status']}</td>"
                hardware_model_summary += f"</tr>"
        
        hardware_model_summary += "</table>"
        
        # Generate visualizations section
        visualizations = ""
        if include_visualizations:
            visualizations = "<h2>Visualizations</h2>"
            visualizations += "<div class='visualization'>"
            visualizations += "<h3>Error Distribution</h3>"
            visualizations += "<div class='visualization-placeholder'>Visualization placeholder: Error distribution chart would be shown here</div>"
            visualizations += "</div>"
            
            visualizations += "<div class='visualization'>"
            visualizations += "<h3>Error by Metric</h3>"
            visualizations += "<div class='visualization-placeholder'>Visualization placeholder: Error by metric chart would be shown here</div>"
            visualizations += "</div>"
            
            visualizations += "<div class='visualization'>"
            visualizations += "<h3>Hardware Comparison</h3>"
            visualizations += "<div class='visualization-placeholder'>Visualization placeholder: Hardware comparison chart would be shown here</div>"
            visualizations += "</div>"
        
        # Generate detailed results section (limited to max_results_per_page)
        detailed_results = "<h2>Detailed Results</h2>"
        detailed_results += "<p>Showing up to {0} of {1} results</p>".format(
            min(self.config["max_results_per_page"], report_data["total_results"]),
            report_data["total_results"]
        )
        
        detailed_results += "<table>"
        detailed_results += "<tr>"
        detailed_results += "<th>Hardware</th>"
        detailed_results += "<th>Model</th>"
        detailed_results += "<th>Batch Size</th>"
        detailed_results += "<th>Precision</th>"
        detailed_results += "<th>Throughput MAPE</th>"
        detailed_results += "<th>Latency MAPE</th>"
        detailed_results += "<th>Memory MAPE</th>"
        detailed_results += "<th>Power MAPE</th>"
        detailed_results += "</tr>"
        
        for i, val_result in enumerate(report_data["validation_results"]):
            if i >= self.config["max_results_per_page"]:
                break
                
            hw_result = val_result.hardware_result
            sim_result = val_result.simulation_result
            
            # Extract MAPE values for each metric
            throughput_mape = val_result.metrics_comparison.get("throughput_items_per_second", {}).get("mape", "N/A")
            latency_mape = val_result.metrics_comparison.get("average_latency_ms", {}).get("mape", "N/A")
            memory_mape = val_result.metrics_comparison.get("memory_peak_mb", {}).get("mape", "N/A")
            power_mape = val_result.metrics_comparison.get("power_consumption_w", {}).get("mape", "N/A")
            
            detailed_results += "<tr>"
            detailed_results += f"<td>{hw_result.hardware_id}</td>"
            detailed_results += f"<td>{hw_result.model_id}</td>"
            detailed_results += f"<td>{hw_result.batch_size}</td>"
            detailed_results += f"<td>{hw_result.precision}</td>"
            detailed_results += f"<td>{throughput_mape if throughput_mape == 'N/A' else f'{throughput_mape:.2f}%'}</td>"
            detailed_results += f"<td>{latency_mape if latency_mape == 'N/A' else f'{latency_mape:.2f}%'}</td>"
            detailed_results += f"<td>{memory_mape if memory_mape == 'N/A' else f'{memory_mape:.2f}%'}</td>"
            detailed_results += f"<td>{power_mape if power_mape == 'N/A' else f'{power_mape:.2f}%'}</td>"
            detailed_results += "</tr>"
        
        detailed_results += "</table>"
        
        # Format overall MAPE
        overall_mape_str = f"{report_data['overall_mape']:.2f}%" if report_data['overall_mape'] is not None else "N/A"
        
        # Render HTML template
        html_report = html_template.format(
            title=report_data["title"],
            timestamp=report_data["timestamp"],
            total_results=report_data["total_results"],
            overall_mape=report_data["overall_mape"] if report_data["overall_mape"] is not None else 0,
            overall_status=report_data["overall_status"],
            overall_status_class=self._status_class(report_data["overall_status"]),
            hardware_summary=hardware_summary,
            model_summary=model_summary,
            hardware_model_summary=hardware_model_summary,
            visualizations=visualizations,
            detailed_results=detailed_results
        )
        
        return html_report
    
    def _generate_markdown_report(self, report_data: Dict[str, Any], include_visualizations: bool) -> str:
        """
        Generate a Markdown report.
        
        Args:
            report_data: Report data
            include_visualizations: Whether to include visualizations
            
        Returns:
            Markdown report as a string
        """
        # Markdown template
        markdown = f"# {report_data['title']}\n\n"
        markdown += f"Generated on: {report_data['timestamp']}\n\n"
        
        # Summary
        markdown += "## Summary\n\n"
        markdown += f"Total validation results: **{report_data['total_results']}**\n\n"
        
        if report_data["overall_mape"] is not None:
            markdown += f"Overall MAPE: **{report_data['overall_mape']:.2f}%**\n\n"
            markdown += f"Overall status: **{report_data['overall_status']}**\n\n"
        else:
            markdown += "Overall MAPE: **N/A**\n\n"
            markdown += "Overall status: **unknown**\n\n"
        
        # Hardware summary
        markdown += "## Results by Hardware\n\n"
        markdown += "| Hardware | Count | MAPE | Status |\n"
        markdown += "| --- | --- | --- | --- |\n"
        
        for hw_id, stats in report_data["hardware_stats"].items():
            if "mean_mape" in stats:
                markdown += f"| {hw_id} | {stats['count']} | {stats['mean_mape']:.2f}% | {stats['status']} |\n"
        
        markdown += "\n"
        
        # Model summary
        markdown += "## Results by Model\n\n"
        markdown += "| Model | Count | MAPE | Status |\n"
        markdown += "| --- | --- | --- | --- |\n"
        
        for model_id, stats in report_data["model_stats"].items():
            if "mean_mape" in stats:
                markdown += f"| {model_id} | {stats['count']} | {stats['mean_mape']:.2f}% | {stats['status']} |\n"
        
        markdown += "\n"
        
        # Hardware-model summary
        markdown += "## Results by Hardware and Model\n\n"
        markdown += "| Hardware | Model | Count | MAPE | Status |\n"
        markdown += "| --- | --- | --- | --- | --- |\n"
        
        for (hw_id, model_id), stats in report_data["hardware_model_stats"].items():
            if "mean_mape" in stats:
                markdown += f"| {hw_id} | {model_id} | {stats['count']} | {stats['mean_mape']:.2f}% | {stats['status']} |\n"
        
        markdown += "\n"
        
        # Visualizations section
        if include_visualizations:
            markdown += "## Visualizations\n\n"
            markdown += "_Note: Visualizations are not available in Markdown format. Please use HTML format to view visualizations._\n\n"
        
        # Detailed results section
        markdown += "## Detailed Results\n\n"
        markdown += f"Showing up to {min(self.config['max_results_per_page'], report_data['total_results'])} of {report_data['total_results']} results\n\n"
        
        markdown += "| Hardware | Model | Batch Size | Precision | Throughput MAPE | Latency MAPE | Memory MAPE | Power MAPE |\n"
        markdown += "| --- | --- | --- | --- | --- | --- | --- | --- |\n"
        
        for i, val_result in enumerate(report_data["validation_results"]):
            if i >= self.config["max_results_per_page"]:
                break
                
            hw_result = val_result.hardware_result
            sim_result = val_result.simulation_result
            
            # Extract MAPE values for each metric
            throughput_mape = val_result.metrics_comparison.get("throughput_items_per_second", {}).get("mape", "N/A")
            latency_mape = val_result.metrics_comparison.get("average_latency_ms", {}).get("mape", "N/A")
            memory_mape = val_result.metrics_comparison.get("memory_peak_mb", {}).get("mape", "N/A")
            power_mape = val_result.metrics_comparison.get("power_consumption_w", {}).get("mape", "N/A")
            
            markdown += f"| {hw_result.hardware_id} | {hw_result.model_id} | {hw_result.batch_size} | {hw_result.precision} "
            markdown += f"| {throughput_mape if throughput_mape == 'N/A' else f'{throughput_mape:.2f}%'} "
            markdown += f"| {latency_mape if latency_mape == 'N/A' else f'{latency_mape:.2f}%'} "
            markdown += f"| {memory_mape if memory_mape == 'N/A' else f'{memory_mape:.2f}%'} "
            markdown += f"| {power_mape if power_mape == 'N/A' else f'{power_mape:.2f}%'} |\n"
        
        markdown += "\n"
        markdown += "_Report generated by the Simulation Accuracy and Validation Framework_"
        
        return markdown
    
    def _generate_json_report(self, report_data: Dict[str, Any]) -> str:
        """
        Generate a JSON report.
        
        Args:
            report_data: Report data
            
        Returns:
            JSON report as a string
        """
        # Filter out data that can't be serialized to JSON
        json_data = {
            "title": report_data["title"],
            "timestamp": report_data["timestamp"],
            "total_results": report_data["total_results"],
            "overall_mape": report_data["overall_mape"],
            "overall_status": report_data["overall_status"],
            "hardware_stats": report_data["hardware_stats"],
            "model_stats": report_data["model_stats"]
        }
        
        # Convert hardware_model_stats keys to strings for JSON
        hw_model_stats = {}
        for (hw_id, model_id), stats in report_data["hardware_model_stats"].items():
            key = f"{hw_id}__{model_id}"
            hw_model_stats[key] = stats
        
        json_data["hardware_model_stats"] = hw_model_stats
        
        # Add limited detailed results
        detailed_results = []
        for i, val_result in enumerate(report_data["validation_results"]):
            if i >= self.config["max_results_per_page"]:
                break
                
            hw_result = val_result.hardware_result
            sim_result = val_result.simulation_result
            
            # Extract MAPE values for each metric
            throughput_mape = val_result.metrics_comparison.get("throughput_items_per_second", {}).get("mape", None)
            latency_mape = val_result.metrics_comparison.get("average_latency_ms", {}).get("mape", None)
            memory_mape = val_result.metrics_comparison.get("memory_peak_mb", {}).get("mape", None)
            power_mape = val_result.metrics_comparison.get("power_consumption_w", {}).get("mape", None)
            
            detailed_results.append({
                "hardware_id": hw_result.hardware_id,
                "model_id": hw_result.model_id,
                "batch_size": hw_result.batch_size,
                "precision": hw_result.precision,
                "throughput_mape": throughput_mape,
                "latency_mape": latency_mape,
                "memory_mape": memory_mape,
                "power_mape": power_mape
            })
        
        json_data["detailed_results"] = detailed_results
        
        # Convert to JSON string
        try:
            return json.dumps(json_data, indent=2)
        except Exception as e:
            logger.error(f"Error generating JSON report: {e}")
            return json.dumps({"error": f"Error generating JSON report: {e}"})
    
    def _generate_text_report(self, report_data: Dict[str, Any]) -> str:
        """
        Generate a plain text report.
        
        Args:
            report_data: Report data
            
        Returns:
            Text report as a string
        """
        # Text template
        text = f"{report_data['title']}\n"
        text += f"Generated on: {report_data['timestamp']}\n\n"
        
        # Summary
        text += "SUMMARY\n"
        text += "=======\n"
        text += f"Total validation results: {report_data['total_results']}\n"
        
        if report_data["overall_mape"] is not None:
            text += f"Overall MAPE: {report_data['overall_mape']:.2f}%\n"
            text += f"Overall status: {report_data['overall_status']}\n\n"
        else:
            text += "Overall MAPE: N/A\n"
            text += "Overall status: unknown\n\n"
        
        # Hardware summary
        text += "RESULTS BY HARDWARE\n"
        text += "===================\n"
        text += f"{'Hardware':<20} {'Count':<8} {'MAPE':<10} {'Status':<15}\n"
        text += "-" * 60 + "\n"
        
        for hw_id, stats in report_data["hardware_stats"].items():
            if "mean_mape" in stats:
                text += f"{hw_id:<20} {stats['count']:<8} {stats['mean_mape']:.2f}%{'':5} {stats['status']:<15}\n"
        
        text += "\n"
        
        # Model summary
        text += "RESULTS BY MODEL\n"
        text += "================\n"
        text += f"{'Model':<25} {'Count':<8} {'MAPE':<10} {'Status':<15}\n"
        text += "-" * 60 + "\n"
        
        for model_id, stats in report_data["model_stats"].items():
            if "mean_mape" in stats:
                text += f"{model_id:<25} {stats['count']:<8} {stats['mean_mape']:.2f}%{'':5} {stats['status']:<15}\n"
        
        text += "\n"
        
        # Hardware-model summary
        text += "RESULTS BY HARDWARE AND MODEL\n"
        text += "=============================\n"
        text += f"{'Hardware':<20} {'Model':<25} {'Count':<8} {'MAPE':<10} {'Status':<15}\n"
        text += "-" * 80 + "\n"
        
        for (hw_id, model_id), stats in report_data["hardware_model_stats"].items():
            if "mean_mape" in stats:
                text += f"{hw_id:<20} {model_id:<25} {stats['count']:<8} {stats['mean_mape']:.2f}%{'':5} {stats['status']:<15}\n"
        
        text += "\n"
        
        # Detailed results section
        text += "DETAILED RESULTS\n"
        text += "===============\n"
        text += f"Showing up to {min(self.config['max_results_per_page'], report_data['total_results'])} of {report_data['total_results']} results\n\n"
        
        col_widths = {
            "hardware": 20,
            "model": 25,
            "batch": 8,
            "precision": 8,
            "throughput": 12,
            "latency": 12,
            "memory": 12,
            "power": 12
        }
        
        # Header
        text += f"{'Hardware':<{col_widths['hardware']}} {'Model':<{col_widths['model']}} {'Batch':<{col_widths['batch']}} {'Prec':<{col_widths['precision']}} "
        text += f"{'Throughput':<{col_widths['throughput']}} {'Latency':<{col_widths['latency']}} {'Memory':<{col_widths['memory']}} {'Power':<{col_widths['power']}}\n"
        text += "-" * (sum(col_widths.values()) + len(col_widths)) + "\n"
        
        for i, val_result in enumerate(report_data["validation_results"]):
            if i >= self.config["max_results_per_page"]:
                break
                
            hw_result = val_result.hardware_result
            sim_result = val_result.simulation_result
            
            # Extract MAPE values for each metric
            throughput_mape = val_result.metrics_comparison.get("throughput_items_per_second", {}).get("mape", "N/A")
            latency_mape = val_result.metrics_comparison.get("average_latency_ms", {}).get("mape", "N/A")
            memory_mape = val_result.metrics_comparison.get("memory_peak_mb", {}).get("mape", "N/A")
            power_mape = val_result.metrics_comparison.get("power_consumption_w", {}).get("mape", "N/A")
            
            # Format MAPE values
            throughput_str = "N/A" if throughput_mape == "N/A" else f"{throughput_mape:.2f}%"
            latency_str = "N/A" if latency_mape == "N/A" else f"{latency_mape:.2f}%"
            memory_str = "N/A" if memory_mape == "N/A" else f"{memory_mape:.2f}%"
            power_str = "N/A" if power_mape == "N/A" else f"{power_mape:.2f}%"
            
            text += f"{hw_result.hardware_id:<{col_widths['hardware']}} {hw_result.model_id:<{col_widths['model']}} {hw_result.batch_size:<{col_widths['batch']}} {hw_result.precision:<{col_widths['precision']}} "
            text += f"{throughput_str:<{col_widths['throughput']}} {latency_str:<{col_widths['latency']}} {memory_str:<{col_widths['memory']}} {power_str:<{col_widths['power']}}\n"
        
        text += "\n"
        text += "Report generated by the Simulation Accuracy and Validation Framework"
        
        return text
    
    def _interpret_mape(self, mape: Optional[float]) -> str:
        """
        Interpret MAPE value as a status.
        
        Args:
            mape: Mean Absolute Percentage Error value
            
        Returns:
            Status string based on MAPE
        """
        if mape is None:
            return "unknown"
        
        if mape < 5:
            return "excellent"
        elif mape < 10:
            return "good"
        elif mape < 15:
            return "acceptable"
        elif mape < 25:
            return "problematic"
        else:
            return "poor"
    
    def _status_class(self, status: str) -> str:
        """
        Convert status to CSS class name.
        
        Args:
            status: Status string
            
        Returns:
            CSS class name
        """
        if status == "excellent":
            return "excellent"
        elif status == "good":
            return "good"
        elif status == "acceptable":
            return "acceptable"
        elif status == "problematic":
            return "problematic"
        elif status == "poor":
            return "poor"
        else:
            return "unknown"