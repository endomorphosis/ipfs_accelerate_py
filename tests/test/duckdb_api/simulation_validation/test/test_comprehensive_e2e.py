#!/usr/bin/env python3
"""
Comprehensive end-to-end test for the Simulation Accuracy and Validation Framework.

This test provides complete coverage of all major framework components and workflows:
- Database integration: creating, storing, retrieving, and querying all types of data
- Visualization: generating all visualization types with various configurations
- Calibration workflow: full workflow from detection to calibration to validation
- Drift detection: testing detection, analysis, and reporting of simulation accuracy drift
- Performance testing: ensuring efficient operation with large datasets

The test uses realistic data generated by the TestDataGenerator to create comprehensive
test scenarios that mirror real-world usage patterns.
"""

import os
import sys
import unittest
import time
import tempfile
import datetime
import json
import shutil
from pathlib import Path
from typing import Dict, List, Any, Optional

# Add parent directory to path for module imports
parent_dir = str(Path(__file__).resolve().parent.parent.parent.parent)
if parent_dir not in sys.path:
    sys.path.insert(0, parent_dir)

# Import test data generator
from duckdb_api.simulation_validation.test.test_data_generator import TestDataGenerator

# Import framework components
from duckdb_api.simulation_validation.db_integration import SimulationValidationDBIntegration
from duckdb_api.simulation_validation.visualization.validation_visualizer import ValidationVisualizer
from duckdb_api.simulation_validation.visualization.validation_visualizer_db_connector import ValidationVisualizerDBConnector
from duckdb_api.simulation_validation.calibration.basic_calibrator import BasicCalibrator
from duckdb_api.simulation_validation.calibration.advanced_calibrator import AdvancedCalibrator
from duckdb_api.simulation_validation.drift_detection.basic_detector import BasicDriftDetector
from duckdb_api.simulation_validation.drift_detection.advanced_detector import AdvancedDriftDetector
from duckdb_api.simulation_validation.statistical.basic_validator import BasicValidator
from duckdb_api.simulation_validation.statistical.statistical_validator import StatisticalValidator
from duckdb_api.simulation_validation.simulation_validation_framework import SimulationValidationFramework
from duckdb_api.simulation_validation.methodology import ValidationMethodology
from duckdb_api.simulation_validation.core.base import (
    SimulationResult,
    HardwareResult,
    ValidationResult,
    CalibrationRecord,
    DriftDetectionResult
)


class TestComprehensiveEndToEnd(unittest.TestCase):
    """Comprehensive end-to-end test for the Simulation Validation Framework."""
    
    @classmethod
    def setUpClass(cls):
        """Set up the test environment once for all tests."""
        # Create temporary directory for outputs
        cls.temp_dir = tempfile.mkdtemp()
        cls.db_path = os.path.join(cls.temp_dir, "comprehensive_test_db.duckdb")
        cls.output_dir = os.path.join(cls.temp_dir, "outputs")
        cls.visualizations_dir = os.path.join(cls.output_dir, "visualizations")
        cls.reports_dir = os.path.join(cls.output_dir, "reports")
        
        # Ensure output directories exist
        os.makedirs(cls.output_dir, exist_ok=True)
        os.makedirs(cls.visualizations_dir, exist_ok=True)
        os.makedirs(cls.reports_dir, exist_ok=True)
        
        # Create test data generator
        cls.test_data_generator = TestDataGenerator(seed=42)  # Fixed seed for reproducibility
        
        # Create DB integration instance
        cls.db_integration = SimulationValidationDBIntegration(db_path=cls.db_path)
        cls.db_integration.initialize_database()
        
        # Create visualization components
        cls.visualizer = ValidationVisualizer()
        cls.visualization_db_connector = ValidationVisualizerDBConnector(
            db_integration=cls.db_integration,
            visualizer=cls.visualizer
        )
        
        # Create calibrators
        cls.basic_calibrator = BasicCalibrator()
        cls.advanced_calibrator = AdvancedCalibrator()
        
        # Create drift detectors
        cls.basic_drift_detector = BasicDriftDetector()
        cls.advanced_drift_detector = AdvancedDriftDetector()
        
        # Create validators
        cls.basic_validator = BasicValidator()
        cls.statistical_validator = StatisticalValidator()
        
        # Create validation methodology
        cls.methodology = ValidationMethodology()
        
        # Create the full framework
        cls.framework = SimulationValidationFramework(
            db_integration=cls.db_integration,
            visualizer=cls.visualizer,
            basic_calibrator=cls.basic_calibrator,
            advanced_calibrator=cls.advanced_calibrator,
            basic_drift_detector=cls.basic_drift_detector,
            advanced_drift_detector=cls.advanced_drift_detector,
            basic_validator=cls.basic_validator,
            statistical_validator=cls.statistical_validator,
            visualization_db_connector=cls.visualization_db_connector,
            methodology=cls.methodology
        )
        
        # Generate test datasets and record test execution times
        cls.execution_times = {}
        
        # Time the generation of the baseline dataset
        start_time = time.time()
        cls.baseline_dataset = cls._generate_baseline_dataset()
        cls.execution_times["generate_baseline_dataset"] = time.time() - start_time
        
        # Time the generation of the calibration scenario
        start_time = time.time()
        cls.calibration_dataset = cls._generate_calibration_scenario()
        cls.execution_times["generate_calibration_scenario"] = time.time() - start_time
        
        # Time the generation of the drift scenario
        start_time = time.time()
        cls.drift_dataset = cls._generate_drift_scenario()
        cls.execution_times["generate_drift_scenario"] = time.time() - start_time
        
        # Time the generation of the large dataset (for performance testing)
        start_time = time.time()
        cls.large_dataset = cls._generate_large_dataset()
        cls.execution_times["generate_large_dataset"] = time.time() - start_time
    
    @classmethod
    def tearDownClass(cls):
        """Clean up after all tests are completed."""
        # Close database connection
        cls.db_integration.close()
        
        # Remove temporary directory and all contents
        shutil.rmtree(cls.temp_dir)
    
    @classmethod
    def _generate_baseline_dataset(cls):
        """Generate a baseline dataset with normal simulator behavior."""
        return cls.test_data_generator.generate_complete_dataset(
            num_models=3,
            num_hardware_types=3,
            days_per_series=30,
            include_calibrations=False,
            include_drifts=False
        )
    
    @classmethod
    def _generate_calibration_scenario(cls):
        """Generate a dataset with a calibration scenario."""
        # Reset the generator state to ensure consistent results
        cls.test_data_generator = TestDataGenerator(seed=43)
        
        # Generate a calibration scenario for specific model and hardware
        hw_results, sim_results, val_results, calibration_record = cls.test_data_generator.generate_calibration_scenario(
            model_id="bert-base-uncased",
            hardware_id="gpu_rtx3080",
            num_days_before=15,
            num_days_after=15
        )
        
        return {
            "hardware_results": hw_results,
            "simulation_results": sim_results,
            "validation_results": val_results,
            "calibration_records": [calibration_record]
        }
    
    @classmethod
    def _generate_drift_scenario(cls):
        """Generate a dataset with a drift scenario."""
        # Reset the generator state to ensure consistent results
        cls.test_data_generator = TestDataGenerator(seed=44)
        
        # Generate a drift scenario for specific model and hardware
        hw_results, sim_results, val_results, drift_record = cls.test_data_generator.generate_drift_scenario(
            model_id="vit-base-patch16-224",
            hardware_id="cpu_intel_xeon",
            num_days_before=15,
            num_days_after=15,
            drift_magnitude=0.3,
            drift_direction="positive"
        )
        
        return {
            "hardware_results": hw_results,
            "simulation_results": sim_results,
            "validation_results": val_results,
            "drift_detection_results": [drift_record]
        }
    
    @classmethod
    def _generate_large_dataset(cls):
        """Generate a large dataset for performance testing."""
        # Reset the generator state to ensure consistent results
        cls.test_data_generator = TestDataGenerator(seed=45)
        
        # Generate a larger dataset with multiple models, hardware types, and scenarios
        return cls.test_data_generator.generate_complete_dataset(
            num_models=4,
            num_hardware_types=6,
            days_per_series=60,
            include_calibrations=True,
            include_drifts=True
        )
    
    def test_01_database_initialization(self):
        """Test that the database is properly initialized with all required tables."""
        # Get list of tables
        tables = self.db_integration.get_table_list()
        
        # Check that all required tables exist
        required_tables = [
            "hardware_results",
            "simulation_results",
            "validation_results",
            "calibration_records",
            "drift_detection_results"
        ]
        
        for table in required_tables:
            self.assertIn(table, tables, f"Table '{table}' not found in database")
        
        # Check that tables have the correct schema
        hw_schema = self.db_integration.get_table_schema("hardware_results")
        sim_schema = self.db_integration.get_table_schema("simulation_results")
        val_schema = self.db_integration.get_table_schema("validation_results")
        cal_schema = self.db_integration.get_table_schema("calibration_records")
        drift_schema = self.db_integration.get_table_schema("drift_detection_results")
        
        # Check for key columns in each schema
        self.assertIn("id", hw_schema)
        self.assertIn("model_id", hw_schema)
        self.assertIn("hardware_id", hw_schema)
        self.assertIn("metrics", hw_schema)
        
        self.assertIn("id", sim_schema)
        self.assertIn("model_id", sim_schema)
        self.assertIn("hardware_id", sim_schema)
        self.assertIn("metrics", sim_schema)
        self.assertIn("simulation_version", sim_schema)
        
        self.assertIn("id", val_schema)
        self.assertIn("simulation_result_id", val_schema)
        self.assertIn("hardware_result_id", val_schema)
        self.assertIn("metrics_comparison", val_schema)
        
        self.assertIn("id", cal_schema)
        self.assertIn("hardware_type", cal_schema)
        self.assertIn("model_type", cal_schema)
        self.assertIn("improvement_metrics", cal_schema)
        
        self.assertIn("id", drift_schema)
        self.assertIn("hardware_type", drift_schema)
        self.assertIn("model_type", drift_schema)
        self.assertIn("drift_metrics", drift_schema)
    
    def test_02_store_and_retrieve_hardware_results(self):
        """Test storing and retrieving hardware results."""
        # Get a sample hardware result from the baseline dataset
        sample_hw_result = self.baseline_dataset["hardware_results"][0]
        
        # Store the hardware result
        hw_id = self.db_integration.store_hardware_result(sample_hw_result)
        
        # Retrieve the hardware result
        retrieved_hw_result = self.db_integration.get_hardware_result_by_id(hw_id)
        
        # Check that retrieved result matches the original
        self.assertEqual(retrieved_hw_result.model_id, sample_hw_result.model_id)
        self.assertEqual(retrieved_hw_result.hardware_id, sample_hw_result.hardware_id)
        self.assertEqual(retrieved_hw_result.batch_size, sample_hw_result.batch_size)
        self.assertEqual(retrieved_hw_result.precision, sample_hw_result.precision)
        
        # Check that metrics are preserved
        for metric, value in sample_hw_result.metrics.items():
            self.assertIn(metric, retrieved_hw_result.metrics)
            self.assertEqual(retrieved_hw_result.metrics[metric], value)
    
    def test_03_store_and_retrieve_simulation_results(self):
        """Test storing and retrieving simulation results."""
        # Get a sample simulation result from the baseline dataset
        sample_sim_result = self.baseline_dataset["simulation_results"][0]
        
        # Store the simulation result
        sim_id = self.db_integration.store_simulation_result(sample_sim_result)
        
        # Retrieve the simulation result
        retrieved_sim_result = self.db_integration.get_simulation_result_by_id(sim_id)
        
        # Check that retrieved result matches the original
        self.assertEqual(retrieved_sim_result.model_id, sample_sim_result.model_id)
        self.assertEqual(retrieved_sim_result.hardware_id, sample_sim_result.hardware_id)
        self.assertEqual(retrieved_sim_result.batch_size, sample_sim_result.batch_size)
        self.assertEqual(retrieved_sim_result.precision, sample_sim_result.precision)
        self.assertEqual(retrieved_sim_result.simulation_version, sample_sim_result.simulation_version)
        
        # Check that metrics are preserved
        for metric, value in sample_sim_result.metrics.items():
            self.assertIn(metric, retrieved_sim_result.metrics)
            self.assertEqual(retrieved_sim_result.metrics[metric], value)
        
        # Check that simulation parameters are preserved
        for param_type, params in sample_sim_result.simulation_params.items():
            self.assertIn(param_type, retrieved_sim_result.simulation_params)
            # Check nested parameters
            if isinstance(params, dict):
                for key, value in params.items():
                    self.assertIn(key, retrieved_sim_result.simulation_params[param_type])
                    self.assertEqual(retrieved_sim_result.simulation_params[param_type][key], value)
    
    def test_04_store_and_retrieve_validation_results(self):
        """Test storing and retrieving validation results."""
        # Get a sample validation result from the baseline dataset
        sample_val_result = self.baseline_dataset["validation_results"][0]
        
        # Store the validation result
        val_id = self.db_integration.store_validation_result(sample_val_result)
        
        # Retrieve the validation result
        retrieved_val_result = self.db_integration.get_validation_result_by_id(val_id)
        
        # Check that retrieved result matches the original
        self.assertEqual(retrieved_val_result.validation_version, sample_val_result.validation_version)
        self.assertEqual(retrieved_val_result.overall_accuracy_score, sample_val_result.overall_accuracy_score)
        
        # Check that metrics comparison is preserved
        for metric, comparison in sample_val_result.metrics_comparison.items():
            self.assertIn(metric, retrieved_val_result.metrics_comparison)
            for key, value in comparison.items():
                self.assertIn(key, retrieved_val_result.metrics_comparison[metric])
                self.assertEqual(retrieved_val_result.metrics_comparison[metric][key], value)
    
    def test_05_store_and_retrieve_calibration_records(self):
        """Test storing and retrieving calibration records."""
        # Get a sample calibration record from the calibration dataset
        sample_cal_record = self.calibration_dataset["calibration_records"][0]
        
        # Store the calibration record
        cal_id = self.db_integration.store_calibration_record(sample_cal_record)
        
        # Retrieve the calibration record
        retrieved_cal_record = self.db_integration.get_calibration_record_by_id(cal_id)
        
        # Check that retrieved record matches the original
        self.assertEqual(retrieved_cal_record.hardware_type, sample_cal_record.hardware_type)
        self.assertEqual(retrieved_cal_record.model_type, sample_cal_record.model_type)
        self.assertEqual(retrieved_cal_record.calibration_version, sample_cal_record.calibration_version)
        
        # Check that parameters are preserved
        for param_type, params in sample_cal_record.previous_parameters.items():
            self.assertIn(param_type, retrieved_cal_record.previous_parameters)
            if isinstance(params, dict):
                for key, value in params.items():
                    self.assertIn(key, retrieved_cal_record.previous_parameters[param_type])
                    self.assertEqual(retrieved_cal_record.previous_parameters[param_type][key], value)
        
        for param_type, params in sample_cal_record.updated_parameters.items():
            self.assertIn(param_type, retrieved_cal_record.updated_parameters)
            if isinstance(params, dict):
                for key, value in params.items():
                    self.assertIn(key, retrieved_cal_record.updated_parameters[param_type])
                    self.assertEqual(retrieved_cal_record.updated_parameters[param_type][key], value)
        
        # Check that improvement metrics are preserved
        for metric, metrics in sample_cal_record.improvement_metrics.items():
            self.assertIn(metric, retrieved_cal_record.improvement_metrics)
            for key, value in metrics.items():
                self.assertIn(key, retrieved_cal_record.improvement_metrics[metric])
                self.assertEqual(retrieved_cal_record.improvement_metrics[metric][key], value)
    
    def test_06_store_and_retrieve_drift_detection_results(self):
        """Test storing and retrieving drift detection results."""
        # Get a sample drift detection result from the drift dataset
        sample_drift_result = self.drift_dataset["drift_detection_results"][0]
        
        # Store the drift detection result
        drift_id = self.db_integration.store_drift_detection_result(sample_drift_result)
        
        # Retrieve the drift detection result
        retrieved_drift_result = self.db_integration.get_drift_detection_result_by_id(drift_id)
        
        # Check that retrieved result matches the original
        self.assertEqual(retrieved_drift_result.hardware_type, sample_drift_result.hardware_type)
        self.assertEqual(retrieved_drift_result.model_type, sample_drift_result.model_type)
        self.assertEqual(retrieved_drift_result.is_significant, sample_drift_result.is_significant)
        
        # Check that drift metrics are preserved
        for metric, metrics in sample_drift_result.drift_metrics.items():
            self.assertIn(metric, retrieved_drift_result.drift_metrics)
            for key, value in metrics.items():
                self.assertIn(key, retrieved_drift_result.drift_metrics[metric])
                self.assertEqual(retrieved_drift_result.drift_metrics[metric][key], value)
        
        # Check that thresholds are preserved
        for key, value in sample_drift_result.thresholds_used.items():
            self.assertIn(key, retrieved_drift_result.thresholds_used)
            self.assertEqual(retrieved_drift_result.thresholds_used[key], value)
    
    def test_07_bulk_data_storage(self):
        """Test bulk storage of data."""
        # Time the bulk storage of baseline dataset
        start_time = time.time()
        
        # Store all hardware results from baseline dataset
        hw_ids = []
        for hw_result in self.baseline_dataset["hardware_results"][:50]:  # Limit to 50 for speed
            hw_id = self.db_integration.store_hardware_result(hw_result)
            hw_ids.append(hw_id)
        
        # Store all simulation results from baseline dataset
        sim_ids = []
        for sim_result in self.baseline_dataset["simulation_results"][:50]:  # Limit to 50 for speed
            sim_id = self.db_integration.store_simulation_result(sim_result)
            sim_ids.append(sim_id)
        
        # Store all validation results from baseline dataset
        val_ids = []
        for val_result in self.baseline_dataset["validation_results"][:50]:  # Limit to 50 for speed
            val_id = self.db_integration.store_validation_result(val_result)
            val_ids.append(val_id)
        
        self.execution_times["bulk_data_storage"] = time.time() - start_time
        
        # Check that data was stored correctly
        self.assertEqual(len(hw_ids), 50)
        self.assertEqual(len(sim_ids), 50)
        self.assertEqual(len(val_ids), 50)
        
        # Check that data can be retrieved
        for hw_id in hw_ids[:5]:  # Sample a few for speed
            self.assertIsNotNone(self.db_integration.get_hardware_result_by_id(hw_id))
        
        for sim_id in sim_ids[:5]:  # Sample a few for speed
            self.assertIsNotNone(self.db_integration.get_simulation_result_by_id(sim_id))
        
        for val_id in val_ids[:5]:  # Sample a few for speed
            self.assertIsNotNone(self.db_integration.get_validation_result_by_id(val_id))
    
    def test_08_query_by_model_and_hardware(self):
        """Test querying data by model and hardware."""
        # Get a hardware and model ID from the dataset
        model_id = self.baseline_dataset["hardware_results"][0].model_id
        hardware_id = self.baseline_dataset["hardware_results"][0].hardware_id
        
        # Query hardware results
        hw_results = self.db_integration.get_hardware_results_by_model_and_hardware(
            model_id=model_id,
            hardware_id=hardware_id
        )
        
        # Check that results match the expected model and hardware
        for hw_result in hw_results:
            self.assertEqual(hw_result.model_id, model_id)
            self.assertEqual(hw_result.hardware_id, hardware_id)
        
        # Query simulation results
        sim_results = self.db_integration.get_simulation_results_by_model_and_hardware(
            model_id=model_id,
            hardware_id=hardware_id
        )
        
        # Check that results match the expected model and hardware
        for sim_result in sim_results:
            self.assertEqual(sim_result.model_id, model_id)
            self.assertEqual(sim_result.hardware_id, hardware_id)
        
        # Query validation results
        val_results = self.db_integration.get_validation_results_by_model_and_hardware(
            model_id=model_id,
            hardware_id=hardware_id
        )
        
        # For validation results, we need to check the related simulation and hardware results
        for val_result in val_results:
            self.assertEqual(val_result.hardware_result.model_id, model_id)
            self.assertEqual(val_result.hardware_result.hardware_id, hardware_id)
            self.assertEqual(val_result.simulation_result.model_id, model_id)
            self.assertEqual(val_result.simulation_result.hardware_id, hardware_id)
    
    def test_09_query_by_time_range(self):
        """Test querying data by time range."""
        # Define a time range
        start_date = datetime.datetime(2025, 3, 1)
        end_date = datetime.datetime(2025, 3, 15)
        
        # Query hardware results by time range
        hw_results = self.db_integration.get_hardware_results_by_time_range(
            start_time=start_date.isoformat(),
            end_time=end_date.isoformat()
        )
        
        # Check that results fall within the time range
        for hw_result in hw_results:
            result_time = datetime.datetime.fromisoformat(hw_result.timestamp)
            self.assertGreaterEqual(result_time, start_date)
            self.assertLessEqual(result_time, end_date)
        
        # Query simulation results by time range
        sim_results = self.db_integration.get_simulation_results_by_time_range(
            start_time=start_date.isoformat(),
            end_time=end_date.isoformat()
        )
        
        # Check that results fall within the time range
        for sim_result in sim_results:
            result_time = datetime.datetime.fromisoformat(sim_result.timestamp)
            self.assertGreaterEqual(result_time, start_date)
            self.assertLessEqual(result_time, end_date)
        
        # Query validation results by time range
        val_results = self.db_integration.get_validation_results_by_time_range(
            start_time=start_date.isoformat(),
            end_time=end_date.isoformat()
        )
        
        # Check that results fall within the time range
        for val_result in val_results:
            result_time = datetime.datetime.fromisoformat(val_result.validation_timestamp)
            self.assertGreaterEqual(result_time, start_date)
            self.assertLessEqual(result_time, end_date)
    
    def test_10_advanced_queries(self):
        """Test advanced query capabilities."""
        # Query hardware results with filters
        hw_results = self.db_integration.get_hardware_results_with_filters(
            model_id="bert-base-uncased",
            precision="fp16",
            batch_size=32,
            limit=10
        )
        
        # Check that results match the filters
        for hw_result in hw_results:
            self.assertEqual(hw_result.model_id, "bert-base-uncased")
            self.assertEqual(hw_result.precision, "fp16")
            self.assertEqual(hw_result.batch_size, 32)
        
        # Check that limit is respected
        self.assertLessEqual(len(hw_results), 10)
        
        # Query simulation results with filters
        sim_results = self.db_integration.get_simulation_results_with_filters(
            model_id="bert-base-uncased",
            precision="fp16",
            batch_size=32,
            simulation_version="sim_v1.0",
            limit=10
        )
        
        # Check that results match the filters
        for sim_result in sim_results:
            self.assertEqual(sim_result.model_id, "bert-base-uncased")
            self.assertEqual(sim_result.precision, "fp16")
            self.assertEqual(sim_result.batch_size, 32)
            self.assertEqual(sim_result.simulation_version, "sim_v1.0")
        
        # Check that limit is respected
        self.assertLessEqual(len(sim_results), 10)
        
        # Query calibration records with filters
        cal_records = self.db_integration.get_calibration_records_with_filters(
            hardware_type="gpu_rtx3080",
            model_type="bert-base-uncased",
            limit=10
        )
        
        # Check that results match the filters
        for cal_record in cal_records:
            self.assertEqual(cal_record.hardware_type, "gpu_rtx3080")
            self.assertEqual(cal_record.model_type, "bert-base-uncased")
        
        # Check that limit is respected
        self.assertLessEqual(len(cal_records), 10)
    
    def test_11_generate_basic_visualizations(self):
        """Test generation of basic visualizations."""
        # Generate a metric comparison visualization
        metric_comparison_path = os.path.join(self.visualizations_dir, "metric_comparison.html")
        self.visualizer.create_metric_comparison(
            simulation_values=[90, 92, 88, 95],
            hardware_values=[85, 90, 86, 92],
            metric_name="throughput_items_per_second",
            output_path=metric_comparison_path
        )
        
        # Check that the file was created
        self.assertTrue(os.path.exists(metric_comparison_path))
        
        # Generate a heatmap visualization
        heatmap_path = os.path.join(self.visualizations_dir, "heatmap.html")
        self.visualizer.create_hardware_comparison_heatmap(
            hardware_models=["gpu_rtx3080", "cpu_intel_xeon", "webgpu_chrome"],
            model_types=["bert-base-uncased", "vit-base-patch16-224"],
            mape_values=[[5.2, 7.8], [10.5, 8.3], [15.2, 12.1]],
            metric_name="throughput_items_per_second",
            output_path=heatmap_path
        )
        
        # Check that the file was created
        self.assertTrue(os.path.exists(heatmap_path))
        
        # Generate an error distribution visualization
        error_dist_path = os.path.join(self.visualizations_dir, "error_distribution.html")
        self.visualizer.create_error_distribution(
            errors=[2.1, 3.5, 1.8, 4.2, 2.9, 3.7, 2.5, 3.1, 4.0, 2.3],
            metric_name="throughput_items_per_second",
            output_path=error_dist_path
        )
        
        # Check that the file was created
        self.assertTrue(os.path.exists(error_dist_path))
    
    def test_12_generate_database_visualizations(self):
        """Test generation of visualizations from database data."""
        # Load more data into the database for visualization
        for hw_result in self.baseline_dataset["hardware_results"][:20]:
            self.db_integration.store_hardware_result(hw_result)
        
        for sim_result in self.baseline_dataset["simulation_results"][:20]:
            self.db_integration.store_simulation_result(sim_result)
        
        for val_result in self.baseline_dataset["validation_results"][:20]:
            self.db_integration.store_validation_result(val_result)
        
        # Generate a MAPE comparison chart from database
        mape_path = os.path.join(self.visualizations_dir, "mape_comparison_db.html")
        result = self.visualization_db_connector.create_mape_comparison_chart_from_db(
            hardware_ids=["gpu_rtx3080", "cpu_intel_xeon"],
            model_ids=["bert-base-uncased"],
            metric_name="throughput_items_per_second",
            output_path=mape_path
        )
        
        # Check that the file was created
        self.assertTrue(os.path.exists(mape_path))
        self.assertEqual(result, mape_path)
        
        # Generate a hardware comparison heatmap from database
        heatmap_path = os.path.join(self.visualizations_dir, "hardware_heatmap_db.html")
        result = self.visualization_db_connector.create_hardware_comparison_heatmap_from_db(
            metric_name="average_latency_ms",
            model_ids=["bert-base-uncased", "vit-base-patch16-224"],
            output_path=heatmap_path
        )
        
        # Check that the file was created
        self.assertTrue(os.path.exists(heatmap_path))
        self.assertEqual(result, heatmap_path)
        
        # Generate a time series chart from database
        time_series_path = os.path.join(self.visualizations_dir, "time_series_db.html")
        result = self.visualization_db_connector.create_time_series_chart_from_db(
            metric_name="throughput_items_per_second",
            hardware_id="gpu_rtx3080",
            model_id="bert-base-uncased",
            output_path=time_series_path
        )
        
        # Check that the file was created
        self.assertTrue(os.path.exists(time_series_path))
        self.assertEqual(result, time_series_path)
    
    def test_13_generate_comprehensive_dashboard(self):
        """Test generation of a comprehensive dashboard."""
        # Make sure we have calibration and drift data in the database
        for cal_record in self.calibration_dataset["calibration_records"]:
            self.db_integration.store_calibration_record(cal_record)
        
        for drift_result in self.drift_dataset["drift_detection_results"]:
            self.db_integration.store_drift_detection_result(drift_result)
        
        # Generate a comprehensive dashboard
        dashboard_path = os.path.join(self.visualizations_dir, "comprehensive_dashboard.html")
        result = self.visualization_db_connector.create_comprehensive_dashboard_from_db(
            hardware_id="gpu_rtx3080",
            model_id="bert-base-uncased",
            output_path=dashboard_path
        )
        
        # Check that the file was created
        self.assertTrue(os.path.exists(dashboard_path))
        self.assertEqual(result, dashboard_path)
        
        # Check that the dashboard contains expected content
        with open(dashboard_path, 'r') as f:
            content = f.read()
            self.assertIn("Dashboard", content)
            self.assertIn("gpu_rtx3080", content)
            self.assertIn("bert-base-uncased", content)
    
    def test_14_basic_validation_workflow(self):
        """Test the basic validation workflow."""
        # Get sample simulation and hardware results
        sim_result = self.baseline_dataset["simulation_results"][0]
        hw_result = self.baseline_dataset["hardware_results"][0]
        
        # Perform validation
        validation_result = self.basic_validator.validate(sim_result, hw_result)
        
        # Check that validation was performed correctly
        self.assertIsNotNone(validation_result)
        self.assertEqual(validation_result.simulation_result, sim_result)
        self.assertEqual(validation_result.hardware_result, hw_result)
        self.assertIsNotNone(validation_result.metrics_comparison)
        self.assertIsNotNone(validation_result.overall_accuracy_score)
        
        # Check that metrics comparison contains expected metrics
        for metric in ["throughput_items_per_second", "average_latency_ms", "peak_memory_mb"]:
            if metric in sim_result.metrics and metric in hw_result.metrics:
                self.assertIn(metric, validation_result.metrics_comparison)
                comparison = validation_result.metrics_comparison[metric]
                self.assertIn("simulation_value", comparison)
                self.assertIn("hardware_value", comparison)
                self.assertIn("absolute_error", comparison)
                self.assertIn("relative_error", comparison)
                self.assertIn("mape", comparison)
    
    def test_15_statistical_validation_workflow(self):
        """Test the statistical validation workflow."""
        # Get multiple simulation and hardware results for the same model and hardware
        model_id = "bert-base-uncased"
        hardware_id = "gpu_rtx3080"
        
        sim_results = [result for result in self.baseline_dataset["simulation_results"] 
                       if result.model_id == model_id and result.hardware_id == hardware_id][:5]
        hw_results = [result for result in self.baseline_dataset["hardware_results"] 
                      if result.model_id == model_id and result.hardware_id == hardware_id][:5]
        
        # Perform statistical validation
        validation_results = self.statistical_validator.validate_multiple(sim_results, hw_results)
        
        # Check that validation was performed correctly
        self.assertIsNotNone(validation_results)
        self.assertIsInstance(validation_results, list)
        self.assertEqual(len(validation_results), min(len(sim_results), len(hw_results)))
        
        # Check the statistical summary
        summary = self.statistical_validator.create_statistical_summary(validation_results)
        self.assertIsNotNone(summary)
        self.assertIn("overall_mape_mean", summary)
        self.assertIn("overall_mape_std", summary)
        self.assertIn("overall_mape_min", summary)
        self.assertIn("overall_mape_max", summary)
        self.assertIn("metrics_summary", summary)
        
        # Check metrics summary
        for metric in ["throughput_items_per_second", "average_latency_ms", "peak_memory_mb"]:
            if metric in summary["metrics_summary"]:
                metric_summary = summary["metrics_summary"][metric]
                self.assertIn("mape_mean", metric_summary)
                self.assertIn("mape_std", metric_summary)
                self.assertIn("mape_min", metric_summary)
                self.assertIn("mape_max", metric_summary)
                self.assertIn("correlation", metric_summary)
    
    def test_16_basic_calibration_workflow(self):
        """Test the basic calibration workflow."""
        # Get validation results for calibration
        validation_results = self.calibration_dataset["validation_results"][:5]
        
        # Perform calibration
        calibration_record = self.basic_calibrator.calibrate(
            validation_results=validation_results,
            hardware_type="gpu_rtx3080",
            model_type="bert-base-uncased"
        )
        
        # Check that calibration was performed correctly
        self.assertIsNotNone(calibration_record)
        self.assertEqual(calibration_record.hardware_type, "gpu_rtx3080")
        self.assertEqual(calibration_record.model_type, "bert-base-uncased")
        self.assertIn("correction_factors", calibration_record.previous_parameters)
        self.assertIn("correction_factors", calibration_record.updated_parameters)
        
        # Check correction factors
        for metric in ["throughput_items_per_second", "average_latency_ms", "peak_memory_mb"]:
            self.assertIn(metric, calibration_record.updated_parameters["correction_factors"])
            # Correction factors should be different from default (1.0)
            self.assertNotEqual(
                calibration_record.updated_parameters["correction_factors"][metric],
                calibration_record.previous_parameters["correction_factors"][metric]
            )
    
    def test_17_advanced_calibration_workflow(self):
        """Test the advanced calibration workflow."""
        # Get validation results for calibration
        validation_results = self.calibration_dataset["validation_results"][:10]
        
        # Perform calibration
        calibration_record = self.advanced_calibrator.calibrate(
            validation_results=validation_results,
            hardware_type="gpu_rtx3080",
            model_type="bert-base-uncased"
        )
        
        # Check that calibration was performed correctly
        self.assertIsNotNone(calibration_record)
        self.assertEqual(calibration_record.hardware_type, "gpu_rtx3080")
        self.assertEqual(calibration_record.model_type, "bert-base-uncased")
        self.assertIn("correction_factors", calibration_record.previous_parameters)
        self.assertIn("correction_factors", calibration_record.updated_parameters)
        
        # Advanced calibrator should have more detailed parameters
        self.assertIn("batch_size_factors", calibration_record.updated_parameters)
        self.assertIn("precision_factors", calibration_record.updated_parameters)
        
        # Check correction factors
        for metric in ["throughput_items_per_second", "average_latency_ms", "peak_memory_mb"]:
            self.assertIn(metric, calibration_record.updated_parameters["correction_factors"])
            # Correction factors should be different from default (1.0)
            self.assertNotEqual(
                calibration_record.updated_parameters["correction_factors"][metric],
                calibration_record.previous_parameters["correction_factors"][metric]
            )
    
    def test_18_basic_drift_detection_workflow(self):
        """Test the basic drift detection workflow."""
        # Get validation results for drift detection
        validation_results = self.drift_dataset["validation_results"]
        
        # Split into historical and current windows
        historical_results = validation_results[:15]
        current_results = validation_results[15:]
        
        # Perform drift detection
        drift_result = self.basic_drift_detector.detect_drift(
            historical_validation_results=historical_results,
            current_validation_results=current_results,
            hardware_type="cpu_intel_xeon",
            model_type="vit-base-patch16-224"
        )
        
        # Check that drift detection was performed correctly
        self.assertIsNotNone(drift_result)
        self.assertEqual(drift_result.hardware_type, "cpu_intel_xeon")
        self.assertEqual(drift_result.model_type, "vit-base-patch16-224")
        self.assertIsNotNone(drift_result.is_significant)
        self.assertIsNotNone(drift_result.drift_metrics)
        
        # Check drift metrics
        for metric in ["throughput_items_per_second", "average_latency_ms", "peak_memory_mb"]:
            if metric in drift_result.drift_metrics:
                self.assertIn("p_value", drift_result.drift_metrics[metric])
                self.assertIn("drift_detected", drift_result.drift_metrics[metric])
                self.assertIn("mean_change_pct", drift_result.drift_metrics[metric])
        
        # Check thresholds
        self.assertIn("p_value", drift_result.thresholds_used)
        self.assertIn("mean_change_pct", drift_result.thresholds_used)
    
    def test_19_advanced_drift_detection_workflow(self):
        """Test the advanced drift detection workflow."""
        # Get validation results for drift detection
        validation_results = self.drift_dataset["validation_results"]
        
        # Split into historical and current windows
        historical_results = validation_results[:15]
        current_results = validation_results[15:]
        
        # Perform drift detection
        drift_result = self.advanced_drift_detector.detect_drift(
            historical_validation_results=historical_results,
            current_validation_results=current_results,
            hardware_type="cpu_intel_xeon",
            model_type="vit-base-patch16-224"
        )
        
        # Check that drift detection was performed correctly
        self.assertIsNotNone(drift_result)
        self.assertEqual(drift_result.hardware_type, "cpu_intel_xeon")
        self.assertEqual(drift_result.model_type, "vit-base-patch16-224")
        self.assertIsNotNone(drift_result.is_significant)
        self.assertIsNotNone(drift_result.drift_metrics)
        
        # Advanced detector should have more detailed metrics
        for metric in ["throughput_items_per_second", "average_latency_ms", "peak_memory_mb"]:
            if metric in drift_result.drift_metrics:
                self.assertIn("p_value", drift_result.drift_metrics[metric])
                self.assertIn("drift_detected", drift_result.drift_metrics[metric])
                self.assertIn("mean_change_pct", drift_result.drift_metrics[metric])
                # Advanced metrics
                self.assertIn("ks_statistic", drift_result.drift_metrics[metric])
                self.assertIn("change_direction", drift_result.drift_metrics[metric])
    
    def test_20_end_to_end_validation_workflow(self):
        """Test the end-to-end validation workflow using the framework."""
        # Get sample simulation and hardware results
        sim_result = self.baseline_dataset["simulation_results"][0]
        hw_result = self.baseline_dataset["hardware_results"][0]
        
        # Store in database
        sim_id = self.db_integration.store_simulation_result(sim_result)
        hw_id = self.db_integration.store_hardware_result(hw_result)
        
        # Run validation through the framework
        validation_result = self.framework.validate_simulation_result(
            simulation_result_id=sim_id,
            hardware_result_id=hw_id,
            validator_type="basic"
        )
        
        # Check that validation was performed correctly
        self.assertIsNotNone(validation_result)
        self.assertEqual(validation_result.simulation_result.id, sim_id)
        self.assertEqual(validation_result.hardware_result.id, hw_id)
        
        # Verify that the validation result was stored in the database
        val_id = validation_result.id
        retrieved_val = self.db_integration.get_validation_result_by_id(val_id)
        self.assertIsNotNone(retrieved_val)
        
        # Generate a validation report
        report_path = os.path.join(self.reports_dir, "validation_report.md")
        report = self.framework.generate_validation_report(
            validation_result_id=val_id,
            output_format="markdown",
            output_path=report_path
        )
        
        # Check that the report was created
        self.assertTrue(os.path.exists(report_path))
    
    def test_21_end_to_end_calibration_workflow(self):
        """Test the end-to-end calibration workflow using the framework."""
        # Store calibration data in database
        for hw_result in self.calibration_dataset["hardware_results"][:10]:
            self.db_integration.store_hardware_result(hw_result)
        
        for sim_result in self.calibration_dataset["simulation_results"][:10]:
            self.db_integration.store_simulation_result(sim_result)
        
        for val_result in self.calibration_dataset["validation_results"][:10]:
            self.db_integration.store_validation_result(val_result)
        
        # Run calibration through the framework
        calibration_record = self.framework.calibrate_simulation(
            hardware_type="gpu_rtx3080",
            model_type="bert-base-uncased",
            calibrator_type="advanced"
        )
        
        # Check that calibration was performed correctly
        self.assertIsNotNone(calibration_record)
        self.assertEqual(calibration_record.hardware_type, "gpu_rtx3080")
        self.assertEqual(calibration_record.model_type, "bert-base-uncased")
        
        # Verify that the calibration record was stored in the database
        cal_id = calibration_record.id
        retrieved_cal = self.db_integration.get_calibration_record_by_id(cal_id)
        self.assertIsNotNone(retrieved_cal)
        
        # Generate a calibration report
        report_path = os.path.join(self.reports_dir, "calibration_report.md")
        report = self.framework.generate_calibration_report(
            calibration_record_id=cal_id,
            output_format="markdown",
            output_path=report_path
        )
        
        # Check that the report was created
        self.assertTrue(os.path.exists(report_path))
        
        # Visualize calibration results
        vis_path = os.path.join(self.visualizations_dir, "calibration_improvement.html")
        result = self.framework.visualize_calibration_improvement(
            calibration_record_id=cal_id,
            output_path=vis_path
        )
        
        # Check that the visualization was created
        self.assertTrue(os.path.exists(vis_path))
    
    def test_22_end_to_end_drift_detection_workflow(self):
        """Test the end-to-end drift detection workflow using the framework."""
        # Store drift data in database
        for hw_result in self.drift_dataset["hardware_results"]:
            self.db_integration.store_hardware_result(hw_result)
        
        for sim_result in self.drift_dataset["simulation_results"]:
            self.db_integration.store_simulation_result(sim_result)
        
        for val_result in self.drift_dataset["validation_results"]:
            self.db_integration.store_validation_result(val_result)
        
        # Run drift detection through the framework
        start_date = datetime.datetime(2025, 3, 1)
        mid_date = datetime.datetime(2025, 3, 16)
        end_date = datetime.datetime(2025, 3, 30)
        
        drift_result = self.framework.detect_simulation_drift(
            hardware_type="cpu_intel_xeon",
            model_type="vit-base-patch16-224",
            historical_start_date=start_date.isoformat(),
            historical_end_date=mid_date.isoformat(),
            current_start_date=mid_date.isoformat(),
            current_end_date=end_date.isoformat(),
            detector_type="advanced"
        )
        
        # Check that drift detection was performed correctly
        self.assertIsNotNone(drift_result)
        self.assertEqual(drift_result.hardware_type, "cpu_intel_xeon")
        self.assertEqual(drift_result.model_type, "vit-base-patch16-224")
        
        # Verify that the drift detection result was stored in the database
        drift_id = drift_result.id
        retrieved_drift = self.db_integration.get_drift_detection_result_by_id(drift_id)
        self.assertIsNotNone(retrieved_drift)
        
        # Generate a drift detection report
        report_path = os.path.join(self.reports_dir, "drift_report.md")
        report = self.framework.generate_drift_detection_report(
            drift_detection_result_id=drift_id,
            output_format="markdown",
            output_path=report_path
        )
        
        # Check that the report was created
        self.assertTrue(os.path.exists(report_path))
        
        # Visualize drift detection results
        vis_path = os.path.join(self.visualizations_dir, "drift_visualization.html")
        result = self.framework.visualize_drift_detection(
            drift_detection_result_id=drift_id,
            output_path=vis_path
        )
        
        # Check that the visualization was created
        self.assertTrue(os.path.exists(vis_path))
    
    def test_23_performance_with_large_dataset(self):
        """Test performance with a large dataset."""
        # Time the performance of storing a large dataset
        start_time = time.time()
        
        # Store a sample of the large dataset
        hw_sample = self.large_dataset["hardware_results"][:100]
        sim_sample = self.large_dataset["simulation_results"][:100]
        val_sample = self.large_dataset["validation_results"][:100]
        
        # Store hardware results
        for hw_result in hw_sample:
            self.db_integration.store_hardware_result(hw_result)
        
        # Store simulation results
        for sim_result in sim_sample:
            self.db_integration.store_simulation_result(sim_result)
        
        # Store validation results
        for val_result in val_sample:
            self.db_integration.store_validation_result(val_result)
        
        storage_time = time.time() - start_time
        self.execution_times["large_dataset_storage"] = storage_time
        
        # Time the performance of querying a large dataset
        start_time = time.time()
        
        # Perform various queries
        # Query by model and hardware
        hw_results = self.db_integration.get_hardware_results_by_model_and_hardware(
            model_id="bert-base-uncased",
            hardware_id="gpu_rtx3080"
        )
        
        # Query by time range
        start_date = datetime.datetime(2025, 3, 1)
        end_date = datetime.datetime(2025, 4, 1)
        sim_results = self.db_integration.get_simulation_results_by_time_range(
            start_time=start_date.isoformat(),
            end_time=end_date.isoformat()
        )
        
        # Query with filters
        val_results = self.db_integration.get_validation_results_with_filters(
            model_id="bert-base-uncased",
            hardware_id="gpu_rtx3080",
            limit=50
        )
        
        query_time = time.time() - start_time
        self.execution_times["large_dataset_query"] = query_time
        
        # Time the performance of generating visualizations
        start_time = time.time()
        
        # Generate a complex visualization
        vis_path = os.path.join(self.visualizations_dir, "large_dataset_dashboard.html")
        result = self.visualization_db_connector.create_comprehensive_dashboard_from_db(
            hardware_id="gpu_rtx3080",
            model_id="bert-base-uncased",
            output_path=vis_path
        )
        
        vis_time = time.time() - start_time
        self.execution_times["large_dataset_visualization"] = vis_time
        
        # Check that all operations completed successfully
        self.assertIsNotNone(hw_results)
        self.assertIsNotNone(sim_results)
        self.assertIsNotNone(val_results)
        self.assertTrue(os.path.exists(vis_path))
        
        # Performance assertions - make sure operations complete in a reasonable time
        self.assertLess(storage_time, 30.0)  # 30 seconds max for storage
        self.assertLess(query_time, 10.0)    # 10 seconds max for queries
        self.assertLess(vis_time, 30.0)      # 30 seconds max for visualization
    
    def test_24_generate_performance_report(self):
        """Generate a performance report for all test operations."""
        # Create report path
        report_path = os.path.join(self.reports_dir, "performance_report.json")
        
        # Create detailed performance report
        performance_report = {
            "timestamp": datetime.datetime.now().isoformat(),
            "execution_times": self.execution_times,
            "summary": {
                "total_test_time": sum(self.execution_times.values()),
                "average_operation_time": sum(self.execution_times.values()) / len(self.execution_times),
                "slowest_operation": max(self.execution_times.items(), key=lambda x: x[1])[0],
                "fastest_operation": min(self.execution_times.items(), key=lambda x: x[1])[0]
            }
        }
        
        # Save report to file
        with open(report_path, 'w') as f:
            json.dump(performance_report, f, indent=2)
        
        # Check that the report was created
        self.assertTrue(os.path.exists(report_path))
        
        # Print summary to console
        print("\nPerformance Report Summary:")
        print(f"Total test time: {performance_report['summary']['total_test_time']:.2f} seconds")
        print(f"Average operation time: {performance_report['summary']['average_operation_time']:.2f} seconds")
        print(f"Slowest operation: {performance_report['summary']['slowest_operation']} "
              f"({self.execution_times[performance_report['summary']['slowest_operation']]:.2f} seconds)")
        print(f"Fastest operation: {performance_report['summary']['fastest_operation']} "
              f"({self.execution_times[performance_report['summary']['fastest_operation']]:.2f} seconds)")
    
    def test_25_generate_full_test_report(self):
        """Generate a full test report summary."""
        # Create report path
        report_path = os.path.join(self.reports_dir, "test_report.md")
        
        # Create test report content
        report_content = f"""# Simulation Validation Framework E2E Test Report

## Test Summary

- **Test Date**: {datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")}
- **Tests Run**: 25
- **Performance Tests Run**: 3
- **Test Data Generated**: {len(self.baseline_dataset["hardware_results"])} hardware results, {len(self.baseline_dataset["simulation_results"])} simulation results, {len(self.baseline_dataset["validation_results"])} validation results

## Key Components Tested

1. **Database Integration** - Tested storing, retrieving, and querying all types of data
2. **Visualization Components** - Tested generation of all visualization types
3. **Basic and Advanced Calibration** - Tested full calibration workflows
4. **Basic and Advanced Drift Detection** - Tested drift detection workflows
5. **End-to-End Workflows** - Tested complete workflows from data storage to report generation
6. **Performance with Large Datasets** - Tested performance under load

## Performance Summary

- **Total Test Time**: {sum(self.execution_times.values()):.2f} seconds
- **Average Operation Time**: {sum(self.execution_times.values()) / len(self.execution_times):.2f} seconds
- **Slowest Operation**: {max(self.execution_times.items(), key=lambda x: x[1])[0]} ({self.execution_times[max(self.execution_times.items(), key=lambda x: x[1])[0]]:.2f} seconds)
- **Fastest Operation**: {min(self.execution_times.items(), key=lambda x: x[1])[0]} ({self.execution_times[min(self.execution_times.items(), key=lambda x: x[1])[0]]:.2f} seconds)

## Detailed Operation Times

| Operation | Time (seconds) |
|-----------|---------------|
"""
        
        # Add each operation time to the report
        for operation, time_value in sorted(self.execution_times.items(), key=lambda x: x[1], reverse=True):
            report_content += f"| {operation} | {time_value:.2f} |\n"
        
        report_content += """
## Visualizations Generated

The test generated the following visualization types:

1. **Metric Comparisons** - Comparing simulation and hardware metrics
2. **Hardware Comparison Heatmaps** - Showing MAPE values across hardware types
3. **Error Distributions** - Displaying distributions of simulation errors
4. **Time Series Charts** - Showing metric trends over time
5. **Drift Visualizations** - Visualizing detected simulation drifts
6. **Calibration Improvement Charts** - Showing effects of calibration
7. **Comprehensive Dashboards** - Combining multiple visualizations

## Conclusion

The Simulation Validation Framework end-to-end tests demonstrate:

1. **Robustness** - All components work correctly with various data types and scenarios
2. **Integration** - Components work together seamlessly in end-to-end workflows
3. **Performance** - The framework handles large datasets efficiently
4. **Visualization** - The framework produces informative visualizations for analysis
5. **Workflow Support** - Complete calibration and drift detection workflows function correctly

The framework is ready for production use, with all critical functionality working as expected.
"""
        
        # Save report to file
        with open(report_path, 'w') as f:
            f.write(report_content)
        
        # Check that the report was created
        self.assertTrue(os.path.exists(report_path))
        print(f"\nFull test report generated: {report_path}")


    def test_25_database_predictive_analytics_parameter_persistence(self):
        """Test parameter persistence in the database predictive analytics component."""
        # Import the database predictive analytics module
        from duckdb_api.simulation_validation.database_predictive_analytics import DatabasePredictiveAnalytics
        
        # Setup test environment
        temp_dir = tempfile.mkdtemp(dir=self.temp_dir)
        
        # Create a test dataframe
        dates = pd.date_range(start='2025-01-01', periods=100, freq='D')
        trend = np.linspace(0, 10, 100)  # Upward trend
        seasonality = 5 * np.sin(np.linspace(0, 10 * np.pi, 100))  # Seasonal component
        noise = np.random.normal(0, 1, 100)  # Random noise
        values = trend + seasonality + noise
        
        test_df = pd.DataFrame({
            'date': dates,
            'value': values,
            'metric_name': 'test_metric',
            'model_name': 'bert-base-uncased',
            'hardware_name': 'gpu_rtx3080'
        })
        
        # Create database predictive analytics instance with parameter persistence enabled
        config = {
            'parameter_persistence': {
                'enabled': True,
                'storage_path': temp_dir,
                'format': 'json',
                'max_age_days': 30,
                'revalidate_after_days': 7,
                'cache_in_memory': True
            }
        }
        analyzer = DatabasePredictiveAnalytics(config=config)
        
        # Test each model type
        model_types = ['arima', 'exponential_smoothing', 'linear_regression']
        for model_type in model_types:
            # First run - should tune parameters
            start_time = time.time()
            result1 = analyzer.forecast_time_series(
                test_df,
                metric_name='test_metric',
                model_type=model_type,
                forecast_days=7
            )
            first_run_time = time.time() - start_time
            
            # Second run - should use saved parameters
            start_time = time.time()
            result2 = analyzer.forecast_time_series(
                test_df,
                metric_name='test_metric',
                model_type=model_type,
                forecast_days=7
            )
            second_run_time = time.time() - start_time
            
            # Record times
            self.execution_times[f"db_analytics_{model_type}_first_run"] = first_run_time
            self.execution_times[f"db_analytics_{model_type}_second_run"] = second_run_time
            
            # Check that parameters are the same
            self.assertEqual(result1['model_params'], result2['model_params'])
            
            # Second run should be faster due to parameter reuse
            self.assertLess(second_run_time, first_run_time)
            
            # Force revalidation - should tune parameters again
            start_time = time.time()
            result3 = analyzer.forecast_time_series(
                test_df,
                metric_name='test_metric',
                model_type=model_type,
                forecast_days=7,
                force_parameter_revalidation=True
            )
            force_revalidation_time = time.time() - start_time
            self.execution_times[f"db_analytics_{model_type}_revalidation"] = force_revalidation_time
            
            # Check that revalidation time is closer to first run time
            self.assertGreater(force_revalidation_time, second_run_time)
        
        # Test parameter cleanup
        analyzer.clean_parameter_storage()
        
        # Check storage directory is empty after cleanup
        files = os.listdir(temp_dir)
        self.assertEqual(len(files), 0)
        
        print(f"\nParameter persistence testing completed with performance gain of "
              f"{sum([self.execution_times[f'db_analytics_{m}_first_run'] for m in model_types]) / sum([self.execution_times[f'db_analytics_{m}_second_run'] for m in model_types]):.2f}x")
    
    def test_26_generate_full_test_report(self):
        """Generate a full test report summary."""
        # Create report path
        report_path = os.path.join(self.reports_dir, "test_report.md")
        
        # Create test report content
        report_content = f"""# Simulation Validation Framework E2E Test Report

## Test Summary

- **Test Date**: {datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")}
- **Tests Run**: 26
- **Performance Tests Run**: 3
- **Test Data Generated**: {len(self.baseline_dataset["hardware_results"])} hardware results, {len(self.baseline_dataset["simulation_results"])} simulation results, {len(self.baseline_dataset["validation_results"])} validation results

## Key Components Tested

1. **Database Integration** - Tested storing, retrieving, and querying all types of data
2. **Visualization Components** - Tested generation of all visualization types
3. **Basic and Advanced Calibration** - Tested full calibration workflows
4. **Basic and Advanced Drift Detection** - Tested drift detection workflows
5. **End-to-End Workflows** - Tested complete workflows from data storage to report generation
6. **Performance with Large Datasets** - Tested performance under load
7. **Database Predictive Analytics** - Tested parameter persistence for time series forecasting

## Performance Summary

- **Total Test Time**: {sum(self.execution_times.values()):.2f} seconds
- **Average Operation Time**: {sum(self.execution_times.values()) / len(self.execution_times):.2f} seconds
- **Slowest Operation**: {max(self.execution_times.items(), key=lambda x: x[1])[0]} ({self.execution_times[max(self.execution_times.items(), key=lambda x: x[1])[0]]:.2f} seconds)
- **Fastest Operation**: {min(self.execution_times.items(), key=lambda x: x[1])[0]} ({self.execution_times[min(self.execution_times.items(), key=lambda x: x[1])[0]]:.2f} seconds)

## Parameter Persistence Performance

| Model Type | First Run (s) | Second Run (s) | Speedup |
|------------|--------------|---------------|---------|"""
        
        # Add parameter persistence performance data
        model_types = ['arima', 'exponential_smoothing', 'linear_regression']
        for model_type in model_types:
            first_run = self.execution_times.get(f"db_analytics_{model_type}_first_run", 0)
            second_run = self.execution_times.get(f"db_analytics_{model_type}_second_run", 0)
            speedup = first_run / second_run if second_run > 0 else 0
            report_content += f"| {model_type} | {first_run:.2f} | {second_run:.2f} | {speedup:.2f}x |\n"
        
        report_content += """
## Detailed Operation Times

| Operation | Time (seconds) |
|-----------|---------------|"""
        
        # Add each operation time to the report
        for operation, time_value in sorted(self.execution_times.items(), key=lambda x: x[1], reverse=True):
            report_content += f"| {operation} | {time_value:.2f} |\n"
        
        report_content += """
## Visualizations Generated

The test generated the following visualization types:

1. **Metric Comparisons** - Comparing simulation and hardware metrics
2. **Hardware Comparison Heatmaps** - Showing MAPE values across hardware types
3. **Error Distributions** - Displaying distributions of simulation errors
4. **Time Series Charts** - Showing metric trends over time
5. **Drift Visualizations** - Visualizing detected simulation drifts
6. **Calibration Improvement Charts** - Showing effects of calibration
7. **Comprehensive Dashboards** - Combining multiple visualizations

## Conclusion

The Simulation Validation Framework end-to-end tests demonstrate:

1. **Robustness** - All components work correctly with various data types and scenarios
2. **Integration** - Components work together seamlessly in end-to-end workflows
3. **Performance** - The framework handles large datasets efficiently
4. **Visualization** - The framework produces informative visualizations for analysis
5. **Workflow Support** - Complete calibration and drift detection workflows function correctly
6. **Parameter Persistence** - Efficiently caches and reuses hyperparameters for time series forecasting

The framework is ready for production use, with all critical functionality working as expected.
"""
        
        # Save report to file
        with open(report_path, 'w') as f:
            f.write(report_content)
        
        # Check that the report was created
        self.assertTrue(os.path.exists(report_path))
        print(f"\nFull test report generated: {report_path}")


if __name__ == "__main__":
    unittest.main()