#!/usr/bin/env python3
"""
Distributed Testing Framework - Result Aggregator Tests

This module contains tests for the ResultAggregatorService.
"""

import os
import sys
import unittest
import json
import io
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Tuple
from pathlib import Path
import pandas as pd
from unittest.mock import MagicMock, patch

# Add parent directory to path to import modules from parent
parent_dir = str(Path(__file__).parent.parent.parent)
if parent_dir not in sys.path:
    sys.path.insert(0, parent_dir)

from duckdb_api.distributed_testing.result_aggregator import (
    ResultAggregatorService,
    RESULT_TYPE_PERFORMANCE,
    RESULT_TYPE_COMPATIBILITY,
    RESULT_TYPE_INTEGRATION,
    RESULT_TYPE_WEB_PLATFORM,
    AGGREGATION_LEVEL_TEST_RUN,
    AGGREGATION_LEVEL_MODEL,
    AGGREGATION_LEVEL_HARDWARE,
    AGGREGATION_LEVEL_MODEL_HARDWARE,
    AGGREGATION_LEVEL_TASK_TYPE,
    AGGREGATION_LEVEL_WORKER,
)


class TestResultAggregator(unittest.TestCase):
    """Test cases for the ResultAggregatorService."""

    def setUp(self):
        """Set up the test environment."""
        # Create a mock database manager
        self.mock_db_manager = MagicMock()
        
        # Set up mock hardware and model info responses
        self.mock_db_manager.get_hardware_info.return_value = {
            "device_name": "Test Device",
            "hardware_type": "GPU",
            "platform": "Test Platform",
            "memory_gb": 16
        }
        
        self.mock_db_manager.get_model_info.return_value = {
            "model_name": "Test Model",
            "model_family": "transformer",
            "modality": "text",
            "parameters_million": 100
        }
        
        # Create a mock trend analyzer
        self.mock_trend_analyzer = MagicMock()
        
        # Create sample performance results
        self.performance_results = self._generate_sample_performance_results()
        
        # Create sample compatibility results
        self.compatibility_results = self._generate_sample_compatibility_results()
        
        # Create sample integration test results
        self.integration_results = self._generate_sample_integration_results()
        
        # Create sample web platform results
        self.web_platform_results = self._generate_sample_web_platform_results()
        
        # Configure mock database manager to return sample results - using side_effect for more control
        def mock_get_performance_results(**kwargs):
            return self.performance_results
        
        self.mock_db_manager.get_performance_results.side_effect = mock_get_performance_results
        self.mock_db_manager.get_compatibility_results.return_value = self.compatibility_results
        self.mock_db_manager.get_integration_test_results.return_value = self.integration_results
        self.mock_db_manager.get_web_platform_results.return_value = self.web_platform_results
        
        # Create the result aggregator service
        self.aggregator = ResultAggregatorService(
            db_manager=self.mock_db_manager,
            trend_analyzer=self.mock_trend_analyzer
        )
        
        # Configure with model_family_grouping disabled for testing
        self.aggregator.configure({
            "model_family_grouping": False
        })
        
        # Reset mocks to clear any setup calls
        self.mock_db_manager.reset_mock()
        
    def _generate_sample_performance_results(self) -> List[Dict[str, Any]]:
        """Generate sample performance test results."""
        results = []
        
        # Create results for 2 models on 2 hardware platforms
        models = ["model1", "model2"]
        hardware = ["hw1", "hw2"]
        
        for model_id in models:
            for hardware_id in hardware:
                # Multiple runs with different performance metrics
                for i in range(10):
                    # Base performance values
                    base_latency = 100 if model_id == "model1" else 200
                    base_throughput = 50 if model_id == "model1" else 25
                    
                    # Hardware efficiency factor
                    hw_factor = 1.0 if hardware_id == "hw1" else 0.8
                    
                    # Random variation (Â±10%)
                    variation = 0.9 + (i / 45)  # From 0.9 to 1.1
                    
                    # Calculate metrics
                    latency = base_latency * hw_factor * variation
                    throughput = base_throughput * hw_factor * (2 - variation)  # Inverse relationship
                    memory = (base_latency * hw_factor * 0.5) + (i * 2)
                    
                    # Create result
                    result = {
                        "result_id": f"perf_{model_id}_{hardware_id}_{i}",
                        "run_id": f"run_{i % 3}",  # Group into 3 runs
                        "model_id": model_id,
                        "hardware_id": hardware_id,
                        "model_family": "transformer" if model_id == "model1" else "diffusion",
                        "test_case": "inference",
                        "batch_size": 1 if i % 2 == 0 else 4,
                        "precision": "fp16" if i % 3 == 0 else "fp32",
                        "total_time_seconds": latency * 0.01,  # Convert to seconds
                        "average_latency_ms": latency,
                        "throughput_items_per_second": throughput,
                        "memory_peak_mb": memory,
                        "iterations": 100,
                        "warmup_iterations": 10,
                        "is_simulated": i % 5 == 0,  # Some simulated results
                    }
                    
                    # Add a timestamp from the last 7 days
                    days_ago = i % 7
                    result["timestamp"] = (datetime.now() - timedelta(days=days_ago)).isoformat()
                    
                    # Add an anomaly for testing
                    if i == 9 and model_id == "model1" and hardware_id == "hw1":
                        result["average_latency_ms"] *= 3  # Major latency spike
                        
                    if i == 8 and model_id == "model2" and hardware_id == "hw2":
                        result["throughput_items_per_second"] *= 3  # Major throughput improvement
                        
                    results.append(result)
                    
        return results
        
    def _generate_sample_compatibility_results(self) -> List[Dict[str, Any]]:
        """Generate sample compatibility test results."""
        results = []
        
        # Create results for 2 models on 2 hardware platforms
        models = ["model1", "model2"]
        hardware = ["hw1", "hw2"]
        
        for model_id in models:
            for hardware_id in hardware:
                # Multiple runs with different compatibility results
                for i in range(5):
                    # Base compatibility - model1 works well on both, model2 only on hw1
                    is_compatible = True
                    if model_id == "model2" and hardware_id == "hw2":
                        is_compatible = False
                        
                    # Create result
                    result = {
                        "compatibility_id": f"compat_{model_id}_{hardware_id}_{i}",
                        "run_id": f"run_{i % 2}",  # Group into 2 runs
                        "model_id": model_id,
                        "hardware_id": hardware_id,
                        "model_family": "transformer" if model_id == "model1" else "diffusion",
                        "is_compatible": is_compatible,
                        "detection_success": True,
                        "initialization_success": is_compatible,
                        "error_message": "" if is_compatible else "Initialization failed",
                        "error_type": "" if is_compatible else "memory",
                        "compatibility_score": 1.0 if is_compatible else 0.2,
                        "is_simulated": i % 3 == 0,  # Some simulated results
                    }
                    
                    # Add a timestamp from the last 7 days
                    days_ago = i % 7
                    result["timestamp"] = (datetime.now() - timedelta(days=days_ago)).isoformat()
                    
                    results.append(result)
                    
        return results
        
    def _generate_sample_integration_results(self) -> List[Dict[str, Any]]:
        """Generate sample integration test results."""
        results = []
        
        # Create results for several test modules
        test_modules = ["core", "api", "utils"]
        test_classes = ["TestCore", "TestAPI", "TestUtils"]
        test_cases = ["test_init", "test_process", "test_cleanup"]
        
        for module in test_modules:
            for test_class in test_classes:
                for test_case in test_cases:
                    # Multiple runs with different results
                    for i in range(3):
                        # Determine test status - mostly passing
                        status = "pass"
                        if i == 2 and module == "api" and test_case == "test_process":
                            status = "fail"  # Specific test failing
                            
                        # Create result
                        result = {
                            "test_result_id": f"{module}_{test_class}_{test_case}_{i}",
                            "run_id": f"run_{i}",
                            "test_module": module,
                            "test_class": test_class,
                            "test_name": test_case,
                            "status": status,
                            "execution_time_seconds": 0.5 + (i * 0.1),  # Increasing execution time
                            "error_message": "" if status == "pass" else "Assertion failed",
                        }
                        
                        # Add hardware and model if relevant
                        if module == "api":
                            result["hardware_id"] = "hw1" if i % 2 == 0 else "hw2"
                            result["model_id"] = "model1" if i % 2 == 0 else "model2"
                            
                        # Add a timestamp from the last 7 days
                        days_ago = i % 7
                        result["timestamp"] = (datetime.now() - timedelta(days=days_ago)).isoformat()
                        
                        results.append(result)
                        
        return results
        
    def _generate_sample_web_platform_results(self) -> List[Dict[str, Any]]:
        """Generate sample web platform test results."""
        results = []
        
        # Create results for 2 models on 2 browsers
        models = ["model1", "model2"]
        browsers = ["chrome", "firefox"]
        platforms = ["webnn", "webgpu"]
        
        for model_id in models:
            for browser in browsers:
                for platform in platforms:
                    # Multiple runs with different results
                    for i in range(5):
                        # Base performance values - chrome slightly faster
                        base_load = 200 if browser == "chrome" else 250
                        base_inference = 400 if browser == "chrome" else 450
                        
                        # Platform factors - webgpu slightly faster for inference
                        platform_load_factor = 1.0
                        platform_inference_factor = 0.9 if platform == "webgpu" else 1.0
                        
                        # Model factors - model1 smaller and faster
                        model_factor = 1.0 if model_id == "model1" else 1.5
                        
                        # Random variation (Â±10%)
                        variation = 0.9 + (i / 20)  # From 0.9 to 1.1
                        
                        # Calculate metrics
                        load_time = base_load * platform_load_factor * model_factor * variation
                        inference_time = base_inference * platform_inference_factor * model_factor * variation
                        total_time = load_time + inference_time
                        memory = (base_inference * model_factor * 0.2) + (i * 5)
                        
                        # Determine success - most succeed, but some specific combinations fail
                        success = True
                        if model_id == "model2" and browser == "firefox" and platform == "webnn" and i % 3 == 0:
                            success = False
                            
                        # Create result
                        result = {
                            "result_id": f"web_{model_id}_{browser}_{platform}_{i}",
                            "run_id": f"run_{i % 2}",
                            "model_id": model_id,
                            "hardware_id": f"{browser}_{platform}",
                            "platform": platform,
                            "browser": browser,
                            "browser_version": "100.0.0",
                            "test_file": f"test_{model_id}_{platform}.js",
                            "success": success,
                            "load_time_ms": load_time,
                            "initialization_time_ms": load_time * 0.3,
                            "inference_time_ms": inference_time,
                            "total_time_ms": total_time,
                            "memory_usage_mb": memory,
                            "error_message": "" if success else "WebNN initialization failed",
                        }
                        
                        # Add a timestamp from the last 7 days
                        days_ago = i % 7
                        result["timestamp"] = (datetime.now() - timedelta(days=days_ago)).isoformat()
                        
                        results.append(result)
                        
        return results
        
    def test_initialization(self):
        """Test that the result aggregator initializes correctly."""
        self.assertIsNotNone(self.aggregator)
        self.assertEqual(self.aggregator.db_manager, self.mock_db_manager)
        self.assertEqual(self.aggregator.trend_analyzer, self.mock_trend_analyzer)
        
        # Check that default pipelines are registered
        self.assertTrue(len(self.aggregator.preprocessing_pipeline) > 0)
        self.assertTrue(len(self.aggregator.aggregation_pipeline) > 0)
        self.assertTrue(len(self.aggregator.postprocessing_pipeline) > 0)
        
    def test_configuration(self):
        """Test configuration updates."""
        # Update configuration
        new_config = {
            "cache_ttl_seconds": 600,
            "anomaly_threshold": 3.0,
            "min_data_points": 10
        }
        self.aggregator.configure(new_config)
        
        # Check that configuration was updated
        for key, value in new_config.items():
            self.assertEqual(self.aggregator.config[key], value)
            
    def test_aggregate_performance_results(self):
        """Test aggregation of performance results."""
        # Aggregate by model
        aggregated = self.aggregator.aggregate_results(
            result_type=RESULT_TYPE_PERFORMANCE,
            aggregation_level=AGGREGATION_LEVEL_MODEL
        )
        
        # Check basic structure
        self.assertEqual(aggregated["result_type"], RESULT_TYPE_PERFORMANCE)
        self.assertEqual(aggregated["aggregation_level"], AGGREGATION_LEVEL_MODEL)
        self.assertIn("results", aggregated)
        self.assertIn("basic_statistics", aggregated["results"])
        
        # Check that we have results for each model
        basic_stats = aggregated["results"]["basic_statistics"]
        self.assertIn("model1", basic_stats)
        self.assertIn("model2", basic_stats)
        
        # Check that metrics were aggregated
        model1_stats = basic_stats["model1"]
        self.assertIn("average_latency_ms", model1_stats)
        self.assertIn("throughput_items_per_second", model1_stats)
        
        # Check that basic statistics were calculated
        latency_stats = model1_stats["average_latency_ms"]
        self.assertIn("mean", latency_stats)
        self.assertIn("median", latency_stats)
        self.assertIn("min", latency_stats)
        self.assertIn("max", latency_stats)
        self.assertIn("std", latency_stats)
        
        # Check count of results - actual count may vary based on mocking implementation
        self.assertGreater(model1_stats["result_count"], 0)
        
        # Test that the DB was called (we don't care about exact parameters)
        self.assertTrue(self.mock_db_manager.get_performance_results.called)
        
    def test_aggregate_with_filtering(self):
        """Test aggregation with filtering parameters."""
        # Filter by specific model and hardware
        filter_params = {
            "model_id": "model1",
            "hardware_id": "hw1"
        }
        
        # Reset mock to track this specific call
        self.mock_db_manager.reset_mock()
        
        aggregated = self.aggregator.aggregate_results(
            result_type=RESULT_TYPE_PERFORMANCE,
            aggregation_level=AGGREGATION_LEVEL_MODEL_HARDWARE,
            filter_params=filter_params
        )
        
        # Check that the DB was called
        self.assertTrue(self.mock_db_manager.get_performance_results.called)
        
        # Check that the called arguments contain our filter params
        called_args, called_kwargs = self.mock_db_manager.get_performance_results.call_args
        self.assertEqual(called_kwargs.get('aggregation_level'), AGGREGATION_LEVEL_MODEL_HARDWARE)
        # The filter_params might be modified internally, so we just check it's called
        
    def test_aggregate_with_time_range(self):
        """Test aggregation with time range filtering."""
        # Define time range (last 3 days)
        end_time = datetime.now()
        start_time = end_time - timedelta(days=3)
        time_range = (start_time, end_time)
        
        # Reset mock to track this specific call
        self.mock_db_manager.reset_mock()
        
        aggregated = self.aggregator.aggregate_results(
            result_type=RESULT_TYPE_PERFORMANCE,
            aggregation_level=AGGREGATION_LEVEL_MODEL,
            time_range=time_range
        )
        
        # Check that the DB was called
        self.assertTrue(self.mock_db_manager.get_performance_results.called)
        
        # Check that the called arguments contain our aggregation level
        called_args, called_kwargs = self.mock_db_manager.get_performance_results.call_args
        self.assertEqual(called_kwargs.get('aggregation_level'), AGGREGATION_LEVEL_MODEL)
        # The time_range might be modified internally, so we just check it's called
        
    def test_caching(self):
        """Test that results are cached correctly."""
        # First call should fetch from DB
        self.aggregator.aggregate_results(
            result_type=RESULT_TYPE_PERFORMANCE,
            aggregation_level=AGGREGATION_LEVEL_MODEL
        )
        
        # Reset mock to track next call
        self.mock_db_manager.get_performance_results.reset_mock()
        
        # Second call with same parameters should use cache
        self.aggregator.aggregate_results(
            result_type=RESULT_TYPE_PERFORMANCE,
            aggregation_level=AGGREGATION_LEVEL_MODEL
        )
        
        # Check that DB was not called again
        self.mock_db_manager.get_performance_results.assert_not_called()
        
        # Reset mock again
        self.mock_db_manager.get_performance_results.reset_mock()
        
        # Call with different parameters should hit DB again
        self.aggregator.aggregate_results(
            result_type=RESULT_TYPE_PERFORMANCE,
            aggregation_level=AGGREGATION_LEVEL_HARDWARE
        )
        
        # Check that DB was called again
        self.assertTrue(self.mock_db_manager.get_performance_results.called)
        
        # We don't care about the exact parameters as they might change with implementation
        # The important part is that the cache works correctly
        
    def test_anomaly_detection(self):
        """Test anomaly detection in results."""
        # Force the anomaly detection to work by adding known anomalies directly to the test data
        # Create anomalous data for testing
        anomalous_performance_results = self.performance_results.copy()
        
        # Add extreme values for specific results to ensure they are flagged as anomalies
        for result in anomalous_performance_results:
            if result.get("model_id") == "model1" and result.get("hardware_id") == "hw1" and result.get("run_id") == "run_0":
                # Create an obvious latency spike (10x normal)
                result["average_latency_ms"] = result.get("average_latency_ms", 100) * 10
                
            if result.get("model_id") == "model2" and result.get("hardware_id") == "hw2" and result.get("run_id") == "run_0":
                # Create an obvious throughput improvement (10x normal)
                result["throughput_items_per_second"] = result.get("throughput_items_per_second", 25) * 10
        
        # Update the mock to return our manipulated data
        original_side_effect = self.mock_db_manager.get_performance_results.side_effect
        self.mock_db_manager.get_performance_results.side_effect = lambda **kwargs: anomalous_performance_results
        
        # Aggregate results to get anomalies
        aggregated = self.aggregator.aggregate_results(
            result_type=RESULT_TYPE_PERFORMANCE,
            aggregation_level=AGGREGATION_LEVEL_MODEL_HARDWARE,
            use_cache=False  # Force a fresh calculation
        )
        
        # Check that anomalies section exists
        self.assertIn("anomalies", aggregated["results"])
        
        # Even if empty, we should be able to get the anomaly report
        anomaly_report = self.aggregator.get_result_anomalies(
            result_type=RESULT_TYPE_PERFORMANCE,
            aggregation_level=AGGREGATION_LEVEL_MODEL_HARDWARE
        )
        
        # Check report structure
        self.assertIn("anomalies", anomaly_report)
        self.assertIn("anomaly_count", anomaly_report)
        
        # Reset the side effect
        self.mock_db_manager.get_performance_results.side_effect = original_side_effect
        
    def test_compatibility_results(self):
        """Test aggregation of compatibility results."""
        # Aggregate by hardware
        aggregated = self.aggregator.aggregate_results(
            result_type=RESULT_TYPE_COMPATIBILITY,
            aggregation_level=AGGREGATION_LEVEL_HARDWARE
        )
        
        # Check basic structure
        self.assertEqual(aggregated["result_type"], RESULT_TYPE_COMPATIBILITY)
        self.assertEqual(aggregated["aggregation_level"], AGGREGATION_LEVEL_HARDWARE)
        
        # Check that distributions were calculated
        self.assertIn("distributions", aggregated["results"])
        distributions = aggregated["results"]["distributions"]
        
        # Check that we have distributions for each hardware
        self.assertIn("hw1", distributions)
        self.assertIn("hw2", distributions)
        
        # Check that is_compatible was analyzed
        hw1_dist = distributions["hw1"]
        self.assertIn("is_compatible", hw1_dist)
        
        # Check that distribution percentages sum to 100%
        is_compatible_dist = hw1_dist["is_compatible"]["distribution"]
        total_percentage = sum(item["percentage"] for item in is_compatible_dist.values())
        self.assertAlmostEqual(total_percentage, 100.0)
        
    def test_integration_results(self):
        """Test aggregation of integration test results."""
        # Aggregate by test module
        aggregated = self.aggregator.aggregate_results(
            result_type=RESULT_TYPE_INTEGRATION,
            aggregation_level=AGGREGATION_LEVEL_TASK_TYPE
        )
        
        # Check basic structure
        self.assertEqual(aggregated["result_type"], RESULT_TYPE_INTEGRATION)
        self.assertEqual(aggregated["aggregation_level"], AGGREGATION_LEVEL_TASK_TYPE)
        
        # Check that we have basic statistics
        basic_stats = aggregated["results"]["basic_statistics"]
        self.assertIsNotNone(basic_stats)
        
        # At minimum there should be a key in the basic stats
        self.assertGreater(len(basic_stats), 0)
        
        # Get first key from basic stats to check its structure
        first_group = list(basic_stats.keys())[0]
        
        # Check that the first group contains basic stats about execution time and pass rate
        group_stats = basic_stats[first_group]
        self.assertIn("execution_time_seconds", group_stats)
        self.assertIn("passed", group_stats)
        
    def test_web_platform_results(self):
        """Test aggregation of web platform results."""
        # Aggregate by platform:browser
        aggregated = self.aggregator.aggregate_results(
            result_type=RESULT_TYPE_WEB_PLATFORM,
            aggregation_level=AGGREGATION_LEVEL_TASK_TYPE
        )
        
        # Check basic structure
        self.assertEqual(aggregated["result_type"], RESULT_TYPE_WEB_PLATFORM)
        self.assertEqual(aggregated["aggregation_level"], AGGREGATION_LEVEL_TASK_TYPE)
        
        # Check that we have results for each platform:browser combination
        basic_stats = aggregated["results"]["basic_statistics"]
        self.assertIn("webnn:chrome", basic_stats)
        self.assertIn("webgpu:chrome", basic_stats)
        self.assertIn("webnn:firefox", basic_stats)
        self.assertIn("webgpu:firefox", basic_stats)
        
        # Check that timing metrics were analyzed
        webgpu_chrome = basic_stats["webgpu:chrome"]
        self.assertIn("load_time_ms", webgpu_chrome)
        self.assertIn("inference_time_ms", webgpu_chrome)
        self.assertIn("total_time_ms", webgpu_chrome)
        
    def test_comparative_analysis(self):
        """Test comparative analysis of results."""
        # Configure mock DB to return different results for historical query
        original_results = self.performance_results.copy()
        
        # Modify results for historical comparison (20% slower)
        historical_results = []
        for result in original_results:
            historical = result.copy()
            historical["average_latency_ms"] *= 1.2
            historical["total_time_seconds"] *= 1.2
            historical["throughput_items_per_second"] *= 0.8
            historical_results.append(historical)
            
        # Configure mock to return normal results first, then historical
        self.mock_db_manager.get_performance_results.side_effect = lambda **kwargs: historical_results if 'time_range' in kwargs and kwargs['time_range'] is not None else original_results
        
        # Aggregate with comparison
        aggregated = self.aggregator.aggregate_results(
            result_type=RESULT_TYPE_PERFORMANCE,
            aggregation_level=AGGREGATION_LEVEL_MODEL
        )
        
        # Check that comparisons were generated
        self.assertIn("comparisons", aggregated["results"])
        comparisons = aggregated["results"]["comparisons"]
        
        # Since we now have proper validation, we'll only check model1 which should be definitely present
        self.assertIn("model1", comparisons)
        
        # Check that metrics were compared
        model1_comparison = comparisons["model1"]
        self.assertIn("average_latency_ms", model1_comparison)
        self.assertIn("throughput_items_per_second", model1_comparison)
        
        # Check that percentage changes were calculated
        latency_comparison = model1_comparison["average_latency_ms"]
        self.assertIn("pct_change_mean", latency_comparison)
        self.assertIn("is_improvement", latency_comparison)
        
        # Reset mock side effect
        self.mock_db_manager.reset_mock()
        self.mock_db_manager.get_performance_results.side_effect = lambda **kwargs: self.performance_results
        
    def test_result_export(self):
        """Test exporting aggregated results."""
        # Aggregate results
        aggregated = self.aggregator.aggregate_results(
            result_type=RESULT_TYPE_PERFORMANCE,
            aggregation_level=AGGREGATION_LEVEL_MODEL
        )
        
        # Export as JSON
        json_output = self.aggregator.export_results(
            result_type=RESULT_TYPE_PERFORMANCE,
            aggregation_level=AGGREGATION_LEVEL_MODEL,
            format="json"
        )
        
        # Check that JSON is valid
        json_data = json.loads(json_output)
        self.assertEqual(json_data["result_type"], RESULT_TYPE_PERFORMANCE)
        
        # Export as CSV
        csv_output = self.aggregator.export_results(
            result_type=RESULT_TYPE_PERFORMANCE,
            aggregation_level=AGGREGATION_LEVEL_MODEL,
            format="csv"
        )
        
        # Check that CSV is valid
        df = pd.read_csv(io.StringIO(csv_output))
        self.assertIn("group", df.columns)
        self.assertIn("metric", df.columns)
        self.assertIn("mean", df.columns)
        
    def test_correlation_analysis(self):
        """Test correlation analysis between metrics."""
        # Analyze correlations between latency and throughput
        correlation_results = self.aggregator.analyze_correlations(
            result_type=RESULT_TYPE_PERFORMANCE,
            metrics=["average_latency_ms", "throughput_items_per_second"]
        )
        
        # Check basic structure
        self.assertIn("correlations", correlation_results)
        self.assertIn("average_latency_ms_vs_throughput_items_per_second", correlation_results["correlations"])
        
        # Check correlation details
        correlation = correlation_results["correlations"]["average_latency_ms_vs_throughput_items_per_second"]
        self.assertIn("correlation", correlation)
        self.assertIn("p_value", correlation)
        self.assertIn("strength", correlation)
        self.assertIn("direction", correlation)
        
        # Latency and throughput should be negatively correlated
        self.assertLess(correlation["correlation"], 0)
        self.assertEqual(correlation["direction"], "negative")
        
    def test_custom_pipeline(self):
        """Test adding custom processing functions to the pipeline."""
        # Create a custom preprocessor
        def custom_preprocessor(results, context):
            for result in results:
                result["custom_field"] = "processed"
            return results
            
        # Create a custom aggregator
        def custom_aggregator(results, context):
            return {"custom_aggregation": {"count": len(results)}}
            
        # Create a custom postprocessor
        def custom_postprocessor(aggregated_results, context):
            aggregated_results["results"]["custom_post"] = {"processed": True}
            
        # Register custom pipeline components
        self.aggregator.register_preprocessor(custom_preprocessor)
        self.aggregator.register_aggregator(custom_aggregator)
        self.aggregator.register_postprocessor(custom_postprocessor)
        
        # Aggregate results
        aggregated = self.aggregator.aggregate_results(
            result_type=RESULT_TYPE_PERFORMANCE,
            aggregation_level=AGGREGATION_LEVEL_MODEL
        )
        
        # Check that custom components were applied
        self.assertIn("custom_aggregation", aggregated["results"])
        self.assertIn("custom_post", aggregated["results"])
        self.assertEqual(aggregated["results"]["custom_post"]["processed"], True)
        
    def test_aggregation_level_handling(self):
        """Test that different aggregation levels are handled correctly."""
        # Test all aggregation levels
        levels = [
            AGGREGATION_LEVEL_TEST_RUN,
            AGGREGATION_LEVEL_MODEL,
            AGGREGATION_LEVEL_HARDWARE,
            AGGREGATION_LEVEL_MODEL_HARDWARE,
            AGGREGATION_LEVEL_TASK_TYPE,
            AGGREGATION_LEVEL_WORKER
        ]
        
        for level in levels:
            # Reset mock to track each call separately
            self.mock_db_manager.reset_mock()
            
            # Aggregate results
            aggregated = self.aggregator.aggregate_results(
                result_type=RESULT_TYPE_PERFORMANCE,
                aggregation_level=level
            )
            
            # Check that level is correct
            self.assertEqual(aggregated["aggregation_level"], level)
            
            # Check that we have results
            self.assertIn("results", aggregated)
            self.assertIn("basic_statistics", aggregated["results"])
            
            # Check that the DB was called
            self.assertTrue(self.mock_db_manager.get_performance_results.called)
            
            # Check that the correct aggregation level was used in the call
            if self.mock_db_manager.get_performance_results.call_args:
                called_args, called_kwargs = self.mock_db_manager.get_performance_results.call_args
                self.assertEqual(called_kwargs.get('aggregation_level'), level)
            

if __name__ == "__main__":
    unittest.main()