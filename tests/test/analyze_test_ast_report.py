#!/usr/bin/env python3
"""
Analyze AST reports generated by generate_test_ast_report.py.

This script takes a JSON report of test file ASTs and produces analytical
insights to aid in test refactoring decisions. It generates visualizations
and patterns to identify redundancy and standardization opportunities.
"""

import json
import argparse
import os
from typing import Dict, List, Any, Set, Tuple, Counter
from collections import defaultdict, Counter
import matplotlib.pyplot as plt
import networkx as nx
from datetime import datetime


def load_report(report_file: str) -> Dict[str, Any]:
    """Load a previously generated AST report from JSON."""
    with open(report_file, 'r', encoding='utf-8') as f:
        return json.load(f)


def identify_test_patterns(report: Dict[str, Any]) -> Dict[str, Any]:
    """Identify common patterns in test files."""
    patterns = {
        'test_method_names': Counter(),
        'class_names': Counter(),
        'base_classes': Counter(),
        'common_imports': Counter(),
        'method_argument_patterns': Counter(),
        'method_size_distribution': defaultdict(int),
        'test_methods_per_class_distribution': defaultdict(int),
        'class_size_distribution': defaultdict(int),
    }
    
    # Pattern identification
    for file_info in report['files']:
        # Analyze imports
        for imp in file_info['imports']:
            module = imp['module']
            if 'pytest' in module or 'unittest' in module:
                patterns['common_imports'][module] += 1
        
        # Analyze classes
        for class_info in file_info['classes']:
            class_name = class_info['name']
            patterns['class_names'][class_name] += 1
            
            # Analyze base classes
            for base in class_info['bases']:
                patterns['base_classes'][base] += 1
            
            # Class size (by number of methods)
            num_methods = len(class_info['methods'])
            patterns['class_size_distribution'][num_methods] += 1
            
            # Test methods per class
            test_methods = sum(1 for m in class_info['methods'] if m.get('is_test', False))
            patterns['test_methods_per_class_distribution'][test_methods] += 1
            
            # Analyze methods
            for method in class_info['methods']:
                if method.get('is_test', False):
                    # Test method naming patterns
                    name = method['name']
                    patterns['test_method_names'][name] += 1
                    
                    # Argument patterns for test methods
                    arg_pattern = ','.join(method['args'])
                    patterns['method_argument_patterns'][arg_pattern] += 1
                    
                    # Method size distribution
                    method_size = method['end_line_number'] - method['line_number']
                    patterns['method_size_distribution'][method_size] += 1
    
    return patterns


def identify_similar_classes(report: Dict[str, Any], threshold: float = 0.7) -> List[Dict[str, Any]]:
    """Identify classes that are similar based on their methods and structure."""
    similar_groups = []
    
    # Build a dictionary of classes with their methods
    class_methods = {}
    for file_info in report['files']:
        for class_info in file_info['classes']:
            class_key = f"{file_info['filename']}:{class_info['name']}"
            methods = {m['name'] for m in class_info['methods']}
            class_methods[class_key] = {
                'file': file_info['filename'],
                'class': class_info['name'],
                'methods': methods,
                'bases': set(class_info['bases']),
                'total_methods': len(methods)
            }
    
    # Compare each pair of classes
    compared = set()
    for class1, info1 in class_methods.items():
        for class2, info2 in class_methods.items():
            if class1 == class2 or (class1, class2) in compared or (class2, class1) in compared:
                continue
            
            compared.add((class1, class2))
            
            # Skip if they have completely different base classes
            if not (info1['bases'] & info2['bases']) and info1['bases'] and info2['bases']:
                continue
            
            # Calculate Jaccard similarity of methods
            common_methods = info1['methods'] & info2['methods']
            all_methods = info1['methods'] | info2['methods']
            
            if not all_methods:
                continue
                
            similarity = len(common_methods) / len(all_methods)
            
            if similarity >= threshold:
                similar_groups.append({
                    'class1': class1,
                    'class2': class2,
                    'similarity': similarity,
                    'common_methods': list(common_methods),
                    'unique_to_1': list(info1['methods'] - info2['methods']),
                    'unique_to_2': list(info2['methods'] - info1['methods']),
                })
    
    # Sort by similarity (descending)
    similar_groups.sort(key=lambda x: x['similarity'], reverse=True)
    return similar_groups


def identify_inheritance_clusters(report: Dict[str, Any]) -> List[Dict[str, Any]]:
    """Identify clusters of related classes through inheritance."""
    G = nx.DiGraph()
    
    # Build inheritance graph
    for relationship in report.get('class_relationships', []):
        G.add_edge(relationship['parent'], relationship['child'])
    
    # Identify connected components
    clusters = []
    for component in nx.weakly_connected_components(G):
        if len(component) > 1:  # Only interested in related clusters
            cluster = {
                'classes': list(component),
                'size': len(component),
                'common_bases': find_common_bases(component, report)
            }
            clusters.append(cluster)
    
    # Sort by cluster size
    clusters.sort(key=lambda x: x['size'], reverse=True)
    return clusters


def find_common_bases(classes: Set[str], report: Dict[str, Any]) -> List[str]:
    """Find common base classes within a cluster of classes."""
    base_counts = Counter()
    
    for file_info in report['files']:
        for class_info in file_info['classes']:
            class_key = f"{file_info['filename']}:{class_info['name']}"
            if class_key in classes:
                for base in class_info['bases']:
                    base_counts[base] += 1
    
    # Return bases used by at least 2 classes in the cluster
    return [base for base, count in base_counts.items() if count >= 2]


def identify_redundant_imports(report: Dict[str, Any]) -> Dict[str, Any]:
    """Identify patterns of redundant imports across test files."""
    import_patterns = defaultdict(list)
    
    for file_info in report['files']:
        file_imports = set()
        for imp in file_info['imports']:
            if imp['type'] == 'import':
                import_name = imp['module']
                if imp['alias']:
                    import_name += f" as {imp['alias']}"
                file_imports.add(import_name)
            else:  # importfrom
                module = imp['module']
                name = imp['name']
                import_name = f"from {module} import {name}"
                if imp['alias']:
                    import_name += f" as {imp['alias']}"
                file_imports.add(import_name)
        
        # Add this file's imports to the pattern dictionary
        import_key = frozenset(file_imports)
        import_patterns[import_key].append(file_info['filename'])
    
    # Filter to patterns that appear in multiple files
    redundant_patterns = {
        imports: files for imports, files in import_patterns.items() 
        if len(files) > 1
    }
    
    # Convert to a more JSON-friendly format
    result = []
    for imports, files in redundant_patterns.items():
        if len(imports) > 2:  # Only include patterns with multiple imports
            result.append({
                'imports': list(imports),
                'files': files,
                'count': len(files)
            })
    
    # Sort by frequency (descending)
    result.sort(key=lambda x: x['count'], reverse=True)
    return result[:30]  # Limit to top 30 patterns


def generate_visualizations(patterns: Dict[str, Any], similar_classes: List[Dict[str, Any]], 
                           clusters: List[Dict[str, Any]], output_dir: str) -> None:
    """Generate visualizations based on the analysis."""
    os.makedirs(output_dir, exist_ok=True)
    
    # 1. Plot test method size distribution
    plot_distribution(
        patterns['method_size_distribution'], 
        'Test Method Size Distribution (lines of code)',
        'Lines of Code',
        'Number of Methods',
        os.path.join(output_dir, 'test_method_size_distribution.png')
    )
    
    # 2. Plot test methods per class distribution
    plot_distribution(
        patterns['test_methods_per_class_distribution'],
        'Test Methods per Class Distribution',
        'Number of Test Methods',
        'Number of Classes',
        os.path.join(output_dir, 'test_methods_per_class_distribution.png')
    )
    
    # 3. Plot class size distribution
    plot_distribution(
        patterns['class_size_distribution'],
        'Class Size Distribution (number of methods)',
        'Number of Methods',
        'Number of Classes',
        os.path.join(output_dir, 'class_size_distribution.png')
    )
    
    # 4. Plot class similarity network
    if similar_classes:
        plot_similarity_network(
            similar_classes, 
            os.path.join(output_dir, 'class_similarity_network.png')
        )
    
    # 5. Plot inheritance hierarchy
    if clusters:
        plot_inheritance_clusters(
            clusters,
            os.path.join(output_dir, 'inheritance_clusters.png')
        )


def plot_distribution(data: Dict[int, int], title: str, xlabel: str, ylabel: str, output_file: str) -> None:
    """Plot a distribution as a histogram."""
    plt.figure(figsize=(10, 6))
    
    # Convert to lists for plotting
    x = list(data.keys())
    y = list(data.values())
    
    plt.bar(x, y)
    plt.title(title)
    plt.xlabel(xlabel)
    plt.ylabel(ylabel)
    plt.grid(axis='y', alpha=0.75)
    
    plt.tight_layout()
    plt.savefig(output_file)
    plt.close()


def plot_similarity_network(similar_classes: List[Dict[str, Any]], output_file: str) -> None:
    """Plot a network visualization of class similarities."""
    G = nx.Graph()
    
    # Add edges for similar classes
    for pair in similar_classes:
        class1 = pair['class1']
        class2 = pair['class2']
        similarity = pair['similarity']
        
        G.add_node(class1)
        G.add_node(class2)
        G.add_edge(class1, class2, weight=similarity)
    
    # Draw the graph
    plt.figure(figsize=(12, 12))
    
    # Use spring layout for node positioning
    pos = nx.spring_layout(G)
    
    # Draw nodes and edges
    nx.draw_networkx_nodes(G, pos, node_size=100, alpha=0.8)
    
    # Edge width based on similarity
    edge_widths = [G[u][v]['weight'] * 2 for u, v in G.edges()]
    nx.draw_networkx_edges(G, pos, width=edge_widths, alpha=0.5)
    
    # Draw labels with small font
    nx.draw_networkx_labels(G, pos, font_size=8)
    
    plt.title("Class Similarity Network")
    plt.axis('off')
    plt.tight_layout()
    plt.savefig(output_file, dpi=300)
    plt.close()


def plot_inheritance_clusters(clusters: List[Dict[str, Any]], output_file: str) -> None:
    """Plot clusters of related classes through inheritance."""
    G = nx.DiGraph()
    
    # Add nodes for all classes in all clusters
    for cluster in clusters:
        for class_name in cluster['classes']:
            G.add_node(class_name)
    
    # Add edges for inheritance relationships
    # (We need to rebuild this since clusters only contain class names)
    for cluster in clusters:
        for i, class1 in enumerate(cluster['classes']):
            for class2 in cluster['classes'][i+1:]:
                if nx.has_path(G, class1, class2):
                    G.add_edge(class1, class2)
                elif nx.has_path(G, class2, class1):
                    G.add_edge(class2, class1)
    
    # Draw the graph
    plt.figure(figsize=(14, 14))
    
    # Use spring layout instead of graphviz_layout to avoid dependency on pygraphviz
    pos = nx.spring_layout(G, k=0.5, iterations=100)
    
    # Draw nodes and edges
    nx.draw_networkx_nodes(G, pos, node_size=100, alpha=0.8)
    nx.draw_networkx_edges(G, pos, width=1.0, alpha=0.5, 
                          arrows=True, arrowsize=15)
    
    # Draw labels with small font
    nx.draw_networkx_labels(G, pos, font_size=8)
    
    plt.title("Class Inheritance Clusters")
    plt.axis('off')
    plt.tight_layout()
    plt.savefig(output_file, dpi=300)
    plt.close()


def generate_recommendation_report(report: Dict[str, Any], patterns: Dict[str, Any], 
                                 similar_classes: List[Dict[str, Any]], 
                                 clusters: List[Dict[str, Any]],
                                 redundant_imports: List[Dict[str, Any]],
                                 output_file: str) -> None:
    """Generate a markdown report with refactoring recommendations."""
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write(f"# Test Refactoring Recommendations\n\n")
        f.write(f"Generated: {timestamp}\n\n")
        
        f.write("## Summary\n\n")
        f.write(f"- Total test files analyzed: {report['summary']['total_files']}\n")
        f.write(f"- Total classes: {report['summary']['total_classes']}\n")
        f.write(f"- Total test methods: {report['summary']['total_test_methods']}\n")
        f.write(f"- Similar class groups identified: {len(similar_classes)}\n")
        f.write(f"- Inheritance clusters identified: {len(clusters)}\n\n")
        
        f.write("## Common Patterns\n\n")
        
        # Top base classes
        f.write("### Common Base Classes\n\n")
        f.write("| Base Class | Usage Count |\n")
        f.write("|------------|-------------|\n")
        for base, count in patterns['base_classes'].most_common(10):
            f.write(f"| {base} | {count} |\n")
        f.write("\n")
        
        # Common imports
        f.write("### Common Test Framework Imports\n\n")
        f.write("| Import | Usage Count |\n")
        f.write("|--------|-------------|\n")
        for imp, count in patterns['common_imports'].most_common(10):
            f.write(f"| {imp} | {count} |\n")
        f.write("\n")
        
        # Common test method names
        f.write("### Common Test Method Names\n\n")
        f.write("| Method Name | Usage Count |\n")
        f.write("|------------|-------------|\n")
        for name, count in patterns['test_method_names'].most_common(15):
            f.write(f"| {name} | {count} |\n")
        f.write("\n")
        
        # Most similar classes
        f.write("## Most Similar Classes\n\n")
        f.write("These classes have high similarity and may be candidates for consolidation.\n\n")
        
        for i, pair in enumerate(similar_classes[:10]):
            f.write(f"### Similarity Group {i+1} (Similarity: {pair['similarity']:.2f})\n\n")
            f.write(f"- Class 1: `{pair['class1']}`\n")
            f.write(f"- Class 2: `{pair['class2']}`\n\n")
            
            f.write("Common methods:\n")
            for method in pair['common_methods']:
                f.write(f"- `{method}`\n")
            f.write("\n")
        
        # Inheritance clusters
        f.write("## Inheritance Clusters\n\n")
        f.write("These groups of classes are related through inheritance and may benefit from standardization.\n\n")
        
        for i, cluster in enumerate(clusters[:5]):
            f.write(f"### Cluster {i+1} (Size: {cluster['size']})\n\n")
            f.write("Classes in this cluster:\n")
            for class_name in cluster['classes']:
                f.write(f"- `{class_name}`\n")
            
            if cluster['common_bases']:
                f.write("\nCommon base classes:\n")
                for base in cluster['common_bases']:
                    f.write(f"- `{base}`\n")
            f.write("\n")
        
        # Redundant import patterns
        f.write("## Redundant Import Patterns\n\n")
        f.write("These sets of imports appear together in multiple files and may be candidates for consolidation in utility modules.\n\n")
        
        for i, pattern in enumerate(redundant_imports[:5]):
            f.write(f"### Import Pattern {i+1} (Used in {pattern['count']} files)\n\n")
            f.write("Imports:\n")
            for imp in pattern['imports']:
                f.write(f"- `{imp}`\n")
            
            f.write("\nFiles using this pattern:\n")
            for file in pattern['files'][:5]:  # Show only first 5 files
                f.write(f"- {file}\n")
            
            if len(pattern['files']) > 5:
                f.write(f"- ... and {len(pattern['files']) - 5} more files\n")
            f.write("\n")
        
        # Recommendations
        f.write("## Refactoring Recommendations\n\n")
        
        f.write("Based on the analysis, here are the key recommendations:\n\n")
        
        # Base class recommendations
        common_bases = [base for base, count in patterns['base_classes'].most_common(3)]
        f.write("1. **Standardize Base Classes**: Consolidate test classes to use a consistent set of base classes:\n")
        for base in common_bases:
            f.write(f"   - `{base}`\n")
        f.write("\n")
        
        # Pattern recommendations
        f.write("2. **Standardize Test Method Naming**: Adopt consistent naming conventions for test methods based on current patterns.\n\n")
        
        # Consolidation recommendations
        f.write("3. **Consolidate Similar Classes**: Merge highly similar classes, particularly:\n")
        for pair in similar_classes[:3]:
            f.write(f"   - `{pair['class1']}` and `{pair['class2']}` (similarity: {pair['similarity']:.2f})\n")
        f.write("\n")
        
        # Import recommendations
        f.write("4. **Create Utility Modules**: Create common test utility modules to minimize import duplication:\n")
        f.write("   - Create a `test_utils.py` module with common imports and helper functions\n")
        f.write("   - Create a `test_fixtures.py` module with common test fixtures\n\n")
        
        # Inheritance recommendations
        f.write("5. **Refactor Inheritance Hierarchy**: Streamline the inheritance structure to reduce complexity:\n")
        f.write("   - Create a consistent set of base test classes\n")
        f.write("   - Remove redundant inheritance levels\n")
        f.write("   - Use composition over inheritance where appropriate\n\n")
        
        # Documentation recommendations
        f.write("6. **Improve Documentation**: Add or improve docstrings in test methods and classes to clarify purpose.\n\n")
        
        # Next steps
        f.write("## Next Steps\n\n")
        f.write("1. Prioritize refactoring efforts based on this analysis\n")
        f.write("2. Create a refactoring plan with specific tasks\n")
        f.write("3. Implement changes gradually, starting with base classes and utility modules\n")
        f.write("4. Validate each change with comprehensive test runs\n")
        f.write("5. Update documentation to reflect the new test organization\n")


def main():
    parser = argparse.ArgumentParser(description='Analyze test AST reports and generate recommendations.')
    parser.add_argument('--report', '-r', type=str, required=True, 
                      help='AST report JSON file (generated by generate_test_ast_report.py)')
    parser.add_argument('--output-dir', '-o', type=str, default='test_analysis',
                      help='Output directory for analysis files (default: test_analysis)')
    parser.add_argument('--similarity', '-s', type=float, default=0.7,
                      help='Similarity threshold for identifying similar classes (default: 0.7)')
    
    args = parser.parse_args()
    
    # Create output directory
    os.makedirs(args.output_dir, exist_ok=True)
    
    # Load the report
    print(f"Loading AST report from {args.report}...")
    report = load_report(args.report)
    
    # Analyze patterns
    print("Identifying test patterns...")
    patterns = identify_test_patterns(report)
    
    # Identify similar classes
    print("Identifying similar classes...")
    similar_classes = identify_similar_classes(report, args.similarity)
    
    # Identify inheritance clusters
    print("Identifying inheritance clusters...")
    clusters = identify_inheritance_clusters(report)
    
    # Identify redundant imports
    print("Identifying redundant import patterns...")
    redundant_imports = identify_redundant_imports(report)
    
    # Generate visualizations
    print("Generating visualizations...")
    generate_visualizations(
        patterns, 
        similar_classes, 
        clusters, 
        os.path.join(args.output_dir, 'visualizations')
    )
    
    # Generate recommendation report
    print("Generating recommendation report...")
    generate_recommendation_report(
        report,
        patterns,
        similar_classes,
        clusters,
        redundant_imports,
        os.path.join(args.output_dir, 'refactoring_recommendations.md')
    )
    
    # Save analysis results as JSON
    print("Saving analysis results...")
    analysis_results = {
        'patterns': {k: dict(v) if isinstance(v, Counter) else dict(sorted(v.items())) 
                    for k, v in patterns.items()},
        'similar_classes': similar_classes[:50],  # Limit to top 50
        'inheritance_clusters': clusters,
        'redundant_imports': redundant_imports,
    }
    
    with open(os.path.join(args.output_dir, 'analysis_results.json'), 'w', encoding='utf-8') as f:
        json.dump(analysis_results, f, indent=2)
    
    print(f"Analysis complete! Results saved to {args.output_dir}")
    print(f"Recommendations available at: {os.path.join(args.output_dir, 'refactoring_recommendations.md')}")


if __name__ == '__main__':
    main()