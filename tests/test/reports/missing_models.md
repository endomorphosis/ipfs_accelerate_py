# HuggingFace Model Coverage Report

**Date:** 2025-03-22 02:58:24

## Summary

- **Total models tracked:** 309
- **Implemented models:** 309 (100.0%)
- **Missing models:** 0 (0.0%)

## Coverage by Architecture

### Decoder-only Models

- **Total decoder-only models:** 27
- **Implemented:** 27 (100.0%)
- **Missing:** 0

### Encoder-decoder Models

- **Total encoder-decoder models:** 22
- **Implemented:** 22 (100.0%)
- **Missing:** 0

### Encoder-only Models

- **Total encoder-only models:** 31
- **Implemented:** 31 (100.0%)
- **Missing:** 0

### Multimodal Models

- **Total multimodal models:** 15
- **Implemented:** 15 (100.0%)
- **Missing:** 0

### Speech Models

- **Total speech models:** 10
- **Implemented:** 10 (100.0%)
- **Missing:** 0

### Unknown Models

- **Total unknown models:** 165
- **Implemented:** 165 (100.0%)
- **Missing:** 0

### Vision Models

- **Total vision models:** 25
- **Implemented:** 25 (100.0%)
- **Missing:** 0

### Vision-text Models

- **Total vision-text models:** 14
- **Implemented:** 14 (100.0%)
- **Missing:** 0

## Coverage by Priority

### Critical Priority Models

- **Total critical models:** 62
- **Implemented:** 62 (100.0%)
- **Missing:** 0

### High Priority Models

- **Total high models:** 28
- **Implemented:** 28 (100.0%)
- **Missing:** 0

### Medium Priority Models

- **Total medium models:** 219
- **Implemented:** 219 (100.0%)
- **Missing:** 0

## Implementation Roadmap

### Critical Priority Models

These models should be implemented first due to their importance and widespread use:

*All critical priority models have been implemented!*

### High Priority Models

These models should be implemented next:

*All high priority models have been implemented!*

### Medium Priority Models

These models can be implemented after critical and high priority models:

## Implemented Models

These models already have test implementations:

### Decoder-only Models

- `bloom`
- `codellama`
- `falcon`
- `gpt-j`
- `gpt2`
- `gpt_j`
- `llama`
- `llama-3`
- `llama2`
- `llama_3`
- `mistral`
- `mistral-next`
- `mistral_next`
- `mixtral`
- `mllama`
- `mpt`
- `phi`
- `phi3`
- `phi4`

### Encoder-decoder Models

- `bart`
- `flan-t5`
- `flan_t5`
- `led`
- `longt5`
- `marian`
- `mbart`
- `mt5`
- `pegasus`
- `pegasus-x`
- `speecht5`
- `t5`
- `umt5`

### Encoder-only Models

- `albert`
- `bert`
- `camembert`
- `convbert`
- `deberta`
- `deberta-v2`
- `deberta_v2`
- `distilbert`
- `electra`
- `flaubert`
- `ibert`
- `megatron-bert`
- `mobilebert`
- `rembert`
- `roberta`
- `squeezebert`
- `xlm-roberta`
- `xlm-roberta-xl`
- `xlm_roberta`

### Multimodal Models

- `git`
- `llava`
- `llava-next`
- `llava-next-video`
- `llava_next`
- `paligemma`
- `pix2struct`
- `video-llava`
- `video_llava`
- `vision-text-dual-encoder`
- `vision_text_dual_encoder`

### Speech Models

- `bark`
- `hubert`
- `wav2vec2`
- `wav2vec2-conformer`
- `wav2vec2_bert`
- `whisper`

### Unknown Models

- `align`
- `audio`
- `audioldm2`
- `bigbird`
- `bit`
- `blenderbot`
- `canine`
- `clap`
- `codegen`
- `command-r`
- `command_r`
- `conditional-detr`
- `ctrl`
- `cvt`
- `data2vec`
- `data2vec-audio`
- `data2vec-text`
- `data2vec-vision`
- `data2vec_audio`
- `data2vec_text`
- `data2vec_vision`
- `decoder_only`
- `depth-anything`
- `detr`
- `dinat`
- `dino`
- `donut`
- `dpt`
- `efficientnet`
- `encodec`
- `encoder_decoder`
- `encoder_only`
- `ernie`
- `esm`
- `flamingo`
- `flava`
- `florence`
- `funnel`
- `fuyu`
- `gemma`
- `gemma2`
- `gemma3`
- `gpt-2`
- `gpt-neo`
- `gpt-neox`
- `gpt_neo`
- `gpt_neox`
- `gptj`
- `idefics`
- `idefics2`
- `idefics3`
- `imagebind`
- `imagegpt`
- `kosmos-2`
- `kosmos2`
- `kosmos_2`
- `layoutlm`
- `longformer`
- `luke`
- `m2m-100`
- `m2m_100`
- `mamba`
- `mask2former`
- `mlp-mixer`
- `mlp_mixer`
- `mobilenet-v1`
- `mobilenet-v2`
- `mobilenet_v2`
- `mpnet`
- `mra`
- `multimodal`
- `musicgen`
- `nemotron`
- `nezha`
- `nystromformer`
- `olmo`
- `olmoe`
- `openai-gpt`
- `opt`
- `perceiver`
- `persimmon`
- `prophetnet`
- `qwen2`
- `qwen2-vl`
- `qwen3`
- `qwen3-vl`
- `recurrent-gemma`
- `reformer`
- `regnet`
- `roformer`
- `rwkv`
- `sam`
- `seamless-m4t`
- `seamless_m4t`
- `segformer`
- `sew`
- `siglip`
- `speech-to-text`
- `speech-to-text-2`
- `speech_to_text`
- `speech_to_text_2`
- `splinter`
- `stablelm`
- `starcoder2`
- `switch-transformers`
- `switch_transformers`
- `tapas`
- `transfo-xl`
- `transfo_xl`
- `trocr_base`
- `trocr_large`
- `unispeech`
- `usm`
- `van`
- `vilt`
- `vinvl`
- `vision`
- `vision_encoder_decoder`
- `wavlm`
- `xlm`
- `xlnet`
- `xmod`
- `yolos`

### Vision Models

- `beit`
- `beit3`
- `convnext`
- `convnextv2`
- `deit`
- `dinov2`
- `levit`
- `mobilevit`
- `poolformer`
- `resnet`
- `swin`
- `swinv2`
- `vit`
- `vitdet`

### Vision-text Models

- `blip`
- `blip-2`
- `blip_2`
- `chinese-clip`
- `chinese_clip`
- `clip`
- `clipseg`
- `instructblip`
- `vision-encoder-decoder`
- `xclip`