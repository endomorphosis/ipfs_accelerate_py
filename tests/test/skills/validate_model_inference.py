#!/usr/bin/env python3
"""
Model-specific inference validation for hyphenated model tests.

This script:
1. Performs real model inference validation using the HuggingFace Transformers library
2. Validates model class selection and initialization
3. Tests inference with actual model weights (optionally with smaller models)
4. Provides detailed error analysis for model architecture compatibility

Usage:
    python validate_model_inference.py --model MODEL_ID [--test-file TEST_FILE] [--use-small]
"""

import os
import sys
import json
import time
import argparse
import logging
import traceback
import importlib.util
from pathlib import Path
from datetime import datetime
from unittest.mock import MagicMock

# Set up logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
)
logger = logging.getLogger(__name__)

# Constants
CURRENT_DIR = Path(os.path.dirname(os.path.abspath(__file__)))
FIXED_TESTS_DIR = CURRENT_DIR / "fixed_tests"
VALIDATION_REPORTS_DIR = CURRENT_DIR / "validation_reports"

# Create validation directories if they don't exist
os.makedirs(FIXED_TESTS_DIR, exist_ok=True)
os.makedirs(VALIDATION_REPORTS_DIR, exist_ok=True)

# We need to load these conditionally rather than at import time
# to allow this validation script to run with mock objects for CI/CD

def to_valid_identifier(text):
    """Convert hyphenated model names to valid Python identifiers."""
    # Replace hyphens with underscores
    text = text.replace("-", "_")
    # Remove special characters
    text = ''.join(c for c in text if c.isalnum() or c == '_')
    # Ensure it doesn't start with a number
    if text and text[0].isdigit():
        text = '_' + text
    return text

def get_smaller_model(model_name):
    """Get a smaller variant of a model for faster testing."""
    # Map of large models to their smaller variants
    model_map = {
        "gpt-j": "hf-internal-testing/tiny-random-gptj",
        "gpt-neo": "hf-internal-testing/tiny-random-gptneo",
        "gpt-neox": "hf-internal-testing/tiny-random-gptneox",
        "xlm-roberta": "hf-internal-testing/tiny-random-xlm-roberta",
        "bert": "hf-internal-testing/tiny-bert-for-token-classification",
        "t5": "hf-internal-testing/tiny-random-t5",
        "vit": "hf-internal-testing/tiny-random-vit",
        "wav2vec2": "hf-internal-testing/tiny-random-wav2vec2",
        "wav2vec2-bert": "hf-internal-testing/tiny-random-wav2vec2",
        "speech-to-text": "hf-internal-testing/tiny-random-speech-encoder-decoder",
        "speech-to-text-2": "hf-internal-testing/tiny-random-speech-encoder-decoder",
        "clip": "hf-internal-testing/tiny-random-clip",
        "chinese-clip": "hf-internal-testing/tiny-random-clip",
        "data2vec-text": "hf-internal-testing/tiny-random-data2vec-text",
        "data2vec-audio": "hf-internal-testing/tiny-random-data2vec-audio",
        "data2vec-vision": "hf-internal-testing/tiny-random-data2vec-vision"
    }
    
    return model_map.get(model_name, f"hf-internal-testing/tiny-random-{model_name}")

def get_model_class_for_architecture(model_name):
    """Get the appropriate HuggingFace model class for a model name."""
    try:
        # Import transformers conditionally
        import transformers
        
        # Map of model names to their appropriate classes
        model_class_map = {
            "gpt-j": transformers.GPTJForCausalLM,
            "gpt-neo": transformers.GPTNeoForCausalLM, 
            "gpt-neox": transformers.GPTNeoXForCausalLM,
            "xlm-roberta": transformers.XLMRobertaForMaskedLM,
            "bert": transformers.BertForMaskedLM,
            "t5": transformers.T5ForConditionalGeneration,
            "vit": transformers.ViTForImageClassification,
            "wav2vec2": transformers.Wav2Vec2ForCTC,
            "wav2vec2-bert": transformers.Wav2Vec2BertForCTC,
            "speech-to-text": transformers.Speech2TextForConditionalGeneration,
            "speech-to-text-2": transformers.Speech2Text2ForCTC,
            "clip": transformers.CLIPModel,
            "chinese-clip": transformers.CLIPModel,
            "vision-text-dual-encoder": transformers.VisionTextDualEncoderModel,
            "data2vec-text": transformers.Data2VecTextForMaskedLM,
            "data2vec-audio": transformers.Data2VecAudioForCTC,
            "data2vec-vision": transformers.Data2VecVisionForImageClassification
        }
        
        if model_name in model_class_map:
            return model_class_map[model_name]
        else:
            # Fall back to auto classes
            return transformers.AutoModel
            
    except ImportError:
        logger.warning("Transformers not available, returning mock class")
        return MagicMock()

def load_and_run_test_file(test_file, model_name=None, use_small=False):
    """Load a test file as a module and run its tests."""
    try:
        # Get the module name from the file name
        module_name = os.path.splitext(os.path.basename(test_file))[0]
        
        # Create a spec from the file path
        spec = importlib.util.spec_from_file_location(module_name, test_file)
        module = importlib.util.module_from_spec(spec)
        
        # Add the parent directory to sys.path temporarily
        parent_dir = os.path.dirname(os.path.abspath(test_file))
        sys.path.insert(0, parent_dir)
        
        # Execute the module
        spec.loader.exec_module(module)
        
        # Get the test class
        test_classes = [obj for name, obj in module.__dict__.items() 
                        if isinstance(obj, type) and name.startswith("Test")]
        
        if not test_classes:
            return False, "No test classes found in the module"
        
        test_class = test_classes[0]
        
        # Determine the model name to use
        if model_name is None:
            # Use the default model name from the module
            model_name = getattr(module, "DEFAULT_MODEL", None)
            
            if model_name is None:
                # Try to extract from class name
                class_name = test_class.__name__
                if class_name.startswith("Test") and class_name.endswith("Models"):
                    model_part = class_name[4:-6]  # Remove "Test" and "Models"
                    model_name = model_part.lower()
        
        # Use a smaller model if requested
        if use_small and model_name:
            small_model = get_smaller_model(model_name)
            logger.info(f"Using small test model: {small_model}")
            model_name = small_model
        
        # Create an instance of the test class
        try:
            tester = test_class(model_name)
        except Exception as e:
            return False, f"Error creating test class instance: {str(e)}"
        
        # Run the tests
        logger.info(f"Running tests for {model_name} using {test_class.__name__}")
        start_time = time.time()
        try:
            results = tester.run_tests(all_hardware=False)
            execution_time = time.time() - start_time
            
            logger.info(f"Tests completed in {execution_time:.2f} seconds")
            
            # Check if tests were successful
            success = False
            for key, result in results.get("results", {}).items():
                if key.startswith("pipeline_") and result.get("pipeline_success"):
                    success = True
                    break
                    
            if not success:
                for key, result in results.get("results", {}).items():
                    if key.startswith("from_pretrained_") and result.get("from_pretrained_success"):
                        success = True
                        break
            
            if success:
                return True, results
            else:
                # Find the errors
                errors = []
                for key, result in results.get("results", {}).items():
                    if key.startswith("pipeline_") and not result.get("pipeline_success"):
                        errors.append(f"Pipeline error: {result.get('pipeline_error', 'Unknown error')}")
                    elif key.startswith("from_pretrained_") and not result.get("from_pretrained_success"):
                        errors.append(f"from_pretrained error: {result.get('from_pretrained_error', 'Unknown error')}")
                
                return False, f"Tests failed: {'; '.join(errors)}"
                
        except Exception as e:
            execution_time = time.time() - start_time
            return False, f"Error running tests (after {execution_time:.2f}s): {str(e)}"
            
    except Exception as e:
        return False, f"Error loading test file: {str(e)}"
    finally:
        # Remove the parent directory from sys.path
        if parent_dir in sys.path:
            sys.path.remove(parent_dir)

def test_model_with_transformers(model_name, use_small=False):
    """Test model class compatibility with transformers (without running the test file)."""
    try:
        # Import transformers directly
        import transformers
        import torch
        
        logger.info(f"Testing model {model_name} directly with transformers")
        
        # Get the actual model name to use (small or original)
        actual_model_name = get_smaller_model(model_name) if use_small else model_name
        
        # Get the appropriate model class
        model_class = get_model_class_for_architecture(model_name)
        if model_class is None:
            return False, f"No model class found for {model_name}"
        
        # Try to load the tokenizer
        start_time = time.time()
        try:
            tokenizer = transformers.AutoTokenizer.from_pretrained(actual_model_name)
            tokenizer_time = time.time() - start_time
            logger.info(f"Tokenizer loaded in {tokenizer_time:.2f}s")
        except Exception as e:
            return False, f"Error loading tokenizer: {str(e)}"
        
        # Try to load the model
        try:
            start_time = time.time()
            model = model_class.from_pretrained(actual_model_name)
            model_time = time.time() - start_time
            logger.info(f"Model loaded in {model_time:.2f}s")
        except Exception as e:
            return False, f"Error loading model: {str(e)}"
        
        # Basic model validation
        if hasattr(model, "config"):
            logger.info(f"Model configuration: {model.config.__class__.__name__}")
        
        # Return success with timing info
        return True, {
            "model": model_name,
            "actual_model": actual_model_name,
            "model_class": model_class.__name__,
            "tokenizer_class": tokenizer.__class__.__name__,
            "tokenizer_load_time": tokenizer_time,
            "model_load_time": model_time,
            "vocab_size": getattr(tokenizer, "vocab_size", None),
            "model_config": str(getattr(model, "config", None))
        }
            
    except ImportError as e:
        return False, f"Transformers or torch not available: {str(e)}"
    except Exception as e:
        return False, f"Error testing model: {str(e)}"

def validate_model_inference(model_name, test_file=None, use_small=False):
    """Run complete model inference validation."""
    results = {
        "model_name": model_name,
        "test_file": test_file,
        "timestamp": datetime.now().isoformat(),
        "use_small_model": use_small,
        "direct_transformers_test": None,
        "test_file_execution": None,
        "issues": [],
        "recommendations": []
    }
    
    # Step 1: Test with transformers directly (if available)
    logger.info(f"Testing {model_name} directly with transformers")
    transformers_success, transformers_result = test_model_with_transformers(model_name, use_small)
    results["direct_transformers_test"] = {
        "success": transformers_success,
        "details": transformers_result
    }
    
    if not transformers_success:
        results["issues"].append(f"Direct transformers test failed: {transformers_result}")
        results["recommendations"].append("Install transformers and torch to enable model inference validation")
    
    # Step 2: If test file provided, run it
    if test_file:
        logger.info(f"Testing {model_name} using test file {test_file}")
        test_success, test_result = load_and_run_test_file(test_file, model_name, use_small)
        results["test_file_execution"] = {
            "success": test_success,
            "details": test_result
        }
        
        if not test_success:
            results["issues"].append(f"Test file execution failed: {test_result}")
            
            # Add specific recommendations based on errors
            error_str = str(test_result).lower()
            if "import" in error_str:
                results["recommendations"].append("Install required dependencies: transformers, torch, tokenizers")
            elif "cuda" in error_str:
                results["recommendations"].append("Use CPU device instead of CUDA for testing")
            elif "class" in error_str:
                results["recommendations"].append("Check model class names for proper capitalization")
    else:
        # Construct default test file path
        model_id = to_valid_identifier(model_name)
        default_test_file = os.path.join(FIXED_TESTS_DIR, f"test_hf_{model_id}.py")
        
        if os.path.exists(default_test_file):
            logger.info(f"Found default test file: {default_test_file}")
            test_success, test_result = load_and_run_test_file(default_test_file, model_name, use_small)
            results["test_file_execution"] = {
                "success": test_success,
                "details": test_result,
                "default_file_used": default_test_file
            }
            
            if not test_success:
                results["issues"].append(f"Default test file execution failed: {test_result}")
        else:
            results["issues"].append(f"No test file provided and default file not found: {default_test_file}")
            results["recommendations"].append(f"Generate test file using integrate_generator_fixes.py")
    
    # Determine overall success
    transformers_ok = results["direct_transformers_test"]["success"] if results["direct_transformers_test"] else False
    test_file_ok = results["test_file_execution"]["success"] if results["test_file_execution"] else False
    
    results["success"] = transformers_ok or test_file_ok
    
    # Generate final recommendations
    if not results["success"]:
        if not transformers_ok and not test_file_ok:
            results["recommendations"].append("Install transformers, torch, and tokenizers for full validation")
        
        if not test_file_ok and test_file:
            results["recommendations"].append("Check test file for compatibility with model architecture")
    
    return results

def save_results(results, output_dir=VALIDATION_REPORTS_DIR):
    """Save validation results to a JSON file."""
    os.makedirs(output_dir, exist_ok=True)
    
    model_name = results["model_name"]
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # Create filename from model name
    safe_model = model_name.replace("/", "__").replace("-", "_")
    filename = f"inference_validation_{safe_model}_{timestamp}.json"
    output_path = os.path.join(output_dir, filename)
    
    with open(output_path, "w") as f:
        json.dump(results, f, indent=2)
    
    logger.info(f"Results saved to {output_path}")
    return output_path

def main():
    """Command-line entry point."""
    parser = argparse.ArgumentParser(description="Validate model inference for hyphenated models")
    parser.add_argument("--model", type=str, required=True, help="Model name to validate")
    parser.add_argument("--test-file", type=str, help="Path to test file (optional)")
    parser.add_argument("--use-small", action="store_true", help="Use smaller model variants for testing")
    parser.add_argument("--output-dir", type=str, default=str(VALIDATION_REPORTS_DIR), help="Output directory for results")
    
    args = parser.parse_args()
    
    # Run validation
    logger.info(f"Validating model inference for {args.model}")
    results = validate_model_inference(args.model, args.test_file, args.use_small)
    
    # Save results
    output_path = save_results(results, args.output_dir)
    
    # Print summary to console
    status = "✅ Passed" if results["success"] else "❌ Failed"
    print(f"\nInference validation for {args.model}: {status}")
    
    if results["issues"]:
        print("\nIssues detected:")
        for issue in results["issues"]:
            print(f"- {issue}")
    
    if results["recommendations"]:
        print("\nRecommendations:")
        for rec in results["recommendations"]:
            print(f"- {rec}")
    
    print(f"\nDetailed results saved to: {output_path}")
    
    return 0 if results["success"] else 1

if __name__ == "__main__":
    sys.exit(main())