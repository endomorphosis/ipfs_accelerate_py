#!/usr/bin/env python3

"""
Generate minimal test files for HuggingFace models.
This script uses a simplified approach that directly writes test files
without complex template substitution.
"""

import os
import sys
import json
import logging
import argparse
import re
from pathlib import Path

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Try to import model lookup
try:
    from find_models import get_recommended_default_model, query_huggingface_api
    HAS_MODEL_LOOKUP = True
    logger.info("Model lookup integration available")
except ImportError:
    HAS_MODEL_LOOKUP = False
    logger.warning("Model lookup not available, using static model registry")

# Default models for common types
DEFAULT_MODELS = {
    "bert": "google-bert/bert-base-uncased",
    "gpt2": "gpt2",
    "t5": "t5-small",
    "vit": "google/vit-base-patch16-224",
    "gpt-j": "EleutherAI/gpt-j-6b",
    "whisper": "openai/whisper-base.en"
}

def get_model_from_registry(model_type):
    """Get the best default model for a model type."""
    if HAS_MODEL_LOOKUP:
        try:
            default_model = get_recommended_default_model(model_type)
            logger.info(f"Using recommended model for {model_type}: {default_model}")
            return default_model
        except Exception as e:
            logger.warning(f"Error getting recommended model for {model_type}: {e}")
            # Fall back to registry lookup
    
    # Use static defaults
    if model_type in DEFAULT_MODELS:
        return DEFAULT_MODELS[model_type]
    
    # For unknown models, use a heuristic approach
    return f"{model_type}-base" if "-base" not in model_type else model_type

def to_valid_identifier(text):
    """Convert text to a valid Python identifier."""
    # Replace hyphens with underscores
    text = text.replace("-", "_")
    # Remove any other invalid characters
    text = re.sub(r'[^a-zA-Z0-9_]', '', text)
    # Ensure it doesn't start with a number
    if text and text[0].isdigit():
        text = '_' + text
    return text

def generate_minimal_test(model_type, output_dir="."):
    """Generate a minimal test file for a model type."""
    logger.info(f"Generating minimal test for {model_type}")
    
    # Get default model
    default_model = get_model_from_registry(model_type)
    
    # Convert model type to valid Python identifier
    model_type_valid = to_valid_identifier(model_type)
    
    # Determine model class and task
    if model_type == "bert":
        model_class = "BertForMaskedLM"
        task = "fill-mask"
        input_text = "The quick brown fox jumps over the [MASK] dog."
    elif model_type == "gpt2":
        model_class = "GPT2LMHeadModel"
        task = "text-generation"
        input_text = "GPT-2 is a transformer model that"
    elif model_type == "t5":
        model_class = "T5ForConditionalGeneration"
        task = "text2text-generation"
        input_text = "translate English to German: The house is wonderful."
    elif model_type == "vit":
        model_class = "ViTForImageClassification"
        task = "image-classification"
        input_text = None
    else:
        model_class = "AutoModel"
        task = "feature-extraction"
        input_text = f"{model_type} is a model that"
    
    # Create test file content
    content = f'''#!/usr/bin/env python3

"""
Test file for {model_type} model.
Generated by generate_minimal_test.py
"""

import os
import sys
import json
import time
import logging
import argparse
from unittest.mock import MagicMock
from pathlib import Path

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Check if dependencies are available
try:
    import torch
    HAS_TORCH = True
except ImportError:
    torch = MagicMock()
    HAS_TORCH = False
    logger.warning("torch not available, using mock")

try:
    import transformers
    HAS_TRANSFORMERS = True
except ImportError:
    transformers = MagicMock()
    HAS_TRANSFORMERS = False
    logger.warning("transformers not available, using mock")

def select_device():
    """Select the best available device for inference."""
    if HAS_TORCH and torch.cuda.is_available():
        return "cuda:0"
    else:
        return "cpu"

class Test{model_type.capitalize()}Models:
    """Test class for {model_type} models."""
    
    def __init__(self, model_id="{default_model}", device=None):
        """Initialize the test class.
        
        Args:
            model_id: The model ID to test
            device: The device to run tests on (default: None = auto-select)
        """
        self.model_id = model_id
        self.device = device if device else select_device()
        self.performance_stats = {{}}
        
    def test_pipeline(self):
        """Test the model using the pipeline API."""
        try:
            if not HAS_TRANSFORMERS:
                logger.warning("Transformers library not available, skipping pipeline test")
                return {{"success": False, "error": "Transformers library not available"}}
                
            logger.info(f"Testing {model_type} model {{self.model_id}} with pipeline API on {{self.device}}")
            
            # Record start time
            start_time = time.time()
            
            # Initialize the pipeline
            pipe = transformers.pipeline(
                "{task}", 
                model=self.model_id,
                device=self.device if self.device != "cpu" else -1
            )
            
            # Record model loading time
            load_time = time.time() - start_time
            logger.info(f"Model loading time: {{load_time:.2f}} seconds")
            
            # Test with a simple input
            test_input = "{input_text}" if "{input_text}" else None
            
            # Skip if no input available (e.g., for image models)
            if not test_input:
                logger.info("Skipping inference for image or audio model")
                return {{"success": True, "model_id": self.model_id, "device": self.device}}
            
            # Record inference start time
            inference_start = time.time()
            
            # Run inference
            outputs = pipe(test_input)
            
            # Record inference time
            inference_time = time.time() - inference_start
            
            # Store performance stats
            self.performance_stats["pipeline"] = {{
                "load_time": load_time,
                "inference_time": inference_time
            }}
            
            return {{
                "success": True,
                "model_id": self.model_id,
                "device": self.device,
                "inference_time": inference_time
            }}
                
        except Exception as e:
            logger.error(f"Error testing pipeline: {{e}}")
            return {{"success": False, "error": str(e)}}
    
    def run_tests(self, all_hardware=False):
        """Run all tests for this model."""
        results = {{}}
        
        # Run pipeline test
        pipeline_result = self.test_pipeline()
        results["pipeline"] = pipeline_result
        
        # Add metadata
        results["metadata"] = {{
            "model_id": self.model_id,
            "device": self.device,
            "has_transformers": HAS_TRANSFORMERS,
            "has_torch": HAS_TORCH
        }}
        
        return results

def main():
    """Command-line entry point."""
    parser = argparse.ArgumentParser(description="Test {model_type} HuggingFace models")
    parser.add_argument("--model", type=str, default="{default_model}", help="Model ID to test")
    parser.add_argument("--device", type=str, help="Device to run tests on (cuda, cpu)")
    
    args = parser.parse_args()
    
    # Initialize the test class
    tester = Test{model_type.capitalize()}Models(model_id=args.model, device=args.device)
    
    # Run the tests
    results = tester.run_tests()
    
    # Print a summary
    success = results["pipeline"].get("success", False)
    
    print(f"\\nTEST RESULTS SUMMARY:")
    
    if success:
        print(f"✅ Successfully tested {{args.model}}")
        print(f"  - Device: {{tester.device}}")
        if "inference_time" in results["pipeline"]:
            print(f"  - Inference time: {{results['pipeline']['inference_time']:.4f}}s")
    else:
        print(f"❌ Failed to test {{args.model}}")
        print(f"  - Error: {{results['pipeline'].get('error', 'Unknown error')}}")
    
    return 0 if success else 1

if __name__ == "__main__":
    sys.exit(main())
'''
    
    # Create output directory if needed
    os.makedirs(output_dir, exist_ok=True)
    
    # Write test file
    output_path = os.path.join(output_dir, f"test_hf_{model_type_valid}.py")
    with open(output_path, "w") as f:
        f.write(content)
    
    # Verify syntax
    try:
        compile(content, output_path, 'exec')
        logger.info(f"✅ Syntax is valid for {output_path}")
        return True
    except SyntaxError as e:
        logger.error(f"❌ Syntax error in generated file: {e}")
        if hasattr(e, 'lineno') and e.lineno is not None:
            lines = content.split('\n')
            line_no = e.lineno - 1  # 0-based index
            if 0 <= line_no < len(lines):
                logger.error(f"Problematic line {e.lineno}: {lines[line_no].rstrip()}")
        return False

def main():
    """Command-line entry point."""
    parser = argparse.ArgumentParser(description="Generate minimal test files for HuggingFace models")
    parser.add_argument("model_type", nargs="?", default="bert", help="Model type to generate test for")
    parser.add_argument("--output-dir", type=str, default="ultra_simple_tests", help="Output directory for test files")
    parser.add_argument("--all", action="store_true", help="Generate tests for all common model types")
    
    args = parser.parse_args()
    
    # Create output directory
    os.makedirs(args.output_dir, exist_ok=True)
    
    if args.all:
        # Generate tests for all common model types
        model_types = ["bert", "gpt2", "t5", "vit"]
        success_count = 0
        
        for model_type in model_types:
            if generate_minimal_test(model_type, args.output_dir):
                success_count += 1
        
        logger.info(f"Generated {success_count} of {len(model_types)} test files successfully")
        return 0 if success_count == len(model_types) else 1
    else:
        # Generate test for single model type
        return 0 if generate_minimal_test(args.model_type, args.output_dir) else 1

if __name__ == "__main__":
    sys.exit(main())