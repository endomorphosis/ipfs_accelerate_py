#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Time Series Performance Tracker for IPFS Accelerate Python Framework

This module implements the time-series performance tracking system mentioned in NEXT_STEPS.md.
It provides components for versioned test results, regression detection, trend visualization,
and notification systems.

Date: March 2025
"""

import os
import sys
import json
import time
import datetime
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from typing import Dict, List, Tuple, Union, Optional, Any
from pathlib import Path

# Add parent directory to path
sys.path.append()))))))str()))))))Path()))))))__file__).resolve()))))))).parent.parent))

# Local imports
try:
    from data.duckdb.core.benchmark_db_api import get_db_connection
    from data.duckdb.core.benchmark_db_query import query_database
    from benchmark_regression_analyzer import analyze_regressions
except ImportError:
    print()))))))"Warning: Some local modules could not be imported.")


class TimeSeriesSchema:
    """Schema extension for versioned test results in DuckDB database."""
    
    def __init__()))))))self, db_path: Optional[str] = None):,,,
    """Initialize with optional database path."""
    self.db_path = db_path or os.environ.get()))))))'BENCHMARK_DB_PATH', './benchmark_db.duckdb')
    
    def create_schema_extensions()))))))self) -> None:
        """Create schema extensions for versioned test results."""
        conn = get_db_connection()))))))self.db_path)
        
        # Create version history table if it doesn't exist
        conn.execute()))))))"""
        CREATE TABLE IF NOT EXISTS version_history ()))))))
        id INTEGER PRIMARY KEY,
        timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
        version_tag VARCHAR,
        description VARCHAR,
        user_id VARCHAR,
        commit_hash VARCHAR,
        git_branch VARCHAR
        )
        """)
        
        # Create performance time series table if it doesn't exist
        conn.execute()))))))"""
        CREATE TABLE IF NOT EXISTS performance_time_series ()))))))
        id INTEGER PRIMARY KEY,
        version_id INTEGER,
        model_id INTEGER,
        hardware_id INTEGER,
        batch_size INTEGER,
        test_type VARCHAR,
        throughput FLOAT,
        latency FLOAT,
        memory_usage FLOAT,
        timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
        FOREIGN KEY ()))))))version_id) REFERENCES version_history()))))))id),
        FOREIGN KEY ()))))))model_id) REFERENCES models()))))))id),
        FOREIGN KEY ()))))))hardware_id) REFERENCES hardware_platforms()))))))id)
        )
        """)
        
        # Create regression alerts table if it doesn't exist
        conn.execute()))))))"""
        CREATE TABLE IF NOT EXISTS regression_alerts ()))))))
        id INTEGER PRIMARY KEY,
        timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
        model_id INTEGER,
        hardware_id INTEGER,
        metric VARCHAR,
        previous_value FLOAT,
        current_value FLOAT,
        percent_change FLOAT,
        severity VARCHAR,
        status VARCHAR DEFAULT 'active',
        notification_sent BOOLEAN DEFAULT FALSE,
        resolved_at TIMESTAMP,
        FOREIGN KEY ()))))))model_id) REFERENCES models()))))))id),
        FOREIGN KEY ()))))))hardware_id) REFERENCES hardware_platforms()))))))id)
        )
        """)
        
        conn.close())))))))
        print()))))))"Schema extensions created successfully!")

:
class RegressionDetector:
    """Detects performance regressions in time series data."""
    
    def __init__()))))))self, db_path: Optional[str] = None, threshold: float = 0.10):,
    """Initialize with database path and regression threshold."""
    self.db_path = db_path or os.environ.get()))))))'BENCHMARK_DB_PATH', './benchmark_db.duckdb')
    self.threshold = threshold
        
    def detect_regressions()))))))self, model_id: Optional[int] = None, 
    hardware_id: Optional[int] = None,
    metric: str = 'throughput',
    window_size: int = 5) -> List[Dict[str, Any]]:,,
    """
    Detect performance regressions for specified model/hardware.
        
        Args:
            model_id: Filter by model ID
            hardware_id: Filter by hardware ID
            metric: Performance metric to analyze ()))))))throughput, latency, memory_usage)
            window_size: Number of previous data points to use for baseline
            
        Returns:
            List of regression events
            """
            conn = get_db_connection()))))))self.db_path)
        
        # Build query filters
            filters = [],,
            params = {}}}}}
        
        if model_id is not None:
            filters.append()))))))"model_id = :model_id")
            params['model_id'] = model_id
            ,
        if hardware_id is not None:
            filters.append()))))))"hardware_id = :hardware_id")
            params['hardware_id'] = hardware_id
            ,
            where_clause = f"WHERE {}}}}' AND '.join()))))))filters)}" if filters else ""
        
        # Query time series data
            query = f"""
            SELECT
            ph.id,
            m.name as model_name,
            hp.type as hardware_type,
            ph.batch_size,
            ph.{}}}}metric},
            ph.timestamp
            FROM
            performance_time_series ph
            JOIN
            models m ON ph.model_id = m.id
            JOIN
            hardware_platforms hp ON ph.hardware_id = hp.id
            {}}}}where_clause}
            ORDER BY
            m.name, hp.type, ph.batch_size, ph.timestamp
            """
        
            result = conn.execute()))))))query, params).fetchall())))))))
            conn.close())))))))
        :
        if not result:
            return [],,
        
        # Convert to pandas DataFrame for analysis
            df = pd.DataFrame()))))))result, columns=['id', 'model_name', 'hardware_type',
            'batch_size', metric, 'timestamp'])
        
        # Group by model, hardware, and batch size
            groups = df.groupby()))))))['model_name', 'hardware_type', 'batch_size'])
            ,
            regressions = [],,
        
        for name, group in groups:
            model_name, hardware_type, batch_size = name
            
            # Sort by timestamp
            group = group.sort_values()))))))'timestamp')
            
            # Need at least window_size+1 points to detect regression
            if len()))))))group) <= window_size:
            continue
                
            # Calculate rolling window statistics
            values = group[metric].values,
            timestamps = group['timestamp'],.values
            ,
            for i in range()))))))window_size, len()))))))values)):
                window = values[i-window_size:i],
                current = values[i]
                ,
                # Calculate baseline ()))))))mean of window)
                baseline = np.mean()))))))window)
                
                # For latency, lower is better, so check for increase
                if metric == 'latency':
                    percent_change = ()))))))current - baseline) / baseline
                    is_regression = percent_change > self.threshold
                # For other metrics, higher is better, so check for decrease
                else:
                    percent_change = ()))))))baseline - current) / baseline
                    is_regression = percent_change > self.threshold
                
                if is_regression:
                    # Determine severity
                    if percent_change > 0.25:
                        severity = 'critical'
                    elif percent_change > 0.15:
                        severity = 'high'
                    else:
                        severity = 'medium'
                        
                    # Add to regressions list
                        regressions.append())))))){}}}}
                        'model_name': model_name,
                        'hardware_type': hardware_type,
                        'batch_size': batch_size,
                        'metric': metric,
                        'previous_value': baseline,
                        'current_value': current,
                        'percent_change': percent_change * 100,
                        'timestamp': timestamps[i],
                        'severity': severity
                        })
        
                        return regressions
    
                        def record_regressions()))))))self, regressions: List[Dict[str, Any]]) -> None:,
                        """
                        Record detected regressions in the database.
        
        Args:
            regressions: List of regression events
            """
        if not regressions:
            return
            
            conn = get_db_connection()))))))self.db_path)
        
        for reg in regressions:
            # Get model_id and hardware_id
            model_query = "SELECT id FROM models WHERE name = ?"
            model_id = conn.execute()))))))model_query, ()))))))reg['model_name'],)).fetchone())))))))[0]
            ,
            hardware_query = "SELECT id FROM hardware_platforms WHERE type = ?"
            hardware_id = conn.execute()))))))hardware_query, ()))))))reg['hardware_type'],)).fetchone())))))))[0]
            ,
            # Insert regression alert
            insert_query = """
            INSERT INTO regression_alerts 
            ()))))))model_id, hardware_id, metric, previous_value, current_value,
            percent_change, severity, timestamp)
            VALUES ()))))))?, ?, ?, ?, ?, ?, ?, ?)
            """
            
            conn.execute()))))))insert_query, ()))))))
            model_id,
            hardware_id,
            reg['metric'],,
            reg['previous_value'],
            reg['current_value'],
            reg['percent_change'],
            reg['severity'],
            reg['timestamp'],
            ))
        
            conn.commit())))))))
            conn.close())))))))
        
            print()))))))f"Recorded {}}}}len()))))))regressions)} regression alerts in the database.")


class TrendVisualizer:
    """Creates visualizations for performance trends over time."""
    
    def __init__()))))))self, db_path: Optional[str] = None):,,,
    """Initialize with database path."""
    self.db_path = db_path or os.environ.get()))))))'BENCHMARK_DB_PATH', './benchmark_db.duckdb')
        
    def visualize_metric_trend()))))))self, 
    model_name: str,
    hardware_type: str,
    metric: str = 'throughput',
    batch_size: Optional[int] = None,
    start_date: Optional[str] = None,
    end_date: Optional[str] = None,
    output_file: Optional[str] = None) -> str:,
    """
    Create visualization for a metric trend over time.
        
        Args:
            model_name: Name of the model
            hardware_type: Type of hardware
            metric: Performance metric to visualize
            batch_size: Optional filter for batch size
            start_date: Optional start date for filtering ()))))))format: YYYY-MM-DD)
            end_date: Optional end date for filtering ()))))))format: YYYY-MM-DD)
            output_file: Path to save the visualization
            
        Returns:
            Path to the saved visualization
            """
            conn = get_db_connection()))))))self.db_path)
        
        # Build query filters
            filters = ["m.name = :model_name", "hp.type = :hardware_type"],
            params = {}}}}
            'model_name': model_name,
            'hardware_type': hardware_type
            }
        
        if batch_size is not None:
            filters.append()))))))"pts.batch_size = :batch_size")
            params['batch_size'] = batch_size
            ,
        if start_date:
            filters.append()))))))"pts.timestamp >= :start_date")
            params['start_date'] = start_date
            ,
        if end_date:
            filters.append()))))))"pts.timestamp <= :end_date")
            params['end_date'] = end_date
            ,
            where_clause = f"WHERE {}}}}' AND '.join()))))))filters)}"
        
        # Query time series data
            query = f"""
            SELECT
            pts.batch_size,
            pts.{}}}}metric},
            pts.timestamp,
            vh.version_tag
            FROM
            performance_time_series pts
            JOIN
            models m ON pts.model_id = m.id
            JOIN
            hardware_platforms hp ON pts.hardware_id = hp.id
            LEFT JOIN
            version_history vh ON pts.version_id = vh.id
            {}}}}where_clause}
            ORDER BY
            pts.timestamp
            """
        
            result = conn.execute()))))))query, params).fetchall())))))))
            conn.close())))))))
        
        if not result:
            print()))))))f"No data found for {}}}}model_name} on {}}}}hardware_type}")
            return ""
        
        # Convert to pandas DataFrame
            df = pd.DataFrame()))))))result, columns=['batch_size', metric, 'timestamp', 'version_tag'])
            ,
        # Convert timestamp to datetime
            df['timestamp'], = pd.to_datetime()))))))df['timestamp'],)
        
        # Set up plot
            plt.figure()))))))figsize=()))))))12, 6))
            sns.set_style()))))))'whitegrid')
        
        # If batch_size is not specified, group by batch_size
        if batch_size is None:
            for bs, group in df.groupby()))))))'batch_size'):
                plt.plot()))))))group['timestamp'],, group[metric], marker='o', linestyle='-', 
                label=f'Batch Size {}}}}bs}')
        else:
            plt.plot()))))))df['timestamp'],, df[metric], marker='o', linestyle='-')
            
            # Add version tags as annotations
            for i, row in df.iterrows()))))))):
                if row['version_tag']:,
                plt.annotate()))))))row['version_tag'],
                ()))))))row['timestamp'],, row[metric]),
                textcoords="offset points",
                xytext=()))))))0, 10),
                ha='center')
        
        # Set labels and title
                plt.xlabel()))))))'Date')
                plt.ylabel()))))))f'{}}}}metric.capitalize())))))))}')
                metric_label = metric.replace()))))))'_', ' ').capitalize())))))))
                plt.title()))))))f'{}}}}metric_label} Trend for {}}}}model_name} on {}}}}hardware_type}')
        
        if batch_size is None:
            plt.legend()))))))title='Batch Size')
            
            plt.xticks()))))))rotation=45)
            plt.tight_layout())))))))
        
        # Save the plot if output file is specified:
        if output_file:
            plt.savefig()))))))output_file)
            print()))))))f"Saved visualization to {}}}}output_file}")
            return output_file
        
        # Generate a default filename
            timestamp = datetime.datetime.now()))))))).strftime()))))))"%Y%m%d_%H%M%S")
            output_dir = "benchmark_visualizations"
            os.makedirs()))))))output_dir, exist_ok=True)
        
            filename = f"{}}}}output_dir}/{}}}}model_name}_{}}}}hardware_type}_{}}}}metric}_{}}}}timestamp}.png"
            plt.savefig()))))))filename)
            plt.close())))))))
        
            print()))))))f"Saved visualization to {}}}}filename}")
                return filename
    
                def create_regression_dashboard()))))))self,
                output_file: Optional[str] = None,
                days: int = 30,
                                  limit: int = 10) -> str:
                                      """
                                      Create a dashboard of recent regressions.
        
        Args:
            output_file: Path to save the dashboard
            days: Number of days to include
            limit: Maximum number of regressions to show
            
        Returns:
            Path to the saved dashboard
            """
            conn = get_db_connection()))))))self.db_path)
        
        # Query recent regression alerts
            query = """
            SELECT
            ra.id,
            m.name as model_name,
            hp.type as hardware_type,
            ra.metric,
            ra.previous_value,
            ra.current_value,
            ra.percent_change,
            ra.severity,
            ra.timestamp
            FROM
            regression_alerts ra
            JOIN
            models m ON ra.model_id = m.id
            JOIN
            hardware_platforms hp ON ra.hardware_id = hp.id
            WHERE
            ra.timestamp >= :start_date
            ORDER BY
            ra.timestamp DESC
            LIMIT :limit
            """
        
            start_date = ()))))))datetime.datetime.now()))))))) - datetime.timedelta()))))))days=days)).strftime()))))))"%Y-%m-%d")
            params = {}}}}'start_date': start_date, 'limit': limit}
        
            result = conn.execute()))))))query, params).fetchall())))))))
            conn.close())))))))
        
        if not result:
            print()))))))f"No regression alerts found in the last {}}}}days} days")
            return ""
        
        # Convert to pandas DataFrame
            df = pd.DataFrame()))))))result, columns=['id', 'model_name', 'hardware_type', 'metric',
            'previous_value', 'current_value', 'percent_change',
            'severity', 'timestamp'])
        
        # Create dashboard
            plt.figure()))))))figsize=()))))))14, 10))
        
        # Set up grid for subplots
            grid_size = min()))))))len()))))))df), 9)  # Show at most 9 plots
            rows = ()))))))grid_size + 2) // 3  # Calculate rows needed
        
        # Summary bar chart for all regressions by severity
            plt.subplot()))))))rows + 1, 1, 1)  # Top subplot spans the width
            severity_counts = df['severity'].value_counts()))))))),
            colors = {}}}}'critical': 'red', 'high': 'orange', 'medium': 'yellow'}
            severity_counts.plot()))))))kind='bar', color=[colors.get()))))))s, 'blue') for s in severity_counts.index]):,
            plt.title()))))))'Regression Alerts by Severity')
            plt.ylabel()))))))'Count')
            plt.tight_layout())))))))
        
        # Individual regression plots
            for i, ()))))))_, row) in enumerate()))))))df.iterrows())))))))[:grid_size]):,
            plt.subplot()))))))rows + 1, 3, i + 4)  # +4 to start after the summary
            
            # Get historical data for this model/hardware
            trend_data = self.get_historical_data()))))))
            row['model_name'],
            row['hardware_type'],
            row['metric'],
            )
            
            # Plot trend
            if trend_data:
                x = range()))))))len()))))))trend_data))
                plt.plot()))))))x, trend_data, marker='o', linestyle='-')
                
                # Mark the regression point
                regression_index = len()))))))trend_data) - 1
                plt.scatter()))))))[regression_index], [trend_data[-1]], color='red', s=100, zorder=5)
                ,
                # Add a horizontal line for the previous baseline
                plt.axhline()))))))y=row['previous_value'], color='green', linestyle='--', alpha=0.7)
                ,
                plt.title()))))))f"{}}}}row['model_name']},\n{}}}}row['hardware_type']},", fontsize=9),
                plt.ylabel()))))))row['metric'],)
            
                plt.tight_layout())))))))
        
        # Save the dashboard
        if output_file:
            plt.savefig()))))))output_file)
                return output_file
            
        # Generate a default filename
                timestamp = datetime.datetime.now()))))))).strftime()))))))"%Y%m%d_%H%M%S")
                output_dir = "benchmark_visualizations"
                os.makedirs()))))))output_dir, exist_ok=True)
        
                filename = f"{}}}}output_dir}/regression_dashboard_{}}}}timestamp}.png"
                plt.savefig()))))))filename)
                plt.close())))))))
        
            return filename
    
            def get_historical_data()))))))self, model_name: str, hardware_type: str, metric: str) -> List[float]:,
            """Get historical data for a model/hardware combination."""
            conn = get_db_connection()))))))self.db_path)
        
            query = """
            SELECT
            pts.{}}}}metric}
            FROM
            performance_time_series pts
            JOIN
            models m ON pts.model_id = m.id
            JOIN
            hardware_platforms hp ON pts.hardware_id = hp.id
            WHERE
            m.name = :model_name
            AND hp.type = :hardware_type
            ORDER BY
            pts.timestamp
            LIMIT 20
            """
        
            query = query.format()))))))metric=metric)
            params = {}}}}'model_name': model_name, 'hardware_type': hardware_type}
        
            result = conn.execute()))))))query, params).fetchall())))))))
            conn.close())))))))
        
        if not result:
            return [],,
            
            return [row[0] for row in result]:,
class NotificationSystem:
    """Notification system for performance regressions."""
    
    def __init__()))))))self, db_path: Optional[str] = None):,,,
    """Initialize with database path."""
    self.db_path = db_path or os.environ.get()))))))'BENCHMARK_DB_PATH', './benchmark_db.duckdb')
        
    def get_pending_notifications()))))))self) -> List[Dict[str, Any]]:,,
    """Get regression alerts that need notifications."""
    conn = get_db_connection()))))))self.db_path)
        
    query = """
    SELECT
    ra.id,
    m.name as model_name,
    hp.type as hardware_type,
    ra.metric,
    ra.previous_value,
    ra.current_value,
    ra.percent_change,
    ra.severity,
    ra.timestamp
    FROM
    regression_alerts ra
    JOIN
    models m ON ra.model_id = m.id
    JOIN
    hardware_platforms hp ON ra.hardware_id = hp.id
    WHERE
    ra.notification_sent = FALSE
    AND ra.status = 'active'
    ORDER BY
    ra.severity DESC,
    ra.timestamp DESC
    """
        
    result = conn.execute()))))))query).fetchall())))))))
    conn.close())))))))
        
        if not result:
    return [],,
            
        # Convert to list of dictionaries
    notifications = [],,
        for row in result:
            notifications.append())))))){}}}}
            'id': row[0],
            'model_name': row[1],
            'hardware_type': row[2],
            'metric': row[3],
            'previous_value': row[4],
            'current_value': row[5],
            'percent_change': row[6],
            'severity': row[7],
            'timestamp': row[8],
            })
            
    return notifications
    
    def mark_notification_sent()))))))self, alert_id: int) -> None:
        """Mark a regression alert as notified."""
        conn = get_db_connection()))))))self.db_path)
        
        query = """
        UPDATE regression_alerts
        SET notification_sent = TRUE
        WHERE id = ?
        """
        
        conn.execute()))))))query, ()))))))alert_id,))
        conn.commit())))))))
        conn.close())))))))
    
        def send_github_issue()))))))self, regression: Dict[str, Any]) -> str:,
        """
        Create a GitHub issue for a regression alert.
        
        Args:
            regression: Regression alert information
            
        Returns:
            Issue URL if successful, empty string otherwise
            """
        # This is a placeholder for GitHub API integration
        # In a real implementation, you would use a library like PyGithub:
            print()))))))f"Would create GitHub issue for regression: {}}}}regression['model_name']}, on {}}}}regression['hardware_type']},")
            ,
        # Generate issue title
            severity_emoji = {}}}}
            'critical': 'ðŸ”´',
            'high': 'ðŸŸ ',
            'medium': 'ðŸŸ¡'
            }
            emoji = severity_emoji.get()))))))regression['severity'], 'âšª')
            ,
            title = f"{}}}}emoji} {}}}}regression['severity'].upper())))))))} regression: {}}}}regression['model_name']}, on {}}}}regression['hardware_type']},"
            ,
        # Generate issue body
            body = f"""
## Performance Regression Detected

            - **Model**: {}}}}regression['model_name']},
            - **Hardware**: {}}}}regression['hardware_type']},
            - **Metric**: {}}}}regression['metric'],}
- **Change**: {}}}}regression['percent_change']:.2f}% {}}}}'increase' if regression['metric'], == 'latency' else 'decrease'}:
    - **Previous**: {}}}}regression['previous_value']:.4f},
    - **Current**: {}}}}regression['current_value']:.4f},
    - **Detected**: {}}}}regression['timestamp'],}
    - **Severity**: {}}}}regression['severity']}
    ,
## Recommended Actions

    1. Verify the regression with additional tests
    2. Check recent code changes that might affect performance
    3. Investigate potential hardware or environment issues
    4. Update benchmark baselines if the change is expected

## Automatically Generated

    This issue was automatically generated by the performance regression detection system.
    """
        
        # Simulate issue creation:
            return f"https://github.com/org/repo/issues/{}}}}int()))))))time.time()))))))))}"
    
            def send_email_notification()))))))self, regression: Dict[str, Any]) -> bool:,
            """
            Send email notification about a regression.
        
        Args:
            regression: Regression alert information
            
        Returns:
            True if sent successfully, False otherwise
            """
        # This is a placeholder for email notification:
            print()))))))f"Would send email notification for regression: {}}}}regression['model_name']}, on {}}}}regression['hardware_type']},")
            ,
            return True
    
    def process_notifications()))))))self) -> None:
        """Process all pending notifications."""
        notifications = self.get_pending_notifications())))))))
        
        if not notifications:
            print()))))))"No pending notifications.")
        return
            
        print()))))))f"Processing {}}}}len()))))))notifications)} notification()))))))s)...")
        
        for notification in notifications:
            # Determine notification method based on severity
            if notification['severity'] == 'critical':,
                # For critical regressions, create GitHub issue and send email
            issue_url = self.send_github_issue()))))))notification)
            email_sent = self.send_email_notification()))))))notification)
                
                if issue_url or email_sent:
                    self.mark_notification_sent()))))))notification['id'])
                    ,    ,,
            elif notification['severity'] == 'high':,
                # For high severity, create GitHub issue
            issue_url = self.send_github_issue()))))))notification)
                
                if issue_url:
                    self.mark_notification_sent()))))))notification['id'])
                    ,    ,,
            else:
                # For medium severity, just mark as sent ()))))))will appear in dashboard)
                self.mark_notification_sent()))))))notification['id'])
                ,
                print()))))))"Notification processing complete.")


def main()))))))):
    """Main function for command-line usage."""
    import argparse
    
    parser = argparse.ArgumentParser()))))))description='Time Series Performance Tracker')
    subparsers = parser.add_subparsers()))))))dest='command', help='Command to execute')
    
    # Schema creation command
    schema_parser = subparsers.add_parser()))))))'create-schema', help='Create schema extensions')
    schema_parser.add_argument()))))))'--db-path', help='Database path')
    
    # Regression detection command
    detect_parser = subparsers.add_parser()))))))'detect', help='Detect performance regressions')
    detect_parser.add_argument()))))))'--db-path', help='Database path')
    detect_parser.add_argument()))))))'--model', help='Model name filter')
    detect_parser.add_argument()))))))'--hardware', help='Hardware type filter')
    detect_parser.add_argument()))))))'--metric', default='throughput', 
    choices=['throughput', 'latency', 'memory_usage'],
    help='Metric to analyze')
    detect_parser.add_argument()))))))'--threshold', type=float, default=0.1,
    help='Regression threshold ()))))))0.1 = 10%%)')
    
    # Visualization command
    visualize_parser = subparsers.add_parser()))))))'visualize', help='Create trend visualizations')
    visualize_parser.add_argument()))))))'--db-path', help='Database path')
    visualize_parser.add_argument()))))))'--model', required=True, help='Model name')
    visualize_parser.add_argument()))))))'--hardware', required=True, help='Hardware type')
    visualize_parser.add_argument()))))))'--metric', default='throughput',
    choices=['throughput', 'latency', 'memory_usage'],
    help='Metric to visualize')
    visualize_parser.add_argument()))))))'--batch-size', type=int, help='Batch size filter')
    visualize_parser.add_argument()))))))'--output', help='Output file path')
    
    # Dashboard command
    dashboard_parser = subparsers.add_parser()))))))'dashboard', help='Create regression dashboard')
    dashboard_parser.add_argument()))))))'--db-path', help='Database path')
    dashboard_parser.add_argument()))))))'--days', type=int, default=30, help='Days to include')
    dashboard_parser.add_argument()))))))'--limit', type=int, default=10, help='Maximum regressions')
    dashboard_parser.add_argument()))))))'--output', help='Output file path')
    
    # Notification command
    notify_parser = subparsers.add_parser()))))))'notify', help='Process notifications')
    notify_parser.add_argument()))))))'--db-path', help='Database path')
    
    args = parser.parse_args())))))))
    
    if args.command == 'create-schema':
        schema = TimeSeriesSchema()))))))args.db_path)
        schema.create_schema_extensions())))))))
        
    elif args.command == 'detect':
        detector = RegressionDetector()))))))args.db_path, args.threshold)
        
        # Get model_id if model name provided
        model_id = None:
        if args.model:
            conn = get_db_connection()))))))args.db_path)
            query = "SELECT id FROM models WHERE name = ?"
            result = conn.execute()))))))query, ()))))))args.model,)).fetchone())))))))
            conn.close())))))))
            
            if result:
                model_id = result[0]
                ,,
        # Get hardware_id if hardware type provided
        hardware_id = None:
        if args.hardware:
            conn = get_db_connection()))))))args.db_path)
            query = "SELECT id FROM hardware_platforms WHERE type = ?"
            result = conn.execute()))))))query, ()))))))args.hardware,)).fetchone())))))))
            conn.close())))))))
            
            if result:
                hardware_id = result[0]
                ,,
                regressions = detector.detect_regressions()))))))model_id, hardware_id, args.metric)
        
        if regressions:
            print()))))))f"Detected {}}}}len()))))))regressions)} regression()))))))s):")
            for reg in regressions:
                print()))))))f"- {}}}}reg['model_name']}, on {}}}}reg['hardware_type']},: {}}}}reg['percent_change']:.2f}% {}}}}args.metric} {}}}}'increase' if args.metric == 'latency' else 'decrease'}")
                
            # Record regressions
            detector.record_regressions()))))))regressions):
        else:
            print()))))))"No regressions detected.")
            
    elif args.command == 'visualize':
        visualizer = TrendVisualizer()))))))args.db_path)
        output_file = visualizer.visualize_metric_trend()))))))
        args.model,
        args.hardware,
        args.metric,
        args.batch_size,
        output_file=args.output
        )
        
        if output_file:
            print()))))))f"Visualization saved to: {}}}}output_file}")
            
    elif args.command == 'dashboard':
        visualizer = TrendVisualizer()))))))args.db_path)
        output_file = visualizer.create_regression_dashboard()))))))
        args.output,
        args.days,
        args.limit
        )
        
        if output_file:
            print()))))))f"Dashboard saved to: {}}}}output_file}")
            
    elif args.command == 'notify':
        notifier = NotificationSystem()))))))args.db_path)
        notifier.process_notifications())))))))
        
    else:
        parser.print_help())))))))


if __name__ == "__main__":
    main())))))))