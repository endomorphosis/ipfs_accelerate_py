#\!/usr/bin/env python3
from hardware_test_templates.template_database import TemplateDatabase

def add_skill_template():
    db = TemplateDatabase()
    
    # Define a skill template
    template = '''#\!/usr/bin/env python3
"""
Skill implementation for {model_name} with hardware platform support
Generated by integrated_skillset_generator.py using templates
Template version: 1.0.1
"""

import os
import sys
import torch
import numpy as np
import logging
from transformers import AutoModel, AutoTokenizer, AutoConfig, AutoFeatureExtractor, AutoProcessor

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Hardware detection
HAS_CUDA = torch.cuda.is_available()
HAS_ROCM = (HAS_CUDA and hasattr(torch, '_C') and hasattr(torch._C, '_rocm_version')) or ('ROCM_HOME' in os.environ)
HAS_MPS = hasattr(torch, "mps") and hasattr(torch.mps, "is_available") and torch.mps.is_available()

class {class_name}:
    """Skill for {model_name} model with hardware platform support."""
    
    def __init__(self, model_id="{model_id}", device=None):
        """Initialize the skill."""
        self.model_id = model_id
        self.device = device or self.get_default_device()
        self.tokenizer = None
        self.processor = None
        self.model = None
        self.modality = "{modality}"
        
    def get_default_device(self):
        """Get the best available device."""
        # Check for CUDA
        if torch.cuda.is_available():
            return "cuda"
        
        # Check for MPS (Apple Silicon)
        if hasattr(torch, "mps") and hasattr(torch.mps, "is_available"):
            if torch.mps.is_available():
                return "mps"
        
        # Default to CPU
        return "cpu"
    
    def load_model(self):
        """Load the model and tokenizer based on modality."""
        if self.model is None:
            # Determine model modality
            modality = self.modality
            
            # Load appropriate tokenizer/processor and model based on modality
            if modality == "audio":
                from transformers import AutoFeatureExtractor, AutoModelForAudioClassification
                self.processor = AutoFeatureExtractor.from_pretrained(self.model_id)
                self.model = AutoModelForAudioClassification.from_pretrained(self.model_id)
            elif modality == "vision":
                from transformers import AutoImageProcessor, AutoModelForImageClassification
                self.processor = AutoImageProcessor.from_pretrained(self.model_id)
                self.model = AutoModelForImageClassification.from_pretrained(self.model_id)
            elif modality == "multimodal":
                from transformers import AutoProcessor, AutoModel
                self.processor = AutoProcessor.from_pretrained(self.model_id)
                self.model = AutoModel.from_pretrained(self.model_id)
            elif modality == "video":
                from transformers import AutoProcessor, AutoModelForVideoClassification
                self.processor = AutoProcessor.from_pretrained(self.model_id)
                self.model = AutoModelForVideoClassification.from_pretrained(self.model_id)
            else:
                # Default to text
                self.tokenizer = AutoTokenizer.from_pretrained(self.model_id)
                self.model = AutoModel.from_pretrained(self.model_id)
            
            # Move to device
            if self.device \!= "cpu":
                self.model = self.model.to(self.device)
                
            logger.info(f"Loaded {self.model_id} on {self.device}")
    
    def process(self, text_input):
        """Process the input text and return the output."""
        # Ensure model is loaded
        self.load_model()
        
        # Process based on modality
        if hasattr(self, 'tokenizer') and self.tokenizer is not None:
            # Text modality
            inputs = self.tokenizer(text_input, return_tensors="pt")
            
            # Move to device
            if self.device \!= "cpu":
                inputs = {{k: v.to(self.device) for k, v in inputs.items()}}
            
            # Run inference
            with torch.no_grad():
                outputs = self.model(**inputs)
            
            # Convert to numpy for consistent output
            if "last_hidden_state" in outputs:
                last_hidden_state = outputs.last_hidden_state.cpu().numpy()
                embedding = last_hidden_state.mean(axis=1)
            else:
                # Fall back to whatever is available
                embedding = next(iter(outputs.values())).cpu().numpy()
                
        elif hasattr(self, 'processor') and self.processor is not None:
            # Non-text modality - dummy implementation as this would need input of the right type
            logger.warning(f"Dummy implementation for {self.modality} modality - needs appropriate input type")
            embedding = np.random.rand(1, 768)  # Dummy embedding
        
        # Return formatted results
        return {{
            "model": self.model_id,
            "device": self.device,
            "embedding_shape": embedding.shape,
            "embedding": embedding.tolist(),
        }}

# Factory function to create skill instance
def create_skill(model_id="{model_id}", device=None):
    """Create a skill instance."""
    return {class_name}(model_id=model_id, device=device)
'''
    
    # Store the skill template
    db.store_template(
        model_id='skill_bert',
        template_content=template,
        model_name='bert',
        model_family='embedding',
        modality='text'
    )
    
    print("Skill template added successfully")

if __name__ == "__main__":
    add_skill_template()
