#!/usr/bin/env python
"""
Enhanced Hardware Benchmark Runner

This module provides an enhanced benchmark runner that uses the improved hardware
selection system to automatically select optimal hardware for different models and
benchmarking scenarios.
"""

import os
import sys
import argparse
import logging
import json
import time
import datetime
from pathlib import Path
from typing import Dict, List, Optional, Any, Union

# Set up logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger("enhanced_hardware_benchmark_runner")

# Add parent directory to path
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# Import hardware selector
from hardware_selector import HardwareSelector

# Import database integration if available
try:
    from benchmark_db_api import BenchmarkDBAPI
    db_api_available = True
except ImportError:
    logger.warning("BenchmarkDBAPI not available, database integration disabled")
    db_api_available = False


class EnhancedHardwareBenchmarkRunner:
    """
    Enhanced benchmark runner that uses hardware selection system to automatically
    select optimal hardware for different models and scenarios.
    """

    def __init__(self, 
                database_path: str = "./benchmark_db.duckdb",
                benchmark_results_path: str = "./benchmark_results",
                config_path: Optional[str] = None,
                use_database: bool = True):
        """
        Initialize the benchmark runner.
        
        Args:
            database_path (str): Path to the benchmark database.
            benchmark_results_path (str): Path to store benchmark results.
            config_path (Optional[str]): Path to configuration file.
            use_database (bool): Whether to use database integration.
        """
        self.benchmark_results_path = Path(benchmark_results_path)
        self.database_path = database_path
        self.config_path = config_path
        self.use_database = use_database and db_api_available
        
        # Create results directory
        os.makedirs(self.benchmark_results_path, exist_ok=True)
        
        # Initialize hardware selector
        self.selector = HardwareSelector(
            database_path=str(self.benchmark_results_path),
            config_path=config_path
        )
        
        # Initialize database API if available
        if self.use_database:
            try:
                self.db_api = BenchmarkDBAPI(db_path=database_path)
                logger.info(f"Database integration enabled, using {database_path}")
            except Exception as e:
                logger.error(f"Failed to initialize database API: {e}")
                self.use_database = False
    
    def detect_available_hardware(self) -> List[str]:
        """
        Detect available hardware platforms.
        
        Returns:
            List[str]: List of available hardware platforms.
        """
        available_hardware = ["cpu"]
        
        # Try to import hardware detection module
        try:
            from hardware_detection import detect_hardware
            hardware_info = detect_hardware()
            
            for hw_type, available in hardware_info.items():
                if available:
                    available_hardware.append(hw_type)
            
            logger.info(f"Detected hardware: {', '.join(available_hardware)}")
            return available_hardware
        except ImportError:
            logger.warning("Hardware detection module not available, using basic detection")
        
        # Basic detection fallback
        try:
            # Check for CUDA
            try:
                import torch
                if torch.cuda.is_available():
                    available_hardware.append("cuda")
                    logger.info(f"CUDA available: {torch.cuda.get_device_name(0)}")
                
                # Check for MPS (Apple Silicon)
                if hasattr(torch, 'backends') and hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
                    available_hardware.append("mps")
                    logger.info("MPS (Apple Silicon) available")
            except ImportError:
                logger.warning("PyTorch not available, CUDA and MPS detection disabled")
            
            # Check for ROCm
            if os.path.exists("/opt/rocm") or "ROCM_PATH" in os.environ:
                available_hardware.append("rocm")
                logger.info("ROCm appears to be available")
            
            # Check for OpenVINO
            try:
                import openvino
                available_hardware.append("openvino")
                logger.info("OpenVINO available")
            except ImportError:
                pass
            
            # Note: WebNN and WebGPU require browser environment, can't be detected here
            logger.info(f"Detected hardware: {', '.join(available_hardware)}")
            return available_hardware
        except Exception as e:
            logger.warning(f"Error during hardware detection: {e}")
            return ["cpu"]
    
    def _run_command(self, command: str) -> Dict[str, Any]:
        """
        Run a benchmark command and parse results.
        
        Args:
            command (str): Command to run.
            
        Returns:
            Dict[str, Any]: Benchmark results.
        """
        logger.info(f"Running command: {command}")
        
        # Create a timestamp for this run
        timestamp = datetime.datetime.now().isoformat()
        
        try:
            start_time = time.time()
            # Execute the command
            result = os.popen(command).read()
            end_time = time.time()
            
            # Try to parse JSON output
            try:
                benchmark_results = json.loads(result)
            except json.JSONDecodeError:
                # If not JSON, create a simple result
                benchmark_results = {
                    "output": result,
                    "execution_time_seconds": end_time - start_time
                }
            
            # Add timestamp
            benchmark_results["timestamp"] = timestamp
            
            return benchmark_results
        except Exception as e:
            logger.error(f"Error running command: {e}")
            return {
                "error": str(e),
                "timestamp": timestamp
            }
    
    def _save_results(self, results: Dict[str, Any], model_name: str, hardware: str, suffix: str = "") -> str:
        """
        Save benchmark results to file.
        
        Args:
            results (Dict[str, Any]): Benchmark results.
            model_name (str): Model name.
            hardware (str): Hardware platform.
            suffix (str): Optional suffix for filename.
            
        Returns:
            str: Path to saved results file.
        """
        # Create a timestamp string
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # Create filename
        filename = f"{model_name}_{hardware}{suffix}_{timestamp}.json"
        
        # Create output directory
        output_dir = self.benchmark_results_path / "raw_results"
        os.makedirs(output_dir, exist_ok=True)
        
        # Save results
        output_path = output_dir / filename
        with open(output_path, "w") as f:
            json.dump(results, f, indent=2)
        
        logger.info(f"Results saved to {output_path}")
        return str(output_path)
    
    def _store_in_database(self, results: Dict[str, Any], model_name: str, 
                          hardware: str, mode: str = "inference") -> bool:
        """
        Store benchmark results in database.
        
        Args:
            results (Dict[str, Any]): Benchmark results.
            model_name (str): Model name.
            hardware (str): Hardware platform.
            mode (str): Benchmark mode (inference or training).
            
        Returns:
            bool: True if successful, False otherwise.
        """
        if not self.use_database:
            return False
        
        try:
            # Extract performance metrics
            perf_metrics = results.get("performance_metrics", {})
            if not perf_metrics and "benchmarks" in results:
                # Try to find metrics in nested structure
                for family in results["benchmarks"]:
                    if model_name in results["benchmarks"][family]:
                        if hardware in results["benchmarks"][family][model_name]:
                            perf_metrics = results["benchmarks"][family][model_name][hardware].get("performance_summary", {})
                            break
            
            if not perf_metrics:
                logger.warning(f"No performance metrics found for {model_name} on {hardware}")
                return False
            
            # Prepare performance result
            batch_size = results.get("batch_size", 1)
            if not isinstance(batch_size, int):
                batch_size = 1
            
            precision = results.get("precision", "fp32")
            if not isinstance(precision, str):
                precision = "fp32"
            
            # Get throughput and latency from metrics
            throughput = 0.0
            latency_avg = 0.0
            memory_peak = 0.0
            
            if isinstance(perf_metrics, dict):
                throughput = perf_metrics.get("throughput", {}).get("mean", 0.0)
                latency_avg = perf_metrics.get("latency", {}).get("mean", 0.0)
                memory_peak = perf_metrics.get("memory_usage", {}).get("mean", 0.0)
            
            # Build performance result
            perf_result = {
                "model_name": model_name,
                "hardware_type": hardware,
                "batch_size": batch_size,
                "precision": precision,
                "throughput": float(throughput),
                "latency_avg": float(latency_avg),
                "memory_peak": float(memory_peak),
                "test_case": f"{mode}_benchmark",
                "metrics": results
            }
            
            # Store in database
            self.db_api.store_performance_result(perf_result)
            logger.info(f"Results stored in database for {model_name} on {hardware}")
            return True
        
        except Exception as e:
            logger.error(f"Error storing results in database: {e}")
            return False

    def run_benchmark(self, 
                     model_name: str,
                     model_family: Optional[str] = None,
                     hardware: Optional[str] = None,
                     batch_sizes: Optional[List[int]] = None,
                     mode: str = "inference",
                     cmd_template: Optional[str] = None,
                     store_db: bool = True) -> Dict[str, Any]:
        """
        Run a benchmark for a specific model and hardware.
        
        Args:
            model_name (str): Model name.
            model_family (Optional[str]): Model family. If None, will be determined from model name.
            hardware (Optional[str]): Hardware platform. If None, will be automatically selected.
            batch_sizes (Optional[List[int]]): Batch sizes to test. If None, defaults will be used.
            mode (str): Benchmark mode (inference or training).
            cmd_template (Optional[str]): Command template for benchmark.
            store_db (bool): Whether to store results in database.
            
        Returns:
            Dict[str, Any]: Benchmark results.
        """
        # Determine model family if not provided
        if model_family is None:
            model_family = self.selector._determine_model_family(model_name)
            logger.info(f"Determined model family: {model_family}")
        
        # Set default batch sizes based on model family
        if batch_sizes is None:
            if model_family == "text_generation":
                batch_sizes = [1, 4, 8]
            elif model_family == "embedding":
                batch_sizes = [1, 16, 64]
            elif model_family == "vision":
                batch_sizes = [1, 8, 32]
            elif model_family == "audio":
                batch_sizes = [1, 4, 16]
            elif model_family == "multimodal":
                batch_sizes = [1, 2, 4]
            else:
                batch_sizes = [1, 8, 32]
        
        # Detect available hardware if not specified
        available_hardware = self.detect_available_hardware() if hardware is None else [hardware]
        
        # Select hardware if not specified
        if hardware is None:
            # Get recommended hardware for each batch size
            recommendations = {}
            for batch_size in batch_sizes:
                result = self.selector.select_hardware(
                    model_family=model_family,
                    model_name=model_name,
                    batch_size=batch_size,
                    mode=mode,
                    available_hardware=available_hardware
                )
                
                primary_rec = result["primary_recommendation"]
                if primary_rec not in recommendations:
                    recommendations[primary_rec] = 0
                recommendations[primary_rec] += 1
            
            # Use most frequently recommended hardware
            hardware = max(recommendations.items(), key=lambda x: x[1])[0]
            logger.info(f"Selected hardware: {hardware}")
        
        # Set default command template if not provided
        if cmd_template is None:
            if mode == "inference":
                cmd_template = f"python -m test.run_model_benchmarks --model {model_name} --hardware {hardware} --batch-size {{batch_size}} --output-file {{output_file}}"
            else:  # training
                cmd_template = f"python -m test.run_training_benchmark --model {model_name} --hardware {hardware} --batch-size {{batch_size}} --output-file {{output_file}}"
        
        # Run benchmarks for each batch size
        results = {
            "model_name": model_name,
            "model_family": model_family,
            "hardware": hardware,
            "mode": mode,
            "timestamp": datetime.datetime.now().isoformat(),
            "batch_results": {}
        }
        
        for batch_size in batch_sizes:
            # Create output file name
            output_file = f"{model_name}_{hardware}_batch{batch_size}_{mode}.json"
            output_path = self.benchmark_results_path / "raw_results" / output_file
            
            # Format command
            cmd = cmd_template.format(
                batch_size=batch_size,
                output_file=str(output_path)
            )
            
            # Run benchmark
            logger.info(f"Running benchmark for {model_name} on {hardware} with batch size {batch_size}")
            result = self._run_command(cmd)
            
            # Store batch result
            results["batch_results"][f"batch_{batch_size}"] = result
            
            # Store in database if requested
            if store_db and self.use_database:
                # Add batch size to result for DB storage
                result["batch_size"] = batch_size
                self._store_in_database(result, model_name, hardware, mode)
        
        # Calculate aggregate metrics
        agg_metrics = self._calculate_aggregate_metrics(results["batch_results"])
        results["aggregate_metrics"] = agg_metrics
        
        # Save consolidated results
        results_file = self._save_results(
            results, 
            model_name, 
            hardware, 
            f"_{mode}_consolidated"
        )
        
        logger.info(f"Benchmark completed for {model_name} on {hardware}")
        return results
    
    def _calculate_aggregate_metrics(self, batch_results: Dict[str, Any]) -> Dict[str, Any]:
        """
        Calculate aggregate metrics from batch results.
        
        Args:
            batch_results (Dict[str, Any]): Batch results.
            
        Returns:
            Dict[str, Any]: Aggregate metrics.
        """
        throughputs = []
        latencies = []
        memory_usages = []
        
        for batch, result in batch_results.items():
            # Try to extract metrics
            metrics = result.get("performance_metrics", {})
            if not metrics and "benchmarks" in result:
                # Try to find in nested structure
                for family in result.get("benchmarks", {}):
                    for model in result["benchmarks"][family]:
                        for hw in result["benchmarks"][family][model]:
                            metrics = result["benchmarks"][family][model][hw].get("performance_summary", {})
                            break
            
            # Extract values
            if isinstance(metrics, dict):
                if "throughput" in metrics:
                    value = metrics["throughput"].get("mean", 0) if isinstance(metrics["throughput"], dict) else metrics["throughput"]
                    throughputs.append(float(value))
                
                if "latency" in metrics:
                    value = metrics["latency"].get("mean", 0) if isinstance(metrics["latency"], dict) else metrics["latency"]
                    latencies.append(float(value))
                
                if "memory_usage" in metrics:
                    value = metrics["memory_usage"].get("mean", 0) if isinstance(metrics["memory_usage"], dict) else metrics["memory_usage"]
                    memory_usages.append(float(value))
        
        # Calculate aggregates
        aggregate_metrics = {}
        
        if throughputs:
            aggregate_metrics["throughput_avg"] = sum(throughputs) / len(throughputs)
            aggregate_metrics["throughput_max"] = max(throughputs)
            aggregate_metrics["throughput_min"] = min(throughputs)
        
        if latencies:
            aggregate_metrics["latency_avg"] = sum(latencies) / len(latencies)
            aggregate_metrics["latency_max"] = max(latencies)
            aggregate_metrics["latency_min"] = min(latencies)
        
        if memory_usages:
            aggregate_metrics["memory_usage_avg"] = sum(memory_usages) / len(memory_usages)
            aggregate_metrics["memory_usage_max"] = max(memory_usages)
            aggregate_metrics["memory_usage_min"] = min(memory_usages)
        
        return aggregate_metrics
    
    def run_distributed_benchmark(self,
                                 model_name: str,
                                 model_family: Optional[str] = None,
                                 gpu_count: int = 4,
                                 batch_sizes: Optional[List[int]] = None,
                                 max_memory_gb: Optional[int] = None,
                                 cmd_template: Optional[str] = None,
                                 store_db: bool = True) -> Dict[str, Any]:
        """
        Run a distributed training benchmark.
        
        Args:
            model_name (str): Model name.
            model_family (Optional[str]): Model family. If None, will be determined from model name.
            gpu_count (int): Number of GPUs to use.
            batch_sizes (Optional[List[int]]): Batch sizes to test. If None, defaults will be used.
            max_memory_gb (Optional[int]): Maximum GPU memory in GB.
            cmd_template (Optional[str]): Command template for benchmark.
            store_db (bool): Whether to store results in database.
            
        Returns:
            Dict[str, Any]: Benchmark results.
        """
        # Determine model family if not provided
        if model_family is None:
            model_family = self.selector._determine_model_family(model_name)
            logger.info(f"Determined model family: {model_family}")
        
        # Set default batch sizes based on model family - smaller for distributed
        if batch_sizes is None:
            if model_family == "text_generation":
                batch_sizes = [1, 2, 4]
            elif model_family == "embedding":
                batch_sizes = [4, 8, 16]
            elif model_family == "vision":
                batch_sizes = [2, 4, 8]
            elif model_family == "audio":
                batch_sizes = [1, 2, 4]
            elif model_family == "multimodal":
                batch_sizes = [1, 2]
            else:
                batch_sizes = [1, 2, 4]
        
        # Get distributed training configuration
        training_config = self.selector.get_distributed_training_config(
            model_family=model_family,
            model_name=model_name,
            gpu_count=gpu_count,
            batch_size=batch_sizes[0],  # Use first batch size for config
            max_memory_gb=max_memory_gb
        )
        
        strategy = training_config["distributed_strategy"]
        logger.info(f"Using distributed strategy: {strategy}")
        
        # Select hardware - for distributed, we currently only support CUDA and ROCm
        available_hardware = ["cuda", "rocm"]
        result = self.selector.select_hardware_for_task(
            model_family=model_family,
            model_name=model_name,
            task_type="training",
            batch_size=batch_sizes[0],
            available_hardware=available_hardware,
            distributed=True,
            gpu_count=gpu_count
        )
        
        hardware = result["primary_recommendation"]
        logger.info(f"Selected hardware for distributed training: {hardware}")
        
        # Apply memory optimizations if specified
        optimizations = training_config.get("memory_optimizations", [])
        optimization_flags = ""
        if optimizations:
            logger.info(f"Applying memory optimizations: {', '.join(optimizations)}")
            
            if "Gradient checkpointing" in optimizations:
                optimization_flags += " --gradient-checkpointing"
            
            if "Gradient accumulation" in optimizations:
                grad_steps = training_config.get("gradient_accumulation_steps", 1)
                optimization_flags += f" --gradient-accumulation-steps {grad_steps}"
            
            if "8-bit Optimizer" in optimizations:
                optimization_flags += " --use-8bit-optimizer"
            
            if "DeepSpeed ZeRO Stage 2" in optimizations or "DeepSpeed ZeRO Stage 3" in optimizations:
                zero_stage = 3 if "DeepSpeed ZeRO Stage 3" in optimizations else 2
                optimization_flags += f" --deepspeed --zero-stage {zero_stage}"
            
            if "FSDP Full Sharding" in optimizations:
                optimization_flags += " --fsdp --sharding-strategy full_shard"
                
                if "FSDP CPU Offloading" in optimizations:
                    optimization_flags += " --fsdp-cpu-offload"
        
        # Set default command template if not provided
        if cmd_template is None:
            cmd_template = f"python -m test.run_training_benchmark --model {model_name} --hardware {hardware} " + \
                           f"--distributed --gpu-count {gpu_count} --batch-size {{batch_size}} " + \
                           f"--strategy {strategy}{optimization_flags} " + \
                           f"--output-file {{output_file}}"
        
        # Run benchmarks for each batch size
        results = {
            "model_name": model_name,
            "model_family": model_family,
            "hardware": hardware,
            "mode": "distributed_training",
            "gpu_count": gpu_count,
            "strategy": strategy,
            "timestamp": datetime.datetime.now().isoformat(),
            "batch_results": {},
            "training_config": training_config
        }
        
        for batch_size in batch_sizes:
            # Create output file name
            output_file = f"{model_name}_{hardware}_dist{gpu_count}_batch{batch_size}.json"
            output_path = self.benchmark_results_path / "raw_results" / output_file
            
            # Format command
            cmd = cmd_template.format(
                batch_size=batch_size,
                output_file=str(output_path)
            )
            
            # Run benchmark
            logger.info(f"Running distributed benchmark for {model_name} on {hardware} with {gpu_count} GPUs, batch size {batch_size}")
            result = self._run_command(cmd)
            
            # Store batch result
            results["batch_results"][f"batch_{batch_size}"] = result
            
            # Store in database if requested
            if store_db and self.use_database:
                # Add batch size to result for DB storage
                result["batch_size"] = batch_size
                result["gpu_count"] = gpu_count
                result["distributed"] = True
                result["distributed_strategy"] = strategy
                self._store_in_database(result, model_name, f"distributed_{hardware}_{strategy.lower()}", "training")
        
        # Calculate aggregate metrics
        agg_metrics = self._calculate_aggregate_metrics(results["batch_results"])
        results["aggregate_metrics"] = agg_metrics
        
        # Save consolidated results
        results_file = self._save_results(
            results, 
            model_name, 
            f"distributed_{hardware}_{strategy.lower()}", 
            f"_{gpu_count}gpu"
        )
        
        logger.info(f"Distributed benchmark completed for {model_name} on {hardware} with {gpu_count} GPUs")
        return results
    
    def run_multiple_models(self,
                          model_families: Optional[List[str]] = None,
                          specific_models: Optional[List[str]] = None,
                          mode: str = "inference",
                          include_distributed: bool = False,
                          max_gpus: int = 4,
                          store_db: bool = True) -> Dict[str, Any]:
        """
        Run benchmarks for multiple models.
        
        Args:
            model_families (Optional[List[str]]): Model families to benchmark.
            specific_models (Optional[List[str]]): Specific models to benchmark.
            mode (str): Benchmark mode (inference, training, or both).
            include_distributed (bool): Include distributed training benchmarks.
            max_gpus (int): Maximum number of GPUs for distributed training.
            store_db (bool): Whether to store results in database.
            
        Returns:
            Dict[str, Any]: Aggregated benchmark results.
        """
        # Set default model families if not provided
        if model_families is None and specific_models is None:
            model_families = ["embedding", "text_generation", "vision", "audio", "multimodal"]
        
        # Define models to benchmark
        models_to_benchmark = []
        
        # Add specific models
        if specific_models:
            for model in specific_models:
                family = self.selector._determine_model_family(model)
                models_to_benchmark.append((model, family))
        
        # Add representative models for each family
        if model_families:
            representative_models = {
                "embedding": ["bert-base-uncased", "distilbert-base-uncased"],
                "text_generation": ["gpt2", "t5-small"],
                "vision": ["vit-base-patch16-224", "resnet50"],
                "audio": ["whisper-tiny", "wav2vec2-base"],
                "multimodal": ["clip-vit-base-patch32"]
            }
            
            for family in model_families:
                if family in representative_models:
                    for model in representative_models[family]:
                        # Check if already added as specific model
                        if specific_models is None or model not in specific_models:
                            models_to_benchmark.append((model, family))
        
        # Run benchmarks
        results = {
            "timestamp": datetime.datetime.now().isoformat(),
            "mode": mode,
            "model_results": {},
            "summary": {
                "total_models": 0,
                "successful_models": 0,
                "distributed_models": 0
            }
        }
        
        # Process modes
        modes_to_run = []
        if mode == "both":
            modes_to_run = ["inference", "training"]
        else:
            modes_to_run = [mode]
        
        # Run benchmarks for each model
        for model_name, model_family in models_to_benchmark:
            results["model_results"][model_name] = {}
            results["summary"]["total_models"] += 1
            
            # Run benchmarks for each mode
            for run_mode in modes_to_run:
                try:
                    # Regular benchmarks
                    benchmark_result = self.run_benchmark(
                        model_name=model_name,
                        model_family=model_family,
                        mode=run_mode,
                        store_db=store_db
                    )
                    
                    # Store results
                    results["model_results"][model_name][run_mode] = {
                        "status": "completed",
                        "hardware": benchmark_result["hardware"],
                        "aggregate_metrics": benchmark_result.get("aggregate_metrics", {})
                    }
                    
                    results["summary"]["successful_models"] += 1
                    
                    # Distributed training benchmarks
                    if include_distributed and run_mode == "training" and model_family in ["text_generation", "embedding", "vision"]:
                        try:
                            distributed_result = self.run_distributed_benchmark(
                                model_name=model_name,
                                model_family=model_family,
                                gpu_count=max_gpus,
                                store_db=store_db
                            )
                            
                            # Store results
                            results["model_results"][model_name]["distributed_training"] = {
                                "status": "completed",
                                "hardware": distributed_result["hardware"],
                                "gpu_count": distributed_result["gpu_count"],
                                "strategy": distributed_result["strategy"],
                                "aggregate_metrics": distributed_result.get("aggregate_metrics", {})
                            }
                            
                            results["summary"]["distributed_models"] += 1
                        except Exception as e:
                            logger.error(f"Error running distributed benchmark for {model_name}: {e}")
                            results["model_results"][model_name]["distributed_training"] = {
                                "status": "error",
                                "error": str(e)
                            }
                except Exception as e:
                    logger.error(f"Error running benchmark for {model_name} in {run_mode} mode: {e}")
                    results["model_results"][model_name][run_mode] = {
                        "status": "error",
                        "error": str(e)
                    }
        
        # Save consolidated results
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        output_path = self.benchmark_results_path / f"consolidated_benchmarks_{timestamp}.json"
        with open(output_path, "w") as f:
            json.dump(results, f, indent=2)
        
        logger.info(f"All benchmarks completed, results saved to {output_path}")
        return results
    
    def create_performance_report(self, results: Dict[str, Any]) -> str:
        """
        Create a performance report from benchmark results.
        
        Args:
            results (Dict[str, Any]): Benchmark results.
            
        Returns:
            str: Markdown performance report.
        """
        # Generate report header
        report = f"# Hardware Performance Benchmark Report\n\n"
        report += f"Generated: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n"
        
        # Add summary
        if "summary" in results:
            report += f"## Summary\n\n"
            report += f"- Total models benchmarked: {results['summary']['total_models']}\n"
            report += f"- Successful models: {results['summary']['successful_models']}\n"
            if results['summary'].get('distributed_models', 0) > 0:
                report += f"- Distributed training models: {results['summary']['distributed_models']}\n"
            report += f"\n"
        
        # Add model results
        report += f"## Model Results\n\n"
        
        for model_name, model_results in results.get("model_results", {}).items():
            report += f"### {model_name}\n\n"
            
            # Add inference results
            if "inference" in model_results:
                inference = model_results["inference"]
                if inference["status"] == "completed":
                    report += f"#### Inference (Hardware: {inference['hardware']})\n\n"
                    report += f"| Metric | Value |\n"
                    report += f"|--------|-------|\n"
                    
                    for metric, value in inference.get("aggregate_metrics", {}).items():
                        report += f"| {metric} | {value:.4f} |\n"
                    
                    report += f"\n"
                else:
                    report += f"#### Inference\n\n"
                    report += f"Status: Error - {inference.get('error', 'Unknown error')}\n\n"
            
            # Add training results
            if "training" in model_results:
                training = model_results["training"]
                if training["status"] == "completed":
                    report += f"#### Training (Hardware: {training['hardware']})\n\n"
                    report += f"| Metric | Value |\n"
                    report += f"|--------|-------|\n"
                    
                    for metric, value in training.get("aggregate_metrics", {}).items():
                        report += f"| {metric} | {value:.4f} |\n"
                    
                    report += f"\n"
                else:
                    report += f"#### Training\n\n"
                    report += f"Status: Error - {training.get('error', 'Unknown error')}\n\n"
            
            # Add distributed training results
            if "distributed_training" in model_results:
                dist = model_results["distributed_training"]
                if dist["status"] == "completed":
                    report += f"#### Distributed Training\n\n"
                    report += f"- Hardware: {dist['hardware']}\n"
                    report += f"- GPU Count: {dist['gpu_count']}\n"
                    report += f"- Strategy: {dist['strategy']}\n\n"
                    report += f"| Metric | Value |\n"
                    report += f"|--------|-------|\n"
                    
                    for metric, value in dist.get("aggregate_metrics", {}).items():
                        report += f"| {metric} | {value:.4f} |\n"
                    
                    report += f"\n"
                else:
                    report += f"#### Distributed Training\n\n"
                    report += f"Status: Error - {dist.get('error', 'Unknown error')}\n\n"
            
            report += f"\n"
        
        # Save report
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        output_path = self.benchmark_results_path / f"performance_report_{timestamp}.md"
        with open(output_path, "w") as f:
            f.write(report)
        
        logger.info(f"Performance report saved to {output_path}")
        return str(output_path)


def main():
    """Main function for command-line usage."""
    parser = argparse.ArgumentParser(description="Enhanced Hardware Benchmark Runner")
    
    # Model and hardware selection
    parser.add_argument("--model", type=str, help="Model name to benchmark")
    parser.add_argument("--model-family", type=str, choices=["embedding", "text_generation", "vision", "audio", "multimodal"],
                      help="Model family")
    parser.add_argument("--hardware", type=str, help="Hardware platform (cpu, cuda, rocm, mps, openvino, webnn, webgpu)")
    parser.add_argument("--batch-sizes", type=str, default="1,8,32",
                      help="Comma-separated list of batch sizes")
    
    # Running multiple models
    parser.add_argument("--multiple-models", action="store_true",
                      help="Run benchmarks for multiple models")
    parser.add_argument("--specific-models", type=str,
                      help="Comma-separated list of specific models to benchmark")
    parser.add_argument("--model-families", type=str,
                      help="Comma-separated list of model families to benchmark")
    
    # Distributed training
    parser.add_argument("--distributed", action="store_true",
                      help="Run distributed training benchmark")
    parser.add_argument("--gpu-count", type=int, default=4,
                      help="Number of GPUs for distributed training")
    parser.add_argument("--max-memory-gb", type=int,
                      help="Maximum GPU memory in GB")
    parser.add_argument("--include-distributed", action="store_true",
                      help="Include distributed training in multiple model benchmarks")
    parser.add_argument("--max-gpus", type=int, default=4,
                      help="Maximum number of GPUs for distributed training in multiple model benchmarks")
    
    # Mode selection
    parser.add_argument("--mode", type=str, default="inference",
                      choices=["inference", "training", "both"],
                      help="Benchmark mode")
    
    # Storage and output
    parser.add_argument("--database-path", type=str, default="./benchmark_db.duckdb",
                      help="Path to benchmark database")
    parser.add_argument("--results-path", type=str, default="./benchmark_results",
                      help="Path to benchmark results")
    parser.add_argument("--config-path", type=str,
                      help="Path to configuration file")
    parser.add_argument("--no-database", action="store_true",
                      help="Disable database integration")
    parser.add_argument("--create-report", action="store_true",
                      help="Create performance report")
    
    # Other options
    parser.add_argument("--cmd-template", type=str,
                      help="Command template for benchmark")
    parser.add_argument("--detect-hardware", action="store_true",
                      help="Detect available hardware platforms and exit")
    parser.add_argument("--debug", action="store_true",
                      help="Enable debug logging")
    
    args = parser.parse_args()
    
    # Configure logging
    if args.debug:
        logging.getLogger().setLevel(logging.DEBUG)
        logging.getLogger("enhanced_hardware_benchmark_runner").setLevel(logging.DEBUG)
    
    # Initialize benchmark runner
    runner = EnhancedHardwareBenchmarkRunner(
        database_path=args.database_path,
        benchmark_results_path=args.results_path,
        config_path=args.config_path,
        use_database=not args.no_database
    )
    
    # Detect hardware and exit if requested
    if args.detect_hardware:
        available_hardware = runner.detect_available_hardware()
        print(f"Detected hardware: {', '.join(available_hardware)}")
        return
    
    # Parse batch sizes
    batch_sizes = [int(b) for b in args.batch_sizes.split(",")]
    
    # Run benchmarks
    if args.multiple_models:
        # Parse model families and specific models
        model_families = None
        if args.model_families:
            model_families = args.model_families.split(",")
        
        specific_models = None
        if args.specific_models:
            specific_models = args.specific_models.split(",")
        
        # Run multiple models
        results = runner.run_multiple_models(
            model_families=model_families,
            specific_models=specific_models,
            mode=args.mode,
            include_distributed=args.include_distributed,
            max_gpus=args.max_gpus,
            store_db=not args.no_database
        )
        
        # Create report if requested
        if args.create_report:
            report_path = runner.create_performance_report(results)
            print(f"Performance report saved to: {report_path}")
        
    elif args.distributed:
        # Run distributed training benchmark
        if not args.model:
            parser.error("--model is required for distributed training benchmark")
        
        runner.run_distributed_benchmark(
            model_name=args.model,
            model_family=args.model_family,
            gpu_count=args.gpu_count,
            batch_sizes=batch_sizes,
            max_memory_gb=args.max_memory_gb,
            cmd_template=args.cmd_template,
            store_db=not args.no_database
        )
        
    else:
        # Run single benchmark
        if not args.model:
            parser.error("--model is required for single benchmark")
        
        runner.run_benchmark(
            model_name=args.model,
            model_family=args.model_family,
            hardware=args.hardware,
            batch_sizes=batch_sizes,
            mode=args.mode,
            cmd_template=args.cmd_template,
            store_db=not args.no_database
        )


if __name__ == "__main__":
    main()