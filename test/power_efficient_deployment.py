#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Power-Efficient Model Deployment Pipeline for Mobile/Edge Devices

This module provides a comprehensive framework for power-efficient deployment of
machine learning models on mobile and edge devices. It includes:

1. Intelligent hardware selection based on power constraints
2. Dynamic power-aware model loading and optimization
3. Runtime power and thermal management
4. Adaptive inference scheduling based on device state
5. Lifecycle management for deployed models
6. Power efficiency monitoring and reporting

The module is designed to work seamlessly with the thermal monitoring system
and Qualcomm quantization support, providing an end-to-end solution for
power-efficient model deployment.

Date: April 2025
"""

import os
import sys
import json
import time
import logging
import threading
import traceback
from typing import Dict, List, Any, Optional, Union, Tuple, Callable
from enum import Enum, auto
from pathlib import Path

# Set up logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Add parent directory to path
sys.path.append(str(Path(__file__).resolve().parent))

# Import local modules
try:
    # Import thermal monitoring components
    from mobile_thermal_monitoring import (
        MobileThermalMonitor, ThermalEventType, CoolingPolicy
    )
    HAS_THERMAL_MONITORING = True
except ImportError:
    logger.warning("Warning: mobile_thermal_monitoring module could not be imported. Thermal management will be disabled.")
    HAS_THERMAL_MONITORING = False

try:
    # Import Qualcomm quantization support
    from qualcomm_quantization_support import QualcommQuantization
    HAS_QUALCOMM_QUANTIZATION = True
except ImportError:
    logger.warning("Warning: qualcomm_quantization_support module could not be imported. Qualcomm-specific optimizations will be disabled.")
    HAS_QUALCOMM_QUANTIZATION = False

try:
    # Import database components
    from benchmark_db_api import BenchmarkDBAPI, get_db_connection
    HAS_DB_API = True
except ImportError:
    logger.warning("Warning: benchmark_db_api could not be imported. Database functionality will be limited.")
    HAS_DB_API = False

try:
    # Import hardware detection components
    from centralized_hardware_detection import hardware_detection
    HAS_HARDWARE_DETECTION = True
except ImportError:
    logger.warning("Warning: hardware_detection module could not be imported. Hardware detection will be limited.")
    HAS_HARDWARE_DETECTION = False

# Define power profiles
class PowerProfile(Enum):
    """Power consumption profiles for different deployment scenarios."""
    MAXIMUM_PERFORMANCE = auto()  # Prioritize performance, no power constraints
    BALANCED = auto()             # Balance performance and power consumption
    POWER_SAVER = auto()          # Prioritize power efficiency over performance
    ULTRA_EFFICIENT = auto()      # Extremely conservative power usage
    THERMAL_AWARE = auto()        # Focus on thermal management
    CUSTOM = auto()               # Custom profile with user-defined parameters

# Define deployment targets
class DeploymentTarget(Enum):
    """Target environments for model deployment."""
    ANDROID = auto()       # Android devices
    IOS = auto()           # iOS devices
    EMBEDDED = auto()      # General embedded systems
    BROWSER = auto()       # Web browser (WebNN/WebGPU)
    QUALCOMM = auto()      # Qualcomm-specific optimizations
    DESKTOP = auto()       # Desktop applications
    CUSTOM = auto()        # Custom deployment target

class PowerEfficientDeployment:
    """
    Main class for power-efficient model deployment.
    
    This class provides comprehensive functionality for deploying and managing
    machine learning models on power-constrained devices. It integrates with
    the thermal monitoring system and Qualcomm quantization support to provide
    an end-to-end solution for power-efficient model deployment.
    """
    
    def __init__(self, 
                 db_path: Optional[str] = None,
                 power_profile: PowerProfile = PowerProfile.BALANCED,
                 deployment_target: DeploymentTarget = DeploymentTarget.ANDROID):
        """
        Initialize power-efficient deployment.
        
        Args:
            db_path: Optional path to benchmark database
            power_profile: Power consumption profile
            deployment_target: Target environment for deployment
        """
        self.db_path = db_path
        self.power_profile = power_profile
        self.deployment_target = deployment_target
        
        # Initialize component modules
        self.thermal_monitor = None
        self.qualcomm_quantization = None
        self.db_api = None
        
        # Initialize configurations
        self.config = self._get_default_config()
        
        # Initialize internal state
        self.deployed_models = {}
        self.active_models = set()
        self.model_stats = {}
        self.monitoring_active = False
        self.monitoring_thread = None
        self.last_device_state = {}
        
        # Initialize components
        self._init_components()
        
        logger.info(f"Initialized power-efficient deployment with {power_profile.name} profile for {deployment_target.name}")
    
    def _init_components(self):
        """Initialize component modules."""
        # Initialize thermal monitoring
        if HAS_THERMAL_MONITORING:
            device_type = self._get_device_type()
            self.thermal_monitor = MobileThermalMonitor(device_type, db_path=self.db_path)
            logger.info(f"Initialized thermal monitoring for {device_type}")
        
        # Initialize Qualcomm quantization
        if HAS_QUALCOMM_QUANTIZATION and self.deployment_target in [DeploymentTarget.QUALCOMM, DeploymentTarget.ANDROID]:
            self.qualcomm_quantization = QualcommQuantization(db_path=self.db_path)
            logger.info(f"Initialized Qualcomm quantization (available: {self.qualcomm_quantization.is_available()})")
        
        # Initialize database API
        if HAS_DB_API and self.db_path:
            self.db_api = BenchmarkDBAPI(self.db_path)
            logger.info(f"Initialized database API with path: {self.db_path}")
    
    def _get_device_type(self) -> str:
        """Get device type based on deployment target."""
        if self.deployment_target == DeploymentTarget.ANDROID:
            return "android"
        elif self.deployment_target == DeploymentTarget.IOS:
            return "ios"
        elif self.deployment_target == DeploymentTarget.QUALCOMM:
            return "android"  # Qualcomm primarily used in Android
        elif self.deployment_target == DeploymentTarget.EMBEDDED:
            return "embedded"
        else:
            return "unknown"
    
    def _get_default_config(self) -> Dict[str, Any]:
        """Get default configuration based on power profile and deployment target."""
        # Base configuration for all profiles
        config = {
            "quantization": {
                "enabled": True,
                "preferred_method": "dynamic",
                "fallback_method": "weight_only"
            },
            "hardware_acceleration": {
                "enabled": True,
                "prefer_dedicated_accelerator": True
            },
            "memory_optimization": {
                "model_caching": True,
                "memory_map_models": True,
                "unload_unused_models": True,
                "idle_timeout_seconds": 300  # 5 minutes
            },
            "thermal_management": {
                "enabled": True,
                "proactive_throttling": False,
                "temperature_check_interval_seconds": 5
            },
            "inference_optimization": {
                "batch_inference_when_possible": True,
                "optimal_batch_size": 1,
                "use_fp16_where_available": True
            },
            "power_management": {
                "dynamic_frequency_scaling": True,
                "sleep_between_inferences": False,
                "sleep_duration_ms": 0
            },
            "monitoring": {
                "collect_metrics": True,
                "metrics_interval_seconds": 10,
                "log_to_database": True
            }
        }
        
        # Profile-specific configurations
        if self.power_profile == PowerProfile.MAXIMUM_PERFORMANCE:
            config["quantization"]["preferred_method"] = "weight_only"
            config["thermal_management"]["proactive_throttling"] = False
            config["inference_optimization"]["optimal_batch_size"] = 8
            config["power_management"]["dynamic_frequency_scaling"] = False
            config["power_management"]["sleep_between_inferences"] = False
        
        elif self.power_profile == PowerProfile.POWER_SAVER:
            config["quantization"]["preferred_method"] = "int8"
            config["thermal_management"]["proactive_throttling"] = True
            config["inference_optimization"]["optimal_batch_size"] = 4
            config["inference_optimization"]["batch_inference_when_possible"] = True
            config["power_management"]["dynamic_frequency_scaling"] = True
            config["power_management"]["sleep_between_inferences"] = True
            config["power_management"]["sleep_duration_ms"] = 10
            config["memory_optimization"]["idle_timeout_seconds"] = 60  # 1 minute
        
        elif self.power_profile == PowerProfile.ULTRA_EFFICIENT:
            config["quantization"]["preferred_method"] = "int8"
            config["thermal_management"]["proactive_throttling"] = True
            config["inference_optimization"]["optimal_batch_size"] = 8
            config["inference_optimization"]["batch_inference_when_possible"] = True
            config["power_management"]["dynamic_frequency_scaling"] = True
            config["power_management"]["sleep_between_inferences"] = True
            config["power_management"]["sleep_duration_ms"] = 20
            config["memory_optimization"]["idle_timeout_seconds"] = 30  # 30 seconds
        
        elif self.power_profile == PowerProfile.THERMAL_AWARE:
            config["quantization"]["preferred_method"] = "int8"
            config["thermal_management"]["proactive_throttling"] = True
            config["thermal_management"]["temperature_check_interval_seconds"] = 2
            config["inference_optimization"]["optimal_batch_size"] = 4
            config["power_management"]["dynamic_frequency_scaling"] = True
        
        # Target-specific configurations
        if self.deployment_target == DeploymentTarget.QUALCOMM:
            config["quantization"]["preferred_method"] = "int8"
            config["hardware_acceleration"]["prefer_dedicated_accelerator"] = True
            if HAS_QUALCOMM_QUANTIZATION:
                # Check if int4 is available on this device
                if self.qualcomm_quantization and self.qualcomm_quantization.is_available():
                    supported_methods = self.qualcomm_quantization.get_supported_methods()
                    if supported_methods.get("int4", False):
                        config["quantization"]["preferred_method"] = "int4"
        
        elif self.deployment_target == DeploymentTarget.BROWSER:
            config["quantization"]["preferred_method"] = "int8"
            config["memory_optimization"]["memory_map_models"] = False
            config["inference_optimization"]["optimal_batch_size"] = 1
        
        elif self.deployment_target == DeploymentTarget.IOS:
            # iOS-specific optimizations
            config["hardware_acceleration"]["prefer_dedicated_accelerator"] = True
            config["inference_optimization"]["use_fp16_where_available"] = True
        
        return config
    
    def update_config(self, config_updates: Dict[str, Any]) -> Dict[str, Any]:
        """
        Update configuration with user-provided values.
        
        Args:
            config_updates: Dictionary with configuration updates
            
        Returns:
            Updated configuration
        """
        # Helper function to recursively update nested dictionaries
        def update_nested_dict(d, u):
            for k, v in u.items():
                if isinstance(v, dict) and k in d and isinstance(d[k], dict):
                    d[k] = update_nested_dict(d[k], v)
                else:
                    d[k] = v
            return d
        
        # Update configuration
        self.config = update_nested_dict(self.config, config_updates)
        logger.info("Updated deployment configuration")
        
        # If we're changing to custom profile, reflect that
        if config_updates:
            self.power_profile = PowerProfile.CUSTOM
        
        return self.config
    
    def prepare_model_for_deployment(self, 
                                    model_path: str, 
                                    output_path: Optional[str] = None,
                                    model_type: Optional[str] = None,
                                    quantization_method: Optional[str] = None,
                                    **kwargs) -> Dict[str, Any]:
        """
        Prepare a model for power-efficient deployment.
        
        This method applies appropriate quantization and optimization techniques
        to the model based on the current power profile and deployment target.
        
        Args:
            model_path: Path to the input model
            output_path: Path for the optimized model (if None, generated automatically)
            model_type: Type of model (text, vision, audio, llm)
            quantization_method: Specific quantization method to use (overrides profile default)
            **kwargs: Additional optimization parameters
            
        Returns:
            Dictionary with deployment information
        """
        start_time = time.time()
        
        # Generate output path if not provided
        if output_path is None:
            model_basename = os.path.basename(model_path)
            profile_name = self.power_profile.name.lower()
            output_path = f"{os.path.splitext(model_path)[0]}_{profile_name}_optimized{os.path.splitext(model_path)[1]}"
        
        # Infer model type if not provided
        if model_type is None:
            model_type = self._infer_model_type(model_path)
            logger.info(f"Inferred model type: {model_type}")
        
        # Determine appropriate quantization method
        method = quantization_method or self.config["quantization"]["preferred_method"]
        
        deployment_info = {
            "input_model_path": model_path,
            "output_model_path": output_path,
            "model_type": model_type,
            "deployment_target": self.deployment_target.name,
            "power_profile": self.power_profile.name,
            "optimizations_applied": [],
            "quantization_method": method,
            "preparation_time_seconds": 0,
            "status": "preparing"
        }
        
        try:
            # Apply quantization if enabled and available
            if self.config["quantization"]["enabled"]:
                if HAS_QUALCOMM_QUANTIZATION and self.qualcomm_quantization and self.qualcomm_quantization.is_available():
                    # Use Qualcomm quantization
                    logger.info(f"Applying Qualcomm quantization method '{method}' to model")
                    quant_result = self.qualcomm_quantization.quantize_model(
                        model_path=model_path,
                        output_path=output_path,
                        method=method,
                        model_type=model_type,
                        **kwargs
                    )
                    
                    if "error" in quant_result:
                        # Try fallback method if available
                        fallback_method = self.config["quantization"]["fallback_method"]
                        logger.warning(f"Quantization with method '{method}' failed. Trying fallback method '{fallback_method}'")
                        
                        quant_result = self.qualcomm_quantization.quantize_model(
                            model_path=model_path,
                            output_path=output_path,
                            method=fallback_method,
                            model_type=model_type,
                            **kwargs
                        )
                        
                        if "error" in quant_result:
                            logger.error(f"Fallback quantization also failed: {quant_result['error']}")
                            deployment_info["status"] = "failed"
                            deployment_info["error"] = f"Quantization failed: {quant_result['error']}"
                            return deployment_info
                        
                        # Update method to fallback
                        method = fallback_method
                        deployment_info["quantization_method"] = method
                    
                    # Extract quantization results
                    deployment_info["size_reduction_ratio"] = quant_result.get("size_reduction_ratio", 1.0)
                    deployment_info["original_size_bytes"] = quant_result.get("original_size", 0)
                    deployment_info["optimized_size_bytes"] = quant_result.get("quantized_size", 0)
                    deployment_info["quantization_details"] = quant_result
                    
                    # Store power metrics
                    if "power_efficiency_metrics" in quant_result:
                        deployment_info["power_efficiency_metrics"] = quant_result["power_efficiency_metrics"]
                    
                    deployment_info["optimizations_applied"].append(f"quantization_{method}")
                else:
                    # Quantization not available or failed
                    logger.warning("Qualcomm quantization not available, skipping quantization")
                    
                    # Simply copy the model for now
                    import shutil
                    shutil.copy2(model_path, output_path)
                    logger.info(f"Copied model from {model_path} to {output_path}")
            else:
                # Quantization disabled, simply copy the model
                import shutil
                shutil.copy2(model_path, output_path)
                logger.info(f"Quantization disabled. Copied model from {model_path} to {output_path}")
            
            # Apply additional optimizations based on deployment target
            self._apply_target_specific_optimizations(output_path, model_type, deployment_info)
            
            # Calculate preparation time
            preparation_time = time.time() - start_time
            deployment_info["preparation_time_seconds"] = preparation_time
            deployment_info["status"] = "ready"
            
            logger.info(f"Model preparation completed in {preparation_time:.2f} seconds")
            logger.info(f"Optimized model saved to {output_path}")
            
            # Store deployment info
            self.deployed_models[output_path] = deployment_info
            
            return deployment_info
            
        except Exception as e:
            error_msg = f"Error preparing model for deployment: {str(e)}"
            logger.error(error_msg)
            logger.error(traceback.format_exc())
            
            deployment_info["status"] = "failed"
            deployment_info["error"] = error_msg
            
            return deployment_info
    
    def _infer_model_type(self, model_path: str) -> str:
        """Infer model type from model path or contents."""
        model_name = os.path.basename(model_path).lower()
        
        # Check model path for indicators
        if any(x in model_name for x in ["vit", "clip", "vision", "image", "resnet", "detr", "vgg"]):
            return "vision"
        elif any(x in model_name for x in ["whisper", "wav2vec", "clap", "audio", "speech", "voice"]):
            return "audio"
        elif any(x in model_name for x in ["llava", "llama", "gpt", "llm", "falcon", "mistral", "phi", "gemma"]):
            return "llm"
        elif any(x in model_name for x in ["bert", "roberta", "text", "embed", "sentence", "bge"]):
            return "text"
        
        # Default to text if no indicators found
        return "text"
    
    def _apply_target_specific_optimizations(self, 
                                            model_path: str, 
                                            model_type: str,
                                            deployment_info: Dict[str, Any]):
        """Apply target-specific optimizations to the model."""
        if self.deployment_target == DeploymentTarget.ANDROID:
            # Android-specific optimizations
            deployment_info["optimizations_applied"].append("android_memory_optimization")
            
            if model_type == "vision":
                # Vision-specific optimizations for Android
                deployment_info["optimizations_applied"].append("android_vision_optimization")
            
            elif model_type == "llm":
                # LLM-specific optimizations for Android
                deployment_info["optimizations_applied"].append("android_llm_optimization")
        
        elif self.deployment_target == DeploymentTarget.IOS:
            # iOS-specific optimizations
            deployment_info["optimizations_applied"].append("ios_memory_optimization")
            
            if model_type == "vision":
                # Vision-specific optimizations for iOS
                deployment_info["optimizations_applied"].append("ios_vision_optimization")
        
        elif self.deployment_target == DeploymentTarget.BROWSER:
            # Browser-specific optimizations
            deployment_info["optimizations_applied"].append("browser_compatibility_optimization")
            
            if model_type == "vision":
                # Vision-specific optimizations for browser
                deployment_info["optimizations_applied"].append("webnn_vision_optimization")
            
            elif model_type == "text":
                # Text-specific optimizations for browser
                deployment_info["optimizations_applied"].append("webnn_text_optimization")
    
    def load_model(self, 
                  model_path: str, 
                  model_loader: Optional[Callable] = None,
                  **kwargs) -> Dict[str, Any]:
        """
        Load a model for power-efficient inference.
        
        Args:
            model_path: Path to the optimized model
            model_loader: Optional custom model loader function
            **kwargs: Additional parameters for model loading
            
        Returns:
            Dictionary with loaded model information
        """
        start_time = time.time()
        
        # Check if model exists
        if not os.path.exists(model_path):
            return {
                "status": "error",
                "error": f"Model file not found: {model_path}",
                "loading_time_seconds": 0
            }
        
        # Get deployment info if available
        deployment_info = self.deployed_models.get(model_path, {})
        model_type = deployment_info.get("model_type", self._infer_model_type(model_path))
        
        # Create model info
        model_info = {
            "model_path": model_path,
            "model_type": model_type,
            "loading_time_seconds": 0,
            "loaded_at": time.time(),
            "last_used_at": time.time(),
            "inference_count": 0,
            "total_inference_time_seconds": 0,
            "power_metrics": {},
            "status": "loading"
        }
        
        try:
            if model_loader:
                # Use provided model loader
                logger.info(f"Loading model {model_path} with custom loader")
                model = model_loader(model_path, **kwargs)
            else:
                # Use default loader based on model type
                logger.info(f"Loading model {model_path} with default loader")
                model = self._default_model_loader(model_path, model_type, **kwargs)
            
            # Update model info
            model_info["model"] = model
            model_info["loading_time_seconds"] = time.time() - start_time
            model_info["status"] = "loaded"
            
            # Add to active models
            self.active_models.add(model_path)
            self.model_stats[model_path] = model_info
            
            logger.info(f"Model {model_path} loaded successfully in {model_info['loading_time_seconds']:.2f} seconds")
            
            # Start monitoring if it's not already running
            if not self.monitoring_active:
                self._start_monitoring()
            
            return model_info
            
        except Exception as e:
            error_msg = f"Error loading model: {str(e)}"
            logger.error(error_msg)
            logger.error(traceback.format_exc())
            
            model_info["status"] = "error"
            model_info["error"] = error_msg
            
            return model_info
    
    def _default_model_loader(self, model_path: str, model_type: str, **kwargs) -> Any:
        """Default model loader implementation."""
        # This is a placeholder for the actual model loading logic
        # In a real implementation, this would dispatch to the appropriate
        # model loading method based on the model type and format
        
        # For demonstration, return a simple mock model
        return {
            "model_path": model_path,
            "model_type": model_type,
            "mock_model": True,
            "params": kwargs
        }
    
    def run_inference(self, 
                     model_path: str, 
                     inputs: Any, 
                     inference_handler: Optional[Callable] = None,
                     **kwargs) -> Dict[str, Any]:
        """
        Run inference with a loaded model.
        
        Args:
            model_path: Path to the loaded model
            inputs: Input data for inference
            inference_handler: Optional custom inference handler function
            **kwargs: Additional parameters for inference
            
        Returns:
            Dictionary with inference results
        """
        # Check if model is loaded
        if model_path not in self.active_models:
            # Try to load the model
            load_result = self.load_model(model_path)
            if load_result["status"] != "loaded":
                return {
                    "status": "error",
                    "error": f"Model not loaded: {model_path}",
                    "inference_time_seconds": 0
                }
        
        # Get model info
        model_info = self.model_stats.get(model_path, {})
        model = model_info.get("model")
        
        # Create inference result structure
        inference_result = {
            "model_path": model_path,
            "inference_time_seconds": 0,
            "status": "running"
        }
        
        # Check thermal status before inference
        thermal_status = self._check_thermal_status()
        thermal_throttling = thermal_status.get("thermal_throttling", False)
        
        # Adjust inference parameters based on thermal status
        if thermal_throttling:
            logger.warning(f"Thermal throttling active. Adjusting inference parameters.")
            throttling_level = thermal_status.get("throttling_level", 0)
            # Adjust batch size or other parameters based on throttling level
            if "batch_size" in kwargs and throttling_level > 2:
                # Reduce batch size during heavy throttling
                original_batch_size = kwargs["batch_size"]
                kwargs["batch_size"] = max(1, original_batch_size // 2)
                logger.warning(f"Reducing batch size from {original_batch_size} to {kwargs['batch_size']} due to thermal throttling")
        
        # Record start time
        start_time = time.time()
        
        try:
            if inference_handler:
                # Use provided inference handler
                logger.debug(f"Running inference with custom handler")
                outputs = inference_handler(model, inputs, **kwargs)
            else:
                # Use default inference method
                logger.debug(f"Running inference with default handler")
                outputs = self._default_inference_handler(model, inputs, **kwargs)
            
            # Calculate inference time
            inference_time = time.time() - start_time
            
            # Update model statistics
            model_info["last_used_at"] = time.time()
            model_info["inference_count"] += 1
            model_info["total_inference_time_seconds"] += inference_time
            
            # Update inference result
            inference_result["outputs"] = outputs
            inference_result["inference_time_seconds"] = inference_time
            inference_result["status"] = "success"
            
            # Add thermal status information
            if thermal_throttling:
                inference_result["thermal_throttling"] = True
                inference_result["throttling_level"] = thermal_status.get("throttling_level", 0)
            
            # Apply power management policies
            self._apply_power_management_policies()
            
            return inference_result
            
        except Exception as e:
            error_msg = f"Error during inference: {str(e)}"
            logger.error(error_msg)
            logger.error(traceback.format_exc())
            
            inference_result["status"] = "error"
            inference_result["error"] = error_msg
            inference_result["inference_time_seconds"] = time.time() - start_time
            
            return inference_result
    
    def _default_inference_handler(self, model: Any, inputs: Any, **kwargs) -> Any:
        """Default inference handler implementation."""
        # This is a placeholder for the actual inference logic
        # In a real implementation, this would dispatch to the appropriate
        # inference method based on the model type
        
        # For demonstration, simulate inference by sleeping
        if "batch_size" in kwargs:
            # Simulate longer inference time for larger batches
            time.sleep(0.01 * kwargs["batch_size"])
        else:
            time.sleep(0.01)
        
        # Return mock results
        if isinstance(inputs, str):
            # Text input
            return {"text_output": f"Processed: {inputs[:20]}..."}
        elif isinstance(inputs, dict) and "image" in inputs:
            # Vision input
            return {"vision_output": "Image processed", "features": [0.1, 0.2, 0.3]}
        else:
            # Generic output
            return {"output": "Inference completed", "features": [0.1, 0.2, 0.3]}
    
    def _check_thermal_status(self) -> Dict[str, Any]:
        """Check thermal status and apply throttling if necessary."""
        if not HAS_THERMAL_MONITORING or not self.thermal_monitor:
            return {"thermal_throttling": False}
        
        # Get current thermal status
        thermal_status = self.thermal_monitor.get_current_thermal_status()
        
        # Extract throttling information
        overall_status = thermal_status.get("overall_status", "NORMAL")
        throttling = thermal_status.get("throttling", {})
        throttling_active = throttling.get("throttling_active", False)
        throttling_level = throttling.get("current_level", 0)
        
        # Create result
        result = {
            "thermal_status": overall_status,
            "thermal_throttling": throttling_active,
            "throttling_level": throttling_level,
            "temperatures": {
                name: zone.get("current_temp", 0) 
                for name, zone in thermal_status.get("thermal_zones", {}).items()
            }
        }
        
        return result
    
    def _apply_power_management_policies(self):
        """Apply power management policies after inference."""
        # Sleep between inferences if configured
        if self.config["power_management"]["sleep_between_inferences"]:
            sleep_duration_ms = self.config["power_management"]["sleep_duration_ms"]
            if sleep_duration_ms > 0:
                time.sleep(sleep_duration_ms / 1000.0)
    
    def _start_monitoring(self):
        """Start background monitoring thread."""
        if self.monitoring_active:
            logger.warning("Monitoring thread already running")
            return
        
        self.monitoring_active = True
        self.monitoring_thread = threading.Thread(target=self._monitoring_loop)
        self.monitoring_thread.daemon = True
        self.monitoring_thread.start()
        
        logger.info("Started monitoring thread")
        
        # Start thermal monitoring if available
        if HAS_THERMAL_MONITORING and self.thermal_monitor:
            self.thermal_monitor.start_monitoring()
            logger.info("Started thermal monitoring")
    
    def _stop_monitoring(self):
        """Stop background monitoring thread."""
        if not self.monitoring_active:
            return
        
        self.monitoring_active = False
        
        if self.monitoring_thread:
            # Wait for the thread to terminate
            self.monitoring_thread.join(timeout=2.0)
            self.monitoring_thread = None
        
        # Stop thermal monitoring if available
        if HAS_THERMAL_MONITORING and self.thermal_monitor:
            self.thermal_monitor.stop_monitoring()
        
        logger.info("Stopped monitoring")
    
    def _monitoring_loop(self):
        """Background thread for monitoring models and device state."""
        logger.info("Monitoring loop started")
        
        metrics_interval = self.config["monitoring"]["metrics_interval_seconds"]
        last_metrics_time = 0
        
        while self.monitoring_active:
            try:
                current_time = time.time()
                
                # Check for idle models to unload
                if self.config["memory_optimization"]["unload_unused_models"]:
                    idle_timeout = self.config["memory_optimization"]["idle_timeout_seconds"]
                    self._check_idle_models(current_time, idle_timeout)
                
                # Collect and store metrics periodically
                if current_time - last_metrics_time >= metrics_interval:
                    self._collect_and_store_metrics()
                    last_metrics_time = current_time
                
                # Sleep for a short time
                time.sleep(1.0)
                
            except Exception as e:
                logger.error(f"Error in monitoring loop: {str(e)}")
                logger.error(traceback.format_exc())
        
        logger.info("Monitoring loop ended")
    
    def _check_idle_models(self, current_time: float, idle_timeout: float):
        """Check for and unload idle models."""
        models_to_unload = []
        
        for model_path in list(self.active_models):
            model_info = self.model_stats.get(model_path, {})
            last_used_at = model_info.get("last_used_at", 0)
            
            # Check if model has been idle for too long
            if current_time - last_used_at > idle_timeout:
                logger.info(f"Model {model_path} idle for {current_time - last_used_at:.1f} seconds. Unloading.")
                models_to_unload.append(model_path)
        
        # Unload idle models
        for model_path in models_to_unload:
            self.unload_model(model_path)
    
    def _collect_and_store_metrics(self):
        """Collect and store performance metrics."""
        if not self.config["monitoring"]["collect_metrics"]:
            return
        
        # Collect metrics from active models
        model_metrics = {}
        for model_path in self.active_models:
            model_info = self.model_stats.get(model_path, {})
            
            metrics = {
                "model_path": model_path,
                "model_type": model_info.get("model_type", "unknown"),
                "inference_count": model_info.get("inference_count", 0),
                "total_inference_time_seconds": model_info.get("total_inference_time_seconds", 0),
                "average_inference_time_ms": 0,
                "timestamp": time.time()
            }
            
            # Calculate average inference time
            if metrics["inference_count"] > 0:
                metrics["average_inference_time_ms"] = (metrics["total_inference_time_seconds"] * 1000) / metrics["inference_count"]
            
            model_metrics[model_path] = metrics
        
        # Collect device state
        device_state = self._collect_device_state()
        
        # Store metrics in database if available and enabled
        if self.db_api and self.config["monitoring"]["log_to_database"]:
            self._store_metrics_in_database(model_metrics, device_state)
    
    def _collect_device_state(self) -> Dict[str, Any]:
        """Collect current device state information."""
        device_state = {
            "timestamp": time.time()
        }
        
        # Collect thermal information if available
        if HAS_THERMAL_MONITORING and self.thermal_monitor:
            thermal_status = self._check_thermal_status()
            device_state["thermal"] = thermal_status
        
        # Collect battery information if available
        try:
            # This is a placeholder for actual battery monitoring
            # In a real implementation, this would use platform-specific APIs
            device_state["battery"] = {
                "level_percent": 80.0,  # Mock battery level
                "is_charging": False
            }
        except:
            pass
        
        # Collect memory information if available
        try:
            import psutil
            memory = psutil.virtual_memory()
            device_state["memory"] = {
                "total_mb": memory.total / (1024 * 1024),
                "available_mb": memory.available / (1024 * 1024),
                "used_mb": memory.used / (1024 * 1024),
                "percent": memory.percent
            }
        except:
            pass
        
        # Store device state
        self.last_device_state = device_state
        
        return device_state
    
    def _store_metrics_in_database(self, model_metrics: Dict[str, Dict[str, Any]], device_state: Dict[str, Any]):
        """Store collected metrics in the database."""
        if not self.db_api:
            return
        
        try:
            # Store model metrics
            for model_path, metrics in model_metrics.items():
                # Create database entry
                query = """
                INSERT INTO model_deployment_metrics (
                    model_path, model_type, deployment_target, power_profile, 
                    inference_count, total_inference_time_seconds, average_inference_time_ms,
                    timestamp
                ) VALUES (?, ?, ?, ?, ?, ?, ?, ?)
                """
                
                params = [
                    model_path,
                    metrics["model_type"],
                    self.deployment_target.name,
                    self.power_profile.name,
                    metrics["inference_count"],
                    metrics["total_inference_time_seconds"],
                    metrics["average_inference_time_ms"],
                    metrics["timestamp"]
                ]
                
                self.db_api.execute_query(query, params)
            
            # Store device state
            query = """
            INSERT INTO device_state_metrics (
                deployment_target, thermal_status, thermal_throttling, throttling_level,
                battery_level_percent, is_charging, memory_used_percent,
                timestamp
            ) VALUES (?, ?, ?, ?, ?, ?, ?, ?)
            """
            
            # Extract values from device state
            thermal = device_state.get("thermal", {})
            battery = device_state.get("battery", {})
            memory = device_state.get("memory", {})
            
            params = [
                self.deployment_target.name,
                thermal.get("thermal_status", "UNKNOWN"),
                1 if thermal.get("thermal_throttling", False) else 0,
                thermal.get("throttling_level", 0),
                battery.get("level_percent", 0),
                1 if battery.get("is_charging", False) else 0,
                memory.get("percent", 0),
                device_state["timestamp"]
            ]
            
            self.db_api.execute_query(query, params)
            
        except Exception as e:
            logger.error(f"Error storing metrics in database: {str(e)}")
    
    def unload_model(self, model_path: str) -> bool:
        """
        Unload a model from memory.
        
        Args:
            model_path: Path to the model to unload
            
        Returns:
            Success status
        """
        if model_path not in self.active_models:
            logger.warning(f"Model {model_path} not currently loaded")
            return False
        
        try:
            # Remove from active models
            self.active_models.remove(model_path)
            
            # Store stats in case model is loaded again
            model_stats = self.model_stats.get(model_path, {})
            
            # Clear model reference to free memory
            if "model" in model_stats:
                model_stats["model"] = None
            
            # Update status
            model_stats["status"] = "unloaded"
            model_stats["unloaded_at"] = time.time()
            
            logger.info(f"Model {model_path} unloaded")
            
            # Stop monitoring if no active models
            if not self.active_models:
                self._stop_monitoring()
            
            return True
            
        except Exception as e:
            logger.error(f"Error unloading model {model_path}: {str(e)}")
            return False
    
    def get_deployment_status(self, model_path: Optional[str] = None) -> Dict[str, Any]:
        """
        Get deployment status for all models or a specific model.
        
        Args:
            model_path: Optional path to a specific model
            
        Returns:
            Dictionary with deployment status information
        """
        if model_path:
            # Get status for a specific model
            deployment_info = self.deployed_models.get(model_path, {})
            model_stats = self.model_stats.get(model_path, {})
            
            if not deployment_info and not model_stats:
                return {
                    "status": "unknown",
                    "error": f"Model {model_path} not found"
                }
            
            # Combine information
            status = {
                "model_path": model_path,
                "deployment_info": deployment_info,
                "active": model_path in self.active_models,
                "stats": {k: v for k, v in model_stats.items() if k != "model"}
            }
            
            return status
        else:
            # Get status for all models
            all_status = {
                "deployment_target": self.deployment_target.name,
                "power_profile": self.power_profile.name,
                "monitoring_active": self.monitoring_active,
                "active_models_count": len(self.active_models),
                "deployed_models_count": len(self.deployed_models),
                "deployed_models": {},
                "device_state": self.last_device_state
            }
            
            # Add status for each deployed model
            for path, info in self.deployed_models.items():
                model_stats = self.model_stats.get(path, {})
                
                all_status["deployed_models"][path] = {
                    "model_type": info.get("model_type", "unknown"),
                    "status": info.get("status", "unknown"),
                    "active": path in self.active_models,
                    "inference_count": model_stats.get("inference_count", 0),
                    "last_used_at": model_stats.get("last_used_at", 0)
                }
            
            return all_status
    
    def get_power_efficiency_report(self, 
                                   model_path: Optional[str] = None,
                                   report_format: str = "json") -> Dict[str, Any]:
        """
        Generate a power efficiency report.
        
        Args:
            model_path: Optional path to a specific model
            report_format: Report format (json, markdown, html)
            
        Returns:
            Power efficiency report
        """
        # Collect device state
        device_state = self._collect_device_state()
        
        # Basic report structure
        report = {
            "timestamp": time.time(),
            "deployment_target": self.deployment_target.name,
            "power_profile": self.power_profile.name,
            "device_state": device_state,
            "models": {}
        }
        
        # Collect model information
        models_to_report = [model_path] if model_path else self.deployed_models.keys()
        
        for path in models_to_report:
            if path not in self.deployed_models:
                continue
                
            deployment_info = self.deployed_models.get(path, {})
            model_stats = self.model_stats.get(path, {})
            
            # Calculate average inference time
            inference_count = model_stats.get("inference_count", 0)
            total_inference_time = model_stats.get("total_inference_time_seconds", 0)
            avg_inference_time = 0
            if inference_count > 0:
                avg_inference_time = (total_inference_time * 1000) / inference_count
            
            # Calculate power efficiency metrics
            power_metrics = deployment_info.get("power_efficiency_metrics", {})
            
            # Add to report
            report["models"][path] = {
                "model_type": deployment_info.get("model_type", "unknown"),
                "status": model_stats.get("status", "unknown"),
                "active": path in self.active_models,
                "inference_count": inference_count,
                "average_inference_time_ms": avg_inference_time,
                "size_reduction_ratio": deployment_info.get("size_reduction_ratio", 1.0),
                "power_consumption_mw": power_metrics.get("power_consumption_mw", 0),
                "energy_efficiency_items_per_joule": power_metrics.get("energy_efficiency_items_per_joule", 0),
                "battery_impact_percent_per_hour": power_metrics.get("battery_impact_percent_per_hour", 0),
                "quantization_method": deployment_info.get("quantization_method", "none"),
                "optimizations_applied": deployment_info.get("optimizations_applied", [])
            }
        
        # Generate the appropriate format
        if report_format == "markdown":
            return self._generate_markdown_report(report)
        elif report_format == "html":
            return self._generate_html_report(report)
        else:
            return report
    
    def _generate_markdown_report(self, report_data: Dict[str, Any]) -> str:
        """Generate a Markdown report from report data."""
        markdown = f"# Power Efficiency Report\n\n"
        markdown += f"**Date:** {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(report_data['timestamp']))}\n"
        markdown += f"**Deployment Target:** {report_data['deployment_target']}\n"
        markdown += f"**Power Profile:** {report_data['power_profile']}\n\n"
        
        # Add device state
        device_state = report_data["device_state"]
        thermal = device_state.get("thermal", {})
        battery = device_state.get("battery", {})
        memory = device_state.get("memory", {})
        
        markdown += "## Device State\n\n"
        
        if thermal:
            markdown += f"**Thermal Status:** {thermal.get('thermal_status', 'Unknown')}\n"
            markdown += f"**Thermal Throttling:** {'Active' if thermal.get('thermal_throttling', False) else 'Inactive'}\n"
            
            if "temperatures" in thermal:
                markdown += "\n**Temperatures:**\n\n"
                for name, temp in thermal["temperatures"].items():
                    markdown += f"- {name.capitalize()}: {temp:.1f}°C\n"
        
        if battery:
            markdown += f"\n**Battery Level:** {battery.get('level_percent', 0):.1f}%\n"
            markdown += f"**Charging:** {'Yes' if battery.get('is_charging', False) else 'No'}\n"
        
        if memory:
            markdown += f"\n**Memory Usage:** {memory.get('percent', 0):.1f}%\n"
            markdown += f"**Available Memory:** {memory.get('available_mb', 0):.1f} MB\n"
        
        # Add model information
        markdown += "\n## Models\n\n"
        
        if not report_data["models"]:
            markdown += "No models deployed.\n"
        else:
            markdown += "| Model | Type | Status | Inferences | Avg Time (ms) | Power (mW) | Battery (%/h) | Size Reduction |\n"
            markdown += "|-------|------|--------|------------|---------------|------------|--------------|---------------|\n"
            
            for path, model_data in report_data["models"].items():
                model_name = os.path.basename(path)
                markdown += f"| {model_name} | {model_data['model_type']} | {model_data['status']} | {model_data['inference_count']} | {model_data['average_inference_time_ms']:.2f} | {model_data['power_consumption_mw']:.2f} | {model_data['battery_impact_percent_per_hour']:.2f} | {model_data['size_reduction_ratio']:.2f}x |\n"
            
            # Add details for each model
            for path, model_data in report_data["models"].items():
                model_name = os.path.basename(path)
                markdown += f"\n### {model_name}\n\n"
                markdown += f"**Model Type:** {model_data['model_type']}\n"
                markdown += f"**Status:** {model_data['status']}\n"
                markdown += f"**Inference Count:** {model_data['inference_count']}\n"
                markdown += f"**Average Inference Time:** {model_data['average_inference_time_ms']:.2f} ms\n"
                markdown += f"**Power Consumption:** {model_data['power_consumption_mw']:.2f} mW\n"
                markdown += f"**Energy Efficiency:** {model_data['energy_efficiency_items_per_joule']:.2f} items/joule\n"
                markdown += f"**Battery Impact:** {model_data['battery_impact_percent_per_hour']:.2f}% per hour\n"
                markdown += f"**Size Reduction:** {model_data['size_reduction_ratio']:.2f}x\n"
                markdown += f"**Quantization Method:** {model_data['quantization_method']}\n"
                
                if model_data["optimizations_applied"]:
                    markdown += "\n**Optimizations Applied:**\n\n"
                    for opt in model_data["optimizations_applied"]:
                        markdown += f"- {opt}\n"
        
        # Add recommendations
        markdown += "\n## Recommendations\n\n"
        
        # Generate power efficiency recommendations based on the data
        recommendations = self._generate_power_recommendations(report_data)
        for recommendation in recommendations:
            markdown += f"- {recommendation}\n"
        
        return markdown
    
    def _generate_html_report(self, report_data: Dict[str, Any]) -> str:
        """Generate an HTML report from report data."""
        # This would generate an HTML version of the report
        # For now, convert the markdown report to simple HTML
        markdown_report = self._generate_markdown_report(report_data)
        
        # Simple conversion of markdown to HTML
        html = f"""
        <!DOCTYPE html>
        <html>
        <head>
            <title>Power Efficiency Report</title>
            <style>
                body {{ font-family: Arial, sans-serif; line-height: 1.6; margin: 20px; }}
                h1 {{ color: #333366; }}
                h2 {{ color: #336699; margin-top: 20px; }}
                h3 {{ color: #339999; }}
                table {{ border-collapse: collapse; width: 100%; margin: 15px 0; }}
                th, td {{ border: 1px solid #ddd; padding: 8px; text-align: left; }}
                th {{ background-color: #f2f2f2; }}
                tr:nth-child(even) {{ background-color: #f9f9f9; }}
            </style>
        </head>
        <body>
            {markdown_report.replace('# ', '<h1>').replace('\n## ', '</h1><h2>').replace('\n### ', '</h2><h3>').replace('\n', '<br>')}
        </body>
        </html>
        """
        
        return html
    
    def _generate_power_recommendations(self, report_data: Dict[str, Any]) -> List[str]:
        """Generate power efficiency recommendations based on report data."""
        recommendations = []
        
        # Check device state
        device_state = report_data["device_state"]
        thermal = device_state.get("thermal", {})
        battery = device_state.get("battery", {})
        
        # Thermal recommendations
        if thermal.get("thermal_throttling", False):
            recommendations.append("Thermal throttling is active. Consider reducing model complexity or batch size to lower power consumption.")
            
            if thermal.get("throttling_level", 0) >= 3:
                recommendations.append("High thermal throttling level detected. Model performance will be significantly reduced.")
        
        # Battery recommendations
        if battery and battery.get("level_percent", 100) < 20 and not battery.get("is_charging", False):
            recommendations.append("Battery level is low. Consider switching to a more power-efficient model or quantization method.")
        
        # Model-specific recommendations
        for path, model_data in report_data["models"].items():
            model_name = os.path.basename(path)
            
            # Check for high battery impact
            if model_data["battery_impact_percent_per_hour"] > 5.0:
                recommendations.append(f"Model {model_name} has high battery impact ({model_data['battery_impact_percent_per_hour']:.1f}% per hour). Consider using a more efficient quantization method.")
            
            # Check for inefficient quantization
            if model_data["quantization_method"] == "none" or model_data["quantization_method"] == "weight_only":
                recommendations.append(f"Model {model_name} is using {model_data['quantization_method']} quantization. Consider using int8 or int4 for better power efficiency.")
            
            # Check for size reduction opportunities
            if model_data["size_reduction_ratio"] < 2.0:
                recommendations.append(f"Model {model_name} has limited size reduction ({model_data['size_reduction_ratio']:.1f}x). Consider more aggressive quantization for better memory efficiency.")
        
        # Add profile recommendations
        current_profile = report_data["power_profile"]
        
        if current_profile == "MAXIMUM_PERFORMANCE" and thermal.get("thermal_throttling", False):
            recommendations.append("Consider switching from MAXIMUM_PERFORMANCE to BALANCED profile to reduce thermal throttling.")
        
        if battery and battery.get("level_percent", 100) < 30 and current_profile != "POWER_SAVER" and current_profile != "ULTRA_EFFICIENT":
            recommendations.append(f"Battery level is {battery.get('level_percent', 0):.1f}%. Consider switching to POWER_SAVER or ULTRA_EFFICIENT profile.")
        
        return recommendations
    
    def cleanup(self):
        """Clean up resources and unload all models."""
        # Stop monitoring
        self._stop_monitoring()
        
        # Unload all active models
        for model_path in list(self.active_models):
            self.unload_model(model_path)
        
        logger.info("Cleaned up power-efficient deployment resources")


def main():
    """Command-line interface for power-efficient deployment."""
    import argparse
    
    parser = argparse.ArgumentParser(description="Power-Efficient Model Deployment")
    
    # Command groups
    command_group = parser.add_subparsers(dest="command", help="Command to execute")
    
    # Prepare model command
    prepare_parser = command_group.add_parser("prepare", help="Prepare a model for power-efficient deployment")
    prepare_parser.add_argument("--model-path", required=True, help="Path to input model")
    prepare_parser.add_argument("--output-path", help="Path for optimized model (optional)")
    prepare_parser.add_argument("--model-type", choices=["text", "vision", "audio", "llm"], help="Model type")
    prepare_parser.add_argument("--quantization-method", help="Quantization method to use")
    prepare_parser.add_argument("--power-profile", choices=[p.name for p in PowerProfile], default="BALANCED", help="Power consumption profile")
    prepare_parser.add_argument("--deployment-target", choices=[t.name for t in DeploymentTarget], default="ANDROID", help="Deployment target")
    
    # Load model command
    load_parser = command_group.add_parser("load", help="Load a model for inference")
    load_parser.add_argument("--model-path", required=True, help="Path to optimized model")
    
    # Run inference command
    inference_parser = command_group.add_parser("inference", help="Run inference with a loaded model")
    inference_parser.add_argument("--model-path", required=True, help="Path to loaded model")
    inference_parser.add_argument("--input", required=True, help="Input data for inference")
    inference_parser.add_argument("--batch-size", type=int, default=1, help="Batch size for inference")
    
    # Status command
    status_parser = command_group.add_parser("status", help="Get deployment status")
    status_parser.add_argument("--model-path", help="Path to specific model (optional)")
    
    # Report command
    report_parser = command_group.add_parser("report", help="Generate power efficiency report")
    report_parser.add_argument("--model-path", help="Path to specific model (optional)")
    report_parser.add_argument("--format", choices=["json", "markdown", "html"], default="json", help="Report format")
    report_parser.add_argument("--output", help="Path to save report (optional)")
    
    # Common options
    parser.add_argument("--db-path", help="Path to DuckDB database")
    parser.add_argument("--verbose", action="store_true", help="Enable verbose output")
    
    args = parser.parse_args()
    
    # Set logging level
    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)
    
    # Create deployment instance
    try:
        power_profile = PowerProfile[args.power_profile] if hasattr(args, 'power_profile') else PowerProfile.BALANCED
        deployment_target = DeploymentTarget[args.deployment_target] if hasattr(args, 'deployment_target') else DeploymentTarget.ANDROID
        
        deployment = PowerEfficientDeployment(
            db_path=args.db_path,
            power_profile=power_profile,
            deployment_target=deployment_target
        )
        
        # Process commands
        if args.command == "prepare":
            result = deployment.prepare_model_for_deployment(
                model_path=args.model_path,
                output_path=args.output_path,
                model_type=args.model_type,
                quantization_method=args.quantization_method
            )
            
            if result["status"] == "ready":
                print(f"\nModel prepared successfully:")
                print(f"- Input: {args.model_path}")
                print(f"- Output: {result['output_model_path']}")
                print(f"- Model Type: {result['model_type']}")
                print(f"- Quantization Method: {result['quantization_method']}")
                print(f"- Status: {result['status']}")
                
                if "size_reduction_ratio" in result:
                    print(f"- Size Reduction: {result['size_reduction_ratio']:.2f}x")
                
                print("\nOptimizations applied:")
                for opt in result["optimizations_applied"]:
                    print(f"- {opt}")
                    
                # Print power efficiency metrics if available
                if "power_efficiency_metrics" in result:
                    metrics = result["power_efficiency_metrics"]
                    print("\nEstimated Power Efficiency Metrics:")
                    print(f"- Power Consumption: {metrics.get('power_consumption_mw', 0):.2f} mW")
                    print(f"- Energy Efficiency: {metrics.get('energy_efficiency_items_per_joule', 0):.2f} items/joule")
                    print(f"- Battery Impact: {metrics.get('battery_impact_percent_per_hour', 0):.2f}% per hour")
            else:
                print(f"\nError preparing model: {result.get('error', 'Unknown error')}")
                return 1
        
        elif args.command == "load":
            result = deployment.load_model(model_path=args.model_path)
            
            if result["status"] == "loaded":
                print(f"\nModel loaded successfully:")
                print(f"- Model: {args.model_path}")
                print(f"- Loading Time: {result['loading_time_seconds']:.2f} seconds")
            else:
                print(f"\nError loading model: {result.get('error', 'Unknown error')}")
                return 1
        
        elif args.command == "inference":
            result = deployment.run_inference(
                model_path=args.model_path,
                inputs=args.input,
                batch_size=args.batch_size
            )
            
            if result["status"] == "success":
                print(f"\nInference completed successfully:")
                print(f"- Model: {args.model_path}")
                print(f"- Inference Time: {result['inference_time_seconds']:.4f} seconds")
                print(f"- Output: {result['outputs']}")
                
                if "thermal_throttling" in result and result["thermal_throttling"]:
                    print(f"\nNote: Thermal throttling was active during inference (level: {result['throttling_level']})")
            else:
                print(f"\nError during inference: {result.get('error', 'Unknown error')}")
                return 1
        
        elif args.command == "status":
            status = deployment.get_deployment_status(args.model_path)
            
            if args.model_path:
                if "error" in status:
                    print(f"\nError: {status['error']}")
                    return 1
                    
                print(f"\nStatus for model {args.model_path}:")
                print(f"- Active: {status['active']}")
                print(f"- Model Type: {status['deployment_info'].get('model_type', 'Unknown')}")
                print(f"- Status: {status['deployment_info'].get('status', 'Unknown')}")
                
                stats = status.get("stats", {})
                if stats:
                    print("\nStatistics:")
                    print(f"- Inference Count: {stats.get('inference_count', 0)}")
                    print(f"- Total Inference Time: {stats.get('total_inference_time_seconds', 0):.2f} seconds")
                    if stats.get('inference_count', 0) > 0:
                        avg_time = (stats.get('total_inference_time_seconds', 0) * 1000) / stats.get('inference_count', 1)
                        print(f"- Average Inference Time: {avg_time:.2f} ms")
            else:
                print(f"\nDeployment Status:")
                print(f"- Deployment Target: {status['deployment_target']}")
                print(f"- Power Profile: {status['power_profile']}")
                print(f"- Active Models: {status['active_models_count']}")
                print(f"- Deployed Models: {status['deployed_models_count']}")
                
                if status["deployed_models"]:
                    print("\nDeployed Models:")
                    for path, model_data in status["deployed_models"].items():
                        active_status = "Active" if model_data["active"] else "Inactive"
                        print(f"- {os.path.basename(path)} ({model_data['model_type']}): {model_data['status']} [{active_status}]")
                
                # Print device state
                device_state = status.get("device_state", {})
                thermal = device_state.get("thermal", {})
                battery = device_state.get("battery", {})
                
                if thermal:
                    print("\nThermal Status:")
                    print(f"- Status: {thermal.get('thermal_status', 'Unknown')}")
                    print(f"- Thermal Throttling: {'Active' if thermal.get('thermal_throttling', False) else 'Inactive'}")
                    if thermal.get('thermal_throttling', False):
                        print(f"- Throttling Level: {thermal.get('throttling_level', 0)}")
                
                if battery:
                    print("\nBattery Status:")
                    print(f"- Level: {battery.get('level_percent', 0):.1f}%")
                    print(f"- Charging: {'Yes' if battery.get('is_charging', False) else 'No'}")
        
        elif args.command == "report":
            report = deployment.get_power_efficiency_report(
                model_path=args.model_path,
                report_format=args.format
            )
            
            if args.output:
                with open(args.output, 'w') as f:
                    if args.format == "json":
                        json.dump(report, f, indent=2)
                    else:
                        f.write(report)
                print(f"Report saved to {args.output}")
            else:
                if args.format == "json":
                    print(json.dumps(report, indent=2))
                else:
                    print(report)
        
        else:
            parser.print_help()
            return 1
        
        # Clean up
        deployment.cleanup()
        return 0
        
    except Exception as e:
        print(f"Error: {str(e)}")
        traceback.print_exc()
        return 1

if __name__ == "__main__":
    sys.exit(main())