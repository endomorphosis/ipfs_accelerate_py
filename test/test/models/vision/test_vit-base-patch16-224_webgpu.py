#!/usr/bin/env python3
# Generated by TemplateRenderer on 2025-03-16 16:24:36
# Model: vit-base-patch16-224
# Template: vision_test (8c4ff6f5-3963-4ec3-8c14-c8ae78b3e29f)
# Hardware: webgpu
# Type: test

#!/usr/bin/env python3
"""
Template for testing vision models (ViT, ResNet, etc.) on different hardware platforms.

This template includes placeholders that will be replaced with actual values during test generation:
- vit-base-patch16-224: Name of the model (e.g., "vit-base-patch16-224")
- vision: Family of the model (e.g., "vision")
- webgpu: Type of hardware to run on (e.g., "cpu", "cuda", "rocm")
- 1: Batch size for testing (e.g., 1, 4, 8)
"""

import os
import time
import logging
import numpy as np
import torch
from transformers import AutoImageProcessor, AutoModel

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger("test_vit_base_patch16_224")

def test_vit_base_patch16_224_on_webgpu():
    """Test vit-base-patch16-224 model on webgpu hardware with batch size 1."""
    
    # Record start time for performance measurement
    start_time = time.time()
    
    # Log test configuration
    logger.info(f"Testing model: vit-base-patch16-224")
    logger.info(f"Hardware: webgpu")
    logger.info(f"Batch size: 1")
    
    try:
        # Initialize processor and model
        processor = AutoImageProcessor.from_pretrained("vit-base-patch16-224")
        model = AutoModel.from_pretrained("vit-base-patch16-224")
        
        # Move model to appropriate device
        device = "webgpu"
        if device != "cpu":
            model = model.to(device)
        
        # Log model information
        logger.info(f"Model loaded successfully: {model.__class__.__name__}")
        
        # Create dummy image for testing
        dummy_image = np.random.randint(0, 255, (224, 224, 3), dtype=np.uint8)
        
        # Process the image
        inputs = processor(dummy_image, return_tensors="pt")
        if device != "cpu":
            inputs = {k: v.to(device) for k, v in inputs.items()}
        
        # Create batch by repeating the input
        # Handle different tensor shapes appropriately
        batch_inputs = {}
        for k, v in inputs.items():
            if v.dim() == 4:  # Image tensor (B, C, H, W)
                batch_inputs[k] = v.repeat(1, 1, 1, 1)
            else:  # Other tensors
                batch_inputs[k] = v.repeat(1, 1)
        
        # Measure inference time
        inference_start = time.time()
        
        # Run inference
        with torch.no_grad():
            outputs = model(**batch_inputs)
        
        # Calculate inference time
        inference_time = time.time() - inference_start
        
        # Extract features
        features = outputs.last_hidden_state
        
        # Log results
        logger.info(f"Inference completed in {inference_time:.4f} seconds")
        logger.info(f"Output shape: {features.shape}")
        
        # Measure memory usage
        if device == "cuda":
            memory_usage = torch.cuda.max_memory_allocated() / (1024 ** 2)  # MB
        else:
            import psutil
            memory_usage = psutil.Process(os.getpid()).memory_info().rss / (1024 ** 2)  # MB
            
        logger.info(f"Memory usage: {memory_usage:.2f} MB")
        
        # Calculate total execution time
        execution_time = time.time() - start_time
        
        # Return test results
        return {
            "test_id": "dce4132d-d61d-4e47-b1ad-b49e75918833",  # Will be replaced with a UUID during generation
            "model_name": "vit-base-patch16-224",
            "model_family": "vision",
            "hardware_type": "webgpu",
            "batch_size": 1,
            "execution_time": execution_time,
            "inference_time": inference_time,
            "memory_usage": memory_usage,
            "feature_shape": features.shape,
            "success": True
        }
        
    except Exception as e:
        # Log error and return failure result
        logger.error(f"Test failed: {str(e)}")
        
        # Calculate total execution time
        execution_time = time.time() - start_time
        
        return {
            "test_id": "dce4132d-d61d-4e47-b1ad-b49e75918833",
            "model_name": "vit-base-patch16-224",
            "model_family": "vision",
            "hardware_type": "webgpu",
            "batch_size": 1,
            "execution_time": execution_time,
            "success": False,
            "error_message": str(e)
        }

if __name__ == "__main__":
    # This allows the template to be run directly for testing
    result = test_vit_base_patch16_224_on_webgpu()
    print(f"Test result: {'Success' if result['success'] else 'Failure'}")
    if result['success']:
        print(f"Execution time: {result['execution_time']:.4f} seconds")
        print(f"Inference time: {result['inference_time']:.4f} seconds")
        print(f"Memory usage: {result['memory_usage']:.2f} MB")
        print(f"Feature shape: {result['feature_shape']}")