#!/usr/bin/env python3
"""
Test script for the Advanced Calibrator in the Simulation Accuracy and Validation Framework.

This script demonstrates how to use the AdvancedSimulationCalibrator to calibrate simulation parameters
using various advanced techniques including Bayesian optimization, neural networks, ensemble methods,
incremental learning, and hardware-specific profiles.
"""

import os
import sys
import logging
import json
import numpy as np
from pathlib import Path
from pprint import pprint

# Configure logging
logging.basicConfig(level=logging.INFO,
                    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger("test_advanced_calibrator")

# Add parent directory to path for module imports
parent_dir = str(Path(__file__).resolve().parent.parent.parent)
if parent_dir not in sys.path:
    sys.path.insert(0, parent_dir)

# Import framework components
from duckdb_api.simulation_validation.calibration.advanced_calibrator import AdvancedSimulationCalibrator
from duckdb_api.simulation_validation.calibration.basic_calibrator import BasicSimulationCalibrator
from duckdb_api.simulation_validation.core.base import (
    SimulationResult,
    HardwareResult,
    ValidationResult
)
from duckdb_api.simulation_validation.test_validator import generate_sample_data

# Create output directory
OUTPUT_DIR = Path(__file__).parent / "output"
OUTPUT_DIR.mkdir(exist_ok=True)


def test_basic_vs_advanced_calibration():
    """Test basic vs. advanced calibration performance."""
    logger.info("Testing basic vs. advanced calibration performance")
    
    # Generate sample data
    simulation_results, hardware_results = generate_sample_data(num_samples=10)
    
    # Create validation results by pairing simulation and hardware results
    validation_results = []
    for i in range(len(simulation_results)):
        # Add metrics comparison as would be generated by comparison pipeline
        metrics_comparison = {}
        
        for metric in ["throughput_items_per_second", "average_latency_ms", "memory_peak_mb", "power_consumption_w"]:
            if (metric in simulation_results[i].metrics and
                metric in hardware_results[i].metrics):
                sim_value = simulation_results[i].metrics[metric]
                hw_value = hardware_results[i].metrics[metric]
                
                if sim_value is not None and hw_value is not None and hw_value != 0:
                    # Calculate various error metrics
                    abs_error = abs(hw_value - sim_value)
                    rel_error = abs_error / abs(hw_value)
                    mape = rel_error * 100  # percentage
                    
                    metrics_comparison[metric] = {
                        "simulation_value": sim_value,
                        "hardware_value": hw_value,
                        "absolute_error": abs_error,
                        "relative_error": rel_error,
                        "mape": mape
                    }
        
        validation_result = ValidationResult(
            simulation_result=simulation_results[i],
            hardware_result=hardware_results[i],
            metrics_comparison=metrics_comparison,
            validation_timestamp=simulation_results[i].timestamp,
            validation_version="test_v1.0",
            additional_metrics={}
        )
        
        validation_results.append(validation_result)
    
    # Initial simulation parameters
    initial_params = {
        "calibration_version": "initial_v1.0",
        "calibration_method": "linear_scaling",
        "correction_factors": {}
    }
    
    # Create basic and advanced calibrators
    basic_calibrator = BasicSimulationCalibrator()
    advanced_calibrator = AdvancedSimulationCalibrator()
    
    # Apply basic calibration
    basic_params = basic_calibrator.calibrate(validation_results, initial_params)
    
    # Apply advanced calibration
    advanced_params = advanced_calibrator.calibrate(validation_results, initial_params)
    
    # Save calibrated parameters
    basic_params_path = OUTPUT_DIR / "basic_calibration_params.json"
    advanced_params_path = OUTPUT_DIR / "advanced_calibration_params.json"
    
    try:
        def convert_to_json_serializable(obj):
            if isinstance(obj, (np.float64, np.float32, np.int64, np.int32, np.bool_)):
                return obj.item()
            elif isinstance(obj, (list, tuple)):
                return [convert_to_json_serializable(i) for i in obj]
            elif isinstance(obj, dict):
                return {k: convert_to_json_serializable(v) for k, v in obj.items()}
            else:
                return obj
        
        with open(basic_params_path, 'w') as f:
            json.dump(convert_to_json_serializable(basic_params), f, indent=2)
        
        with open(advanced_params_path, 'w') as f:
            json.dump(convert_to_json_serializable(advanced_params), f, indent=2)
        
        logger.info(f"Saved basic calibration parameters to {basic_params_path}")
        logger.info(f"Saved advanced calibration parameters to {advanced_params_path}")
    except Exception as e:
        logger.error(f"Error saving calibration parameters: {e}")
    
    # Test calibration application on a new simulation result
    test_sim_result = simulation_results[0]
    
    # Apply basic calibration
    basic_calibrated = basic_calibrator.apply_calibration(test_sim_result, basic_params)
    
    # Apply advanced calibration
    advanced_calibrated = advanced_calibrator.apply_calibration(test_sim_result, advanced_params)
    
    # Compare results
    for metric in ["throughput_items_per_second", "average_latency_ms", "memory_peak_mb", "power_consumption_w"]:
        if metric in test_sim_result.metrics:
            original = test_sim_result.metrics[metric]
            basic_val = basic_calibrated.metrics.get(metric, "N/A")
            advanced_val = advanced_calibrated.metrics.get(metric, "N/A")
            
            logger.info(f"Metric: {metric}")
            logger.info(f"  Original: {original}")
            logger.info(f"  Basic Calibration: {basic_val} (change: {(basic_val - original) / original * 100 if original and basic_val != 'N/A' else 'N/A'}%)")
            logger.info(f"  Advanced Calibration: {advanced_val} (change: {(advanced_val - original) / original * 100 if original and advanced_val != 'N/A' else 'N/A'}%)")
    
    # Save calibrated results
    calibration_comparison = {
        "original": {
            "metrics": test_sim_result.metrics,
            "metadata": test_sim_result.additional_metadata
        },
        "basic_calibrated": {
            "metrics": basic_calibrated.metrics,
            "metadata": basic_calibrated.additional_metadata
        },
        "advanced_calibrated": {
            "metrics": advanced_calibrated.metrics,
            "metadata": advanced_calibrated.additional_metadata
        }
    }
    
    comparison_path = OUTPUT_DIR / "calibration_comparison.json"
    
    try:
        with open(comparison_path, 'w') as f:
            json.dump(convert_to_json_serializable(calibration_comparison), f, indent=2)
        
        logger.info(f"Saved calibration comparison to {comparison_path}")
    except Exception as e:
        logger.error(f"Error saving calibration comparison: {e}")
    
    return {
        "basic_params": basic_params,
        "advanced_params": advanced_params,
        "basic_calibrated": basic_calibrated,
        "advanced_calibrated": advanced_calibrated
    }


def test_multiple_calibration_methods():
    """Test different calibration methods in the advanced calibrator."""
    logger.info("Testing multiple calibration methods")
    
    # Generate sample data
    simulation_results, hardware_results = generate_sample_data(num_samples=15)
    
    # Create validation results
    validation_results = []
    for i in range(len(simulation_results)):
        # Add metrics comparison
        metrics_comparison = {}
        
        for metric in ["throughput_items_per_second", "average_latency_ms", "memory_peak_mb", "power_consumption_w"]:
            if (metric in simulation_results[i].metrics and
                metric in hardware_results[i].metrics):
                sim_value = simulation_results[i].metrics[metric]
                hw_value = hardware_results[i].metrics[metric]
                
                if sim_value is not None and hw_value is not None and hw_value != 0:
                    # Calculate error metrics
                    abs_error = abs(hw_value - sim_value)
                    rel_error = abs_error / abs(hw_value)
                    mape = rel_error * 100
                    
                    metrics_comparison[metric] = {
                        "simulation_value": sim_value,
                        "hardware_value": hw_value,
                        "absolute_error": abs_error,
                        "relative_error": rel_error,
                        "mape": mape
                    }
        
        validation_result = ValidationResult(
            simulation_result=simulation_results[i],
            hardware_result=hardware_results[i],
            metrics_comparison=metrics_comparison,
            validation_timestamp=simulation_results[i].timestamp,
            validation_version="test_v1.0",
            additional_metrics={}
        )
        
        validation_results.append(validation_result)
    
    # Initial simulation parameters
    initial_params = {
        "calibration_version": "initial_v1.0",
        "calibration_method": "linear_scaling",
        "correction_factors": {}
    }
    
    # Test various methods
    methods = [
        "linear_scaling",
        "regression",
        "ensemble",
        "incremental"
    ]
    
    try:
        # Import sklearn to see if advanced methods are available
        import sklearn
        methods.extend(["bayesian", "network"])
    except ImportError:
        logger.warning("sklearn not available, skipping advanced methods test")
    
    # Results dictionary
    results = {}
    
    # Test each method
    for method in methods:
        logger.info(f"Testing calibration method: {method}")
        
        # Create advanced calibrator with specific method
        calibrator = AdvancedSimulationCalibrator({
            "calibration_method": method,
            "learning_rate": 0.8
        })
        
        # Apply calibration
        calibrated_params = calibrator.calibrate(validation_results, initial_params)
        
        # Apply to test result
        test_sim_result = simulation_results[0]
        calibrated_result = calibrator.apply_calibration(test_sim_result, calibrated_params)
        
        # Store results
        results[method] = {
            "parameters": calibrated_params,
            "calibrated_result": {
                "metrics": calibrated_result.metrics,
                "metadata": calibrated_result.additional_metadata
            }
        }
    
    # Save results
    methods_path = OUTPUT_DIR / "calibration_methods_comparison.json"
    
    try:
        def convert_to_json_serializable(obj):
            if isinstance(obj, (np.float64, np.float32, np.int64, np.int32, np.bool_)):
                return obj.item()
            elif isinstance(obj, (list, tuple)):
                return [convert_to_json_serializable(i) for i in obj]
            elif isinstance(obj, dict):
                return {k: convert_to_json_serializable(v) for k, v in obj.items()}
            else:
                return obj
        
        with open(methods_path, 'w') as f:
            json.dump(convert_to_json_serializable(results), f, indent=2)
        
        logger.info(f"Saved calibration methods comparison to {methods_path}")
    except Exception as e:
        logger.error(f"Error saving calibration methods comparison: {e}")
    
    return results


def test_hardware_specific_profiles():
    """Test hardware-specific profiles in advanced calibration."""
    logger.info("Testing hardware-specific profiles")
    
    # Generate sample data with specific hardware types
    hardware_types = ["cpu_intel_xeon", "gpu_rtx3080", "webgpu_chrome"]
    
    # Collect all validation results
    all_validation_results = []
    
    for hardware_type in hardware_types:
        logger.info(f"Generating data for hardware type: {hardware_type}")
        
        # Generate sample data specific to this hardware
        simulation_results, hardware_results = generate_sample_data(num_samples=5)
        
        # Override hardware IDs to match our test types
        for i in range(len(simulation_results)):
            simulation_results[i].hardware_id = hardware_type
            hardware_results[i].hardware_id = hardware_type
        
        # Create validation results
        for i in range(len(simulation_results)):
            # Add metrics comparison
            metrics_comparison = {}
            
            for metric in ["throughput_items_per_second", "average_latency_ms", "memory_peak_mb", "power_consumption_w"]:
                if (metric in simulation_results[i].metrics and
                    metric in hardware_results[i].metrics):
                    sim_value = simulation_results[i].metrics[metric]
                    hw_value = hardware_results[i].metrics[metric]
                    
                    if sim_value is not None and hw_value is not None and hw_value != 0:
                        # Calculate error metrics
                        abs_error = abs(hw_value - sim_value)
                        rel_error = abs_error / abs(hw_value)
                        mape = rel_error * 100
                        
                        metrics_comparison[metric] = {
                            "simulation_value": sim_value,
                            "hardware_value": hw_value,
                            "absolute_error": abs_error,
                            "relative_error": rel_error,
                            "mape": mape
                        }
            
            validation_result = ValidationResult(
                simulation_result=simulation_results[i],
                hardware_result=hardware_results[i],
                metrics_comparison=metrics_comparison,
                validation_timestamp=simulation_results[i].timestamp,
                validation_version="test_v1.0",
                additional_metrics={}
            )
            
            all_validation_results.append(validation_result)
    
    # Initial simulation parameters
    initial_params = {
        "calibration_version": "initial_v1.0",
        "calibration_method": "linear_scaling",
        "correction_factors": {}
    }
    
    # Create advanced calibrator with profiles
    profile_calibrator = AdvancedSimulationCalibrator({
        "calibration_method": "ensemble",
        "use_hardware_profiles": True
    })
    
    # Create advanced calibrator without profiles
    no_profile_calibrator = AdvancedSimulationCalibrator({
        "calibration_method": "ensemble",
        "use_hardware_profiles": False
    })
    
    # Apply calibration with profiles
    with_profile_params = profile_calibrator.calibrate(all_validation_results, initial_params)
    
    # Apply calibration without profiles
    without_profile_params = no_profile_calibrator.calibrate(all_validation_results, initial_params)
    
    # Test calibration on different hardware types
    results = {}
    
    for hardware_type in hardware_types:
        logger.info(f"Testing calibration for hardware type: {hardware_type}")
        
        # Create test simulation result for this hardware
        test_sim_result = SimulationResult(
            model_id="bert-base-uncased",
            hardware_id=hardware_type,
            metrics={
                "throughput_items_per_second": 100.0,
                "average_latency_ms": 50.0,
                "memory_peak_mb": 1000.0,
                "power_consumption_w": 100.0
            },
            batch_size=16,  # Use larger batch size to see batch-specific adjustments
            precision="fp16",  # Use fp16 to see precision-specific adjustments
            timestamp="2025-03-15T00:00:00",
            simulation_version="test_v1.0",
            additional_metadata={}
        )
        
        # Apply calibration with profiles
        with_profile_result = profile_calibrator.apply_calibration(test_sim_result, with_profile_params)
        
        # Apply calibration without profiles
        without_profile_result = no_profile_calibrator.apply_calibration(test_sim_result, without_profile_params)
        
        # Compare results
        comparison = {}
        
        for metric in ["throughput_items_per_second", "average_latency_ms", "memory_peak_mb", "power_consumption_w"]:
            if metric in test_sim_result.metrics:
                original = test_sim_result.metrics[metric]
                with_profile = with_profile_result.metrics.get(metric, "N/A")
                without_profile = without_profile_result.metrics.get(metric, "N/A")
                
                comparison[metric] = {
                    "original": original,
                    "with_profile": with_profile,
                    "without_profile": without_profile,
                    "with_profile_change_pct": (with_profile - original) / original * 100 if isinstance(with_profile, (int, float)) else "N/A",
                    "without_profile_change_pct": (without_profile - original) / original * 100 if isinstance(without_profile, (int, float)) else "N/A"
                }
                
                logger.info(f"Metric: {metric}")
                logger.info(f"  Original: {original}")
                logger.info(f"  With Profile: {with_profile} (change: {comparison[metric]['with_profile_change_pct'] if comparison[metric]['with_profile_change_pct'] != 'N/A' else 'N/A'}%)")
                logger.info(f"  Without Profile: {without_profile} (change: {comparison[metric]['without_profile_change_pct'] if comparison[metric]['without_profile_change_pct'] != 'N/A' else 'N/A'}%)")
        
        results[hardware_type] = comparison
    
    # Save results
    profile_path = OUTPUT_DIR / "hardware_profile_comparison.json"
    
    try:
        def convert_to_json_serializable(obj):
            if isinstance(obj, (np.float64, np.float32, np.int64, np.int32, np.bool_)):
                return obj.item()
            elif isinstance(obj, (list, tuple)):
                return [convert_to_json_serializable(i) for i in obj]
            elif isinstance(obj, dict):
                return {k: convert_to_json_serializable(v) for k, v in obj.items()}
            else:
                return obj
        
        with open(profile_path, 'w') as f:
            json.dump(convert_to_json_serializable(results), f, indent=2)
        
        logger.info(f"Saved hardware profile comparison to {profile_path}")
    except Exception as e:
        logger.error(f"Error saving hardware profile comparison: {e}")
    
    return results


def test_incremental_learning():
    """Test incremental learning with trend analysis."""
    logger.info("Testing incremental learning")
    
    # Generate multiple batches of data to simulate learning over time
    batches = []
    
    # Generate 3 batches with progressively increasing bias
    for i in range(3):
        # Adjust bias in generate_sample_data function
        simulation_results, hardware_results = generate_sample_data(num_samples=5)
        
        # Create validation results
        validation_results = []
        for j in range(len(simulation_results)):
            # Add metrics comparison
            metrics_comparison = {}
            
            for metric in ["throughput_items_per_second", "average_latency_ms", "memory_peak_mb", "power_consumption_w"]:
                if (metric in simulation_results[j].metrics and
                    metric in hardware_results[j].metrics):
                    sim_value = simulation_results[j].metrics[metric]
                    hw_value = hardware_results[j].metrics[metric]
                    
                    # Introduce progressive bias based on batch
                    if metric == "throughput_items_per_second":
                        # Increasingly overestimate throughput
                        sim_value *= (1.0 + (i * 0.1))
                    elif metric == "average_latency_ms":
                        # Increasingly underestimate latency
                        sim_value *= (1.0 - (i * 0.05))
                    
                    if sim_value is not None and hw_value is not None and hw_value != 0:
                        # Calculate error metrics
                        abs_error = abs(hw_value - sim_value)
                        rel_error = abs_error / abs(hw_value)
                        mape = rel_error * 100
                        
                        metrics_comparison[metric] = {
                            "simulation_value": sim_value,
                            "hardware_value": hw_value,
                            "absolute_error": abs_error,
                            "relative_error": rel_error,
                            "mape": mape
                        }
            
            validation_result = ValidationResult(
                simulation_result=simulation_results[j],
                hardware_result=hardware_results[j],
                metrics_comparison=metrics_comparison,
                validation_timestamp=simulation_results[j].timestamp,
                validation_version=f"test_v1.{i}",
                additional_metrics={}
            )
            
            validation_results.append(validation_result)
        
        batches.append(validation_results)
    
    # Initialize calibrators
    basic_calibrator = BasicSimulationCalibrator()
    incremental_calibrator = AdvancedSimulationCalibrator({
        "calibration_method": "incremental",
        "learning_rate": 0.8
    })
    
    # Initial parameters
    basic_params = {
        "calibration_version": "basic_v1.0",
        "calibration_method": "linear_scaling",
        "correction_factors": {}
    }
    
    incremental_params = {
        "calibration_version": "incremental_v1.0",
        "calibration_method": "incremental",
        "correction_factors": {},
        "calibration_history": []
    }
    
    # Apply calibration in sequence
    basic_history = []
    incremental_history = []
    
    for i, batch in enumerate(batches):
        logger.info(f"Processing batch {i+1}/{len(batches)}")
        
        # Apply basic calibration
        new_basic_params = basic_calibrator.calibrate(batch, basic_params)
        basic_params = new_basic_params
        
        # Apply incremental calibration
        new_incremental_params = incremental_calibrator.calibrate(batch, incremental_params)
        incremental_params = new_incremental_params
        
        # Create test simulation result
        test_sim_result = SimulationResult(
            model_id="bert-base-uncased",
            hardware_id="gpu_rtx3080",
            metrics={
                "throughput_items_per_second": 100.0 * (1.0 + (i * 0.1)),  # Introduce progressive bias
                "average_latency_ms": 50.0 * (1.0 - (i * 0.05)),
                "memory_peak_mb": 1000.0,
                "power_consumption_w": 100.0
            },
            batch_size=4,
            precision="fp32",
            timestamp=f"2025-03-15T{i:02d}:00:00",
            simulation_version=f"test_v1.{i}",
            additional_metadata={}
        )
        
        # Apply calibration to test result
        basic_result = basic_calibrator.apply_calibration(test_sim_result, basic_params)
        incremental_result = incremental_calibrator.apply_calibration(test_sim_result, incremental_params)
        
        # Compare results
        metrics_comparison = {}
        
        for metric in ["throughput_items_per_second", "average_latency_ms", "memory_peak_mb", "power_consumption_w"]:
            if metric in test_sim_result.metrics:
                original = test_sim_result.metrics[metric]
                basic_val = basic_result.metrics.get(metric, "N/A")
                incremental_val = incremental_result.metrics.get(metric, "N/A")
                
                metrics_comparison[metric] = {
                    "original": original,
                    "basic": basic_val,
                    "incremental": incremental_val,
                    "basic_change_pct": (basic_val - original) / original * 100 if isinstance(basic_val, (int, float)) else "N/A",
                    "incremental_change_pct": (incremental_val - original) / original * 100 if isinstance(incremental_val, (int, float)) else "N/A"
                }
                
                logger.info(f"Batch {i+1}, Metric: {metric}")
                logger.info(f"  Original: {original}")
                logger.info(f"  Basic: {basic_val} (change: {metrics_comparison[metric]['basic_change_pct'] if metrics_comparison[metric]['basic_change_pct'] != 'N/A' else 'N/A'}%)")
                logger.info(f"  Incremental: {incremental_val} (change: {metrics_comparison[metric]['incremental_change_pct'] if metrics_comparison[metric]['incremental_change_pct'] != 'N/A' else 'N/A'}%)")
        
        # Store calibration history
        basic_history.append({
            "batch": i+1,
            "params": basic_params,
            "test_result": metrics_comparison
        })
        
        incremental_history.append({
            "batch": i+1,
            "params": incremental_params,
            "test_result": metrics_comparison
        })
    
    # Save results
    incremental_path = OUTPUT_DIR / "incremental_learning_comparison.json"
    
    try:
        def convert_to_json_serializable(obj):
            if isinstance(obj, (np.float64, np.float32, np.int64, np.int32, np.bool_)):
                return obj.item()
            elif isinstance(obj, (list, tuple)):
                return [convert_to_json_serializable(i) for i in obj]
            elif isinstance(obj, dict):
                return {k: convert_to_json_serializable(v) for k, v in obj.items()}
            else:
                return obj
        
        results = {
            "basic_history": convert_to_json_serializable(basic_history),
            "incremental_history": convert_to_json_serializable(incremental_history)
        }
        
        with open(incremental_path, 'w') as f:
            json.dump(results, f, indent=2)
        
        logger.info(f"Saved incremental learning comparison to {incremental_path}")
    except Exception as e:
        logger.error(f"Error saving incremental learning comparison: {e}")
    
    return {
        "basic_history": basic_history,
        "incremental_history": incremental_history
    }


def main():
    """Main function to run tests."""
    logger.info("Testing Advanced Calibrator")
    
    # Create a results summary
    results_summary = {
        "basic_vs_advanced": test_basic_vs_advanced_calibration(),
        "multiple_methods": test_multiple_calibration_methods(),
        "hardware_profiles": test_hardware_specific_profiles(),
        "incremental_learning": test_incremental_learning()
    }
    
    # Print summary of results
    logger.info("Testing completed. Results saved to output directory.")
    
    # List output files
    output_files = list(OUTPUT_DIR.glob("*.json"))
    logger.info(f"Generated {len(output_files)} output files:")
    for file in output_files:
        logger.info(f"  - {file.name}")


if __name__ == "__main__":
    main()