# Distributed Testing Framework - Design Documentation

## Overview

The Distributed Testing Framework is a comprehensive system designed to execute tests across multiple worker nodes with different hardware capabilities. It enables efficient utilization of heterogeneous computing resources for testing AI models across various hardware platforms. The system uses a coordinator-worker architecture with a central coordinator managing test distribution and workers executing tests on different platforms.

## Architecture

### Core Components

1. **Coordinator**: Central server that manages test distribution, monitors workers, and collects results
2. **Workers**: Distributed nodes that execute tests on specific hardware platforms
3. **DuckDB Integration**: Efficient database system for storing test results
4. **Template Generator**: Creates tests dynamically from templates
5. **Cross-Platform Support**: Enables workers to run on different operating systems and environments
6. **Authentication**: Secure communication between coordinator and workers
7. **Resource Manager**: Dynamic allocation of resources based on workload

### Component Relationships

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚    Coordinator    â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚         DuckDB Integration           â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚             â”‚                         â”‚             â”‚
â–¼             â–¼                         â–¼             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Worker 1â”‚ â”‚ Worker 2â”‚     ...     â”‚Worker N-1â”‚ â”‚Worker N â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Communication Flow

1. **Test Submission**: Tests are submitted to the coordinator
2. **Task Distribution**: Coordinator assigns tests to appropriate workers
3. **Test Execution**: Workers execute tests and collect results
4. **Result Submission**: Results are sent back to the coordinator
5. **Result Storage**: Coordinator stores results in DuckDB database
6. **Reporting**: Coordinator generates reports from test results

## Component Details

### 1. DuckDB Integration (Phase 1 - Completed)

The DuckDB integration provides efficient storage and retrieval of test results with features including:

- **Connection Pooling**: Efficient management of database connections
- **Batch Processing**: Bulk insertion of test results for performance
- **Transaction Support**: ACID-compliant operations with rollback capability
- **Schema Validation**: Ensures data integrity with field validation
- **Result Querying**: Flexible SQL-based result analysis

Key components:
- `DuckDBResultProcessor`: Core class for database interactions
- `WorkerDuckDBIntegration`: Worker-side result management
- `CoordinatorDuckDBIntegration`: Coordinator-side result processing

### 2. Template-Based Test Generator (Phase 2 - Completed)

The template-based test generator creates tests dynamically from templates:

- **Model Family Detection**: Automatically identifies model types
- **Template Database**: Stores and retrieves templates
- **Dependency Tracking**: Manages execution order dependencies
- **Resource Estimation**: Calculates hardware requirements for tests
- **Priority Calculation**: Determines execution priority

Key components:
- `TestGeneratorIntegration`: Main class for test generation
- `TemplateDatabase`: Manages template storage and retrieval
- `DependencyTracker`: Handles test execution dependencies

### 3. Cross-Platform Worker Support (Phase 4 - Completed)

The Cross-Platform Worker Support module enables workers to run on different operating systems and container environments, providing a unified interface for platform-specific functionality while maximizing each platform's capabilities.

#### Core Features

- **Platform Detection**: Automatically identifies the operating system or container environment
- **Hardware Detection**: Discovers available CPU, memory, GPU, and disk resources with platform-specific methods
- **Deployment Script Generation**: Creates platform-specific deployment scripts (shell scripts, batch files, docker-compose)
- **Startup Script Generation**: Produces scripts with proper environment configuration and logging
- **Path Conversion**: Ensures correct path formats across different operating systems
- **Dependency Management**: Handles installation of dependencies with platform-specific approaches

#### Key Components

- **CrossPlatformWorkerSupport**: Main class that provides a unified interface for all platform-specific operations
- **PlatformHandler**: Abstract base class defining the interface for platform-specific handlers
- **Platform-Specific Handlers**:
  - **LinuxPlatformHandler**: Linux-specific implementation with shell script generation and Linux hardware detection
  - **WindowsPlatformHandler**: Windows-specific implementation with batch file generation and Windows-specific hardware detection
  - **MacOSPlatformHandler**: macOS-specific implementation with Apple Silicon detection and macOS-specific hardware detection
  - **ContainerPlatformHandler**: Container-specific implementation with Docker/Kubernetes support and resource limit detection

#### Hardware Detection Details

The hardware detection system uses platform-specific methods to gather detailed information:

**CPU Detection**:
- **Linux**: Uses `/proc/cpuinfo` to get CPU model and core count
- **Windows**: Uses WMI or `wmic` fallback to get CPU information
- **macOS**: Uses `sysctl` to get CPU details with Apple Silicon detection
- **Container**: Detects CPU limits from container environment

**Memory Detection**:
- **Linux**: Reads from `/proc/meminfo` to get total memory
- **Windows**: Uses `psutil` for memory information
- **macOS**: Uses `sysctl hw.memsize` to get total memory
- **Container**: Detects memory limits from cgroups

**GPU Detection**:
- **NVIDIA GPUs**: Uses `nvidia-smi` across platforms for detailed GPU information
- **AMD GPUs**: Uses `rocm-smi` on Linux/Windows for AMD GPU detection
- **Intel/Integrated GPUs**: Platform-specific detection methods
- **Apple GPUs**: Uses `system_profiler` on macOS for Metal-capable GPU detection

**Disk Detection**:
- All platforms use `shutil.disk_usage()` for consistent disk information

#### Deployment Script Generation

The module generates platform-specific deployment scripts:

**Linux Deployment**:
- Shell scripts with proper executable permissions
- Package manager detection (apt, yum, dnf)
- Python dependency installation
- Proper environment variable configuration
- Error handling and logging setup

**Windows Deployment**:
- Batch files with Windows-specific syntax
- PowerShell integration for advanced scenarios
- Path handling with backslashes
- Windows-specific environment variable setup

**macOS Deployment**:
- Shell scripts with macOS-specific configurations
- Homebrew integration for package management
- Apple Silicon compatibility handling
- macOS-specific path resolution

**Container Deployment**:
- `docker-compose.yml` for multi-container orchestration
- Dockerfile generation with proper base image selection
- Volume mounting for logs and persistent data
- Container-specific environment configuration
- Resource limit configuration

#### Startup Script Generation

The module creates platform-specific worker startup scripts:

**Linux/macOS Startup**:
- Proper shebang and executable permissions
- Environment variable configuration
- Log directory creation
- Background process execution with `nohup`
- PID file generation for process management
- Error redirection and log file setup

**Windows Startup**:
- Batch file format with Windows command syntax
- Windows environment variable setup
- Background execution with `start /b`
- Windows-specific error handling
- Log directory creation with Windows path syntax

**Container Startup**:
- Container-specific entrypoint
- Container environment variable configuration
- Proper log handling for containerized processes
- Resource limit adherence

#### Path Conversion

The path conversion system ensures consistent path formatting across platforms:

- **Linux/macOS**: Forward slash path separators (`/`)
- **Windows**: Backslash path separators (`\`)
- **Relative Path Handling**: Proper conversion of relative paths
- **Drive Letter Support**: Handles Windows drive letters appropriately
- **UNC Path Support**: Handles Windows UNC paths

#### Implementation Status

The Cross-Platform Worker Support module has been fully implemented with comprehensive testing and documentation. Key milestones include:

- Initial implementation: March 18, 2025
- Unit test implementation: March 19, 2025
- Example script creation: March 19, 2025
- Documentation completion: March 20, 2025
- Integration with worker system: March 20, 2025
- Final testing and validation: March 20, 2025

#### Integration with Other Components

The Cross-Platform Worker Support module integrates with:

- **Worker Node System**: Provides platform-specific implementations for worker nodes
- **Hardware Detection**: Supplies detailed hardware information for task matching
- **Resource Monitoring**: Enables resource tracking across diverse platforms
- **Deployment System**: Facilitates worker node setup across various environments
- **CI/CD Pipeline**: Supports testing across different operating systems

#### Code Examples

**Platform Detection**:
```python
support = CrossPlatformWorkerSupport()
platform_info = support.get_platform_info()
print(f"Platform: {platform_info['platform']}")
print(f"System: {platform_info['system']}")
print(f"Architecture: {platform_info['architecture']}")
```

**Hardware Detection**:
```python
support = CrossPlatformWorkerSupport()
hardware_info = support.detect_hardware()
print(f"CPU: {hardware_info['cpu']['model']} with {hardware_info['cpu']['cores']} cores")
print(f"Memory: {hardware_info['memory']['total_gb']} GB")
print(f"GPUs: {hardware_info['gpu']['count']}")
for i, device in enumerate(hardware_info['gpu']['devices']):
    print(f"  GPU {i}: {device['name']} ({device['type']})")
```

**Deployment Script Generation**:
```python
support = CrossPlatformWorkerSupport()
config = {
    "coordinator_url": "http://coordinator.example.com:8080",
    "api_key": "your_api_key",
    "worker_id": "worker_123"
}
script_path = support.create_deployment_script(config, "deploy_worker")
```

**Startup Script Generation**:
```python
support = CrossPlatformWorkerSupport()
startup_script = support.get_startup_script(
    coordinator_url="http://coordinator.example.com:8080",
    api_key="your_api_key",
    worker_id="worker_abc123"
)
```

**Path Conversion**:
```python
support = CrossPlatformWorkerSupport()
linux_path = "/home/user/data/models.json"
windows_path = "C:\\Users\\user\\Documents\\models.json"
platform_path1 = support.convert_path_for_platform(linux_path)
platform_path2 = support.convert_path_for_platform(windows_path)
```

#### Testing

The module includes comprehensive testing:

- **Platform Detection Tests**: Verify correct platform identification
- **Hardware Detection Tests**: Validate hardware detection across platforms
- **Deployment Script Tests**: Check script generation for all platforms
- **Startup Script Tests**: Verify startup script generation
- **Path Conversion Tests**: Ensure paths are correctly converted
- **Dependency Management Tests**: Validate dependency installation
- **Integration Tests**: Verify integration with worker management system
- **Error Handling Tests**: Ensure proper handling of error conditions
- **Mocked Platform Tests**: Test cross-platform functionality in a single environment

#### Documentation

Detailed documentation is provided in `CROSS_PLATFORM_WORKER_README.md` including:

- Complete API reference
- Usage examples for all functionality
- Platform-specific considerations
- Integration guidelines
- Best practices for cross-platform deployment

Supported platforms:
- Linux (Ubuntu, CentOS, Debian, etc.)
- Windows
- macOS
- Containers (Docker, Kubernetes)

### 4. JWT Authentication (Phase 3 - Deferred)

Enhanced JWT authentication is planned to provide secure communication between the coordinator and workers:

- **Role-Based Access Control**: Specific permissions based on roles
- **Token Refresh**: Mechanism for long-running workers
- **Token Revocation**: Security incident response capability
- **Fine-Grained Permissions**: Detailed operation-level access control
- **Secure Storage**: Proper token storage on worker nodes

### 5. CI/CD Pipeline Integration (Phase 5 - Planned)

CI/CD integration will automate distributed testing during development workflows:

- **GitHub Actions**: Workflow for distributed testing
- **Coordinator Deployment**: Automatic deployment of coordinator
- **Worker Provisioning**: Dynamic provisioning of worker nodes
- **Status Reporting**: Test status reporting to GitHub
- **PR Review Dashboards**: Comprehensive test result visualization

### 6. Dynamic Resource Management (Phase 6 - Planned)

Dynamic resource management will optimize resource allocation based on workload:

- **Resource Tracking**: Fine-grained tracking of available resources
- **Adaptive Scaling**: Dynamic scaling based on workload patterns
- **Worker Reassessment**: Periodic reassessment of worker capabilities
- **Cloud Integration**: Support for ephemeral workers on cloud platforms
- **Reservation Tracking**: Resource reservation and release tracking

## Implementation Status

| Phase | Component | Status | Completion Date |
|-------|-----------|--------|-----------------|
| 1 | DuckDB Integration | âœ… Complete | March 15, 2025 |
| 2 | Template-Based Test Generator | âœ… Complete | March 16, 2025 |
| 3 | JWT Authentication | â¸ï¸ Deferred | - |
| 4 | Cross-Platform Worker Support | âœ… Complete | March 20, 2025 |
| 5 | CI/CD Pipeline Integration | âœ… Complete | March 27, 2025 |
| 6 | Dynamic Resource Management | ğŸ“… Planned | Target: April 5, 2025 |

## Usage Examples

### DuckDB Integration

```python
from duckdb_api.distributed_testing.worker_duckdb_integration import WorkerDuckDBIntegration

# Initialize worker-side integration
worker_db = WorkerDuckDBIntegration(db_path="./results.duckdb")

# Store test result
result = {
    "test_id": "test_123",
    "model_name": "bert-base-uncased",
    "hardware_type": "cuda",
    "success": True,
    "execution_time": 10.5,
    "memory_usage": 1.2
}
worker_db.store_result(result)

# Store batch of results
results = [result1, result2, result3]
worker_db.store_batch_results(results)
```

### Template Generator

```python
from duckdb_api.distributed_testing.test_generator_integration import TestGeneratorIntegration

# Initialize test generator
generator = TestGeneratorIntegration(template_db_path="./templates.duckdb", 
                                    coordinator_url="http://coordinator:8080")

# Generate and submit tests
results = generator.generate_and_submit_tests(
    model_name="bert-base-uncased",
    hardware_types=["cpu", "cuda", "rocm"],
    batch_sizes=[1, 4, 16]
)
```

### Cross-Platform Worker Support

```python
from duckdb_api.distributed_testing.cross_platform_worker_support import CrossPlatformWorkerSupport

# Initialize for the current platform
support = CrossPlatformWorkerSupport()

# Detect hardware capabilities
hardware_info = support.detect_hardware()
print(f"Platform: {hardware_info['platform']}")
print(f"CPU: {hardware_info['cpu']['cores']} cores")
print(f"Memory: {hardware_info['memory']['total_gb']} GB")

# Create deployment script
config = {
    "coordinator_url": "http://coordinator.example.com:8080",
    "api_key": "your_api_key",
    "worker_id": "worker_123"
}
script_path = support.create_deployment_script(config, "deploy_worker")

# Generate startup script
startup_script = support.get_startup_script(
    coordinator_url="http://coordinator.example.com:8080",
    api_key="your_api_key",
    worker_id="worker_abc123"
)
```

## Testing

Each component includes comprehensive testing:

1. **Unit Tests**: Verify individual component functionality
2. **Integration Tests**: Validate interaction between components
3. **End-to-End Tests**: Test complete workflows
4. **Fault Tolerance Tests**: Verify resilience during failures
5. **Load Tests**: Evaluate performance under high concurrency
6. **Cross-Platform Tests**: Verify functionality across environments

## Documentation

Detailed documentation is available for each component:

- **DISTRIBUTED_TESTING_INTEGRATION_PR.md**: Overall integration roadmap
- **CROSS_PLATFORM_WORKER_README.md**: Cross-platform support documentation
- **PHASE4_COMPLETION_SUMMARY.md**: Summary of Phase 4 completion
- **templates/*.md**: Documentation for template-based generation

## Future Enhancements

1. **Predictive Scheduling**: Use performance history to predict optimal test scheduling
2. **Advanced Load Balancing**: Intelligent load distribution based on worker capabilities
3. **Result Visualization**: Interactive dashboard for result analysis
4. **Test Prioritization**: Risk-based test prioritization for critical components
5. **Auto-Scaling**: Integration with cloud platforms for automatic worker scaling

## Conclusion

The Distributed Testing Framework provides a comprehensive solution for testing AI models across heterogeneous computing resources. With the completion of core components including DuckDB integration, template-based test generation, and cross-platform support, the framework is approaching a fully integrated state. The upcoming phases will focus on CI/CD integration and dynamic resource management to further enhance the system's capabilities.

The modular architecture ensures the system is maintainable and extensible, allowing for future enhancements. The focus on cross-platform compatibility enables the framework to utilize a wide range of computing resources efficiently, making it a versatile tool for comprehensive AI model testing.