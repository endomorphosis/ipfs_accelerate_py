#!/usr/bin/env python3
"""
Fix Generator Hardware Support

This script updates all test generators to have consistent hardware support code.
It applies the latest hardware detection templates to all generators:
- merged_test_generator.py
- fixed_merged_test_generator.py
- integrated_skillset_generator.py
- implementation_generator.py

This enhanced version includes:
1. Full WebNN and WebGPU support integration
2. Proper hardware detection for all 6 platforms (CPU, CUDA, OpenVINO, MPS, ROCm, WebNN, WebGPU)
3. Consistent hardware initialization methods across all generators
4. Comprehensive platform-specific handling for all 13 key model types
5. Support for the March 2025 optimizations:
   - WebGPU Compute Shader optimization for audio models
   - Parallel Model Loading for multimodal models
   - Shader Precompilation for faster startup

Usage:
  python fix_generator_hardware_support.py [--force]
"""

import os
import sys
import shutil
import re
import argparse
from datetime import datetime
from pathlib import Path

# Import template generator if available
try:
    from template_hardware_detection import (
        generate_hardware_detection_code,
        generate_hardware_init_methods,
        generate_creation_methods
    )
    HAS_HARDWARE_TEMPLATE = True
except ImportError:
    HAS_HARDWARE_TEMPLATE = False
    print("Warning: template_hardware_detection.py not found. Creating minimal implementation.")

# Paths
PROJECT_ROOT = Path(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
TEST_DIR = PROJECT_ROOT / "test"
GENERATOR_FILES = [
    TEST_DIR / "merged_test_generator.py",
    TEST_DIR / "fixed_merged_test_generator.py",
    TEST_DIR / "integrated_skillset_generator.py",
    TEST_DIR / "implementation_generator.py"
]

# Additional files for 2025 web platform support
WEB_PLATFORM_FILES = [
    TEST_DIR / "fixed_web_platform/unified_framework/platform_detector.py",
    TEST_DIR / "fixed_web_platform/webgpu_streaming_inference.py",
    TEST_DIR / "fixed_web_platform/unified_web_framework.py"
]

# Key model configuration with hardware support levels
KEY_MODEL_HARDWARE_CONFIG = {
    # Text models
    "bert": { # BERT model family (high priority)
        "cpu": "REAL",        # CPU support: fully implemented
        "cuda": "REAL",       # CUDA support: fully implemented
        "openvino": "REAL",   # OpenVINO support: fully implemented
        "mps": "REAL",        # MPS (Apple Silicon) support: fully implemented
        "rocm": "REAL",       # ROCm (AMD) support: fully implemented
        "webnn": "REAL",      # WebNN support: fully implemented
        "webgpu": "REAL"      # WebGPU support: fully implemented
    },
    "t5": { # T5 model family (high priority)
        "cpu": "REAL",        # CPU support: fully implemented
        "cuda": "REAL",       # CUDA support: fully implemented
        "openvino": "REAL",   # OpenVINO support: fully implemented
        "mps": "REAL",        # MPS (Apple Silicon) support: fully implemented
        "rocm": "REAL",       # ROCm (AMD) support: fully implemented
        "webnn": "REAL",      # WebNN support: fully implemented
        "webgpu": "REAL"      # WebGPU support: fully implemented
    },
    "llama": { # LLAMA/LLM model family (high priority)
        "cpu": "REAL",        # CPU support: fully implemented
        "cuda": "REAL",       # CUDA support: fully implemented
        "openvino": "REAL",   # OpenVINO support: fully implemented
        "mps": "REAL",        # MPS (Apple) support: fully implemented
        "rocm": "REAL",       # ROCm (AMD) support: fully implemented
        "webnn": "SIMULATION", # WebNN support: simulation mode
        "webgpu": "SIMULATION" # WebGPU support: simulation mode
    },
    
    # Vision models
    "vit": { # Vision Transformer model family (high priority)
        "cpu": "REAL",        # CPU support: fully implemented
        "cuda": "REAL",       # CUDA support: fully implemented
        "openvino": "REAL",   # OpenVINO support: fully implemented
        "mps": "REAL",        # MPS (Apple) support: fully implemented
        "rocm": "REAL",       # ROCm (AMD) support: fully implemented
        "webnn": "REAL",      # WebNN support: fully implemented 
        "webgpu": "REAL"      # WebGPU support: fully implemented
    },
    "clip": { # CLIP model family (high priority)
        "cpu": "REAL",        # CPU support: fully implemented
        "cuda": "REAL",       # CUDA support: fully implemented
        "openvino": "REAL",   # OpenVINO support: fully implemented
        "mps": "REAL",        # MPS (Apple) support: fully implemented
        "rocm": "REAL",       # ROCm (AMD) support: fully implemented
        "webnn": "REAL",      # WebNN support: fully implemented
        "webgpu": "REAL"      # WebGPU support: fully implemented
    },
    "detr": { # DETR object detection model family (high priority)
        "cpu": "REAL",        # CPU support: fully implemented 
        "cuda": "REAL",       # CUDA support: fully implemented
        "openvino": "REAL",   # OpenVINO support: fully implemented
        "mps": "REAL",        # MPS (Apple) support: fully implemented
        "rocm": "REAL",       # ROCm (AMD) support: fully implemented
        "webnn": "SIMULATION", # WebNN support: simulation mode
        "webgpu": "SIMULATION" # WebGPU support: simulation mode
    },
    
    # Audio models
    "clap": { # CLAP model family (high priority)
        "cpu": "REAL",        # CPU support: fully implemented
        "cuda": "REAL",       # CUDA support: fully implemented
        "openvino": "REAL",   # OpenVINO support: fully implemented
        "mps": "REAL",        # MPS (Apple) support: fully implemented
        "rocm": "REAL",       # ROCm (AMD) support: fully implemented
        "webnn": "SIMULATION", # WebNN support: simulation mode
        "webgpu": "SIMULATION" # WebGPU support: simulation mode
    },
    "wav2vec2": { # Wav2Vec2 model family (high priority)
        "cpu": "REAL",        # CPU support: fully implemented
        "cuda": "REAL",       # CUDA support: fully implemented
        "openvino": "REAL",   # OpenVINO support: fully implemented
        "mps": "REAL",        # MPS (Apple) support: fully implemented
        "rocm": "REAL",       # ROCm (AMD) support: fully implemented
        "webnn": "SIMULATION", # WebNN support: simulation mode
        "webgpu": "SIMULATION" # WebGPU support: simulation mode
    },
    "whisper": { # Whisper model family (high priority)
        "cpu": "REAL",        # CPU support: fully implemented
        "cuda": "REAL",       # CUDA support: fully implemented
        "openvino": "REAL",   # OpenVINO support: fully implemented
        "mps": "REAL",        # MPS (Apple) support: fully implemented
        "rocm": "REAL",       # ROCm (AMD) support: fully implemented
        "webnn": "SIMULATION", # WebNN support: simulation mode
        "webgpu": "SIMULATION" # WebGPU support: simulation mode
    },
    
    # Multimodal models
    "llava": { # LLaVA model family (high priority)
        "cpu": "REAL",        # CPU support: fully implemented
        "cuda": "REAL",       # CUDA support: fully implemented
        "openvino": "SIMULATION", # OpenVINO support: simulation mode
        "mps": "SIMULATION",   # MPS (Apple) support: simulation mode
        "rocm": "SIMULATION",  # ROCm (AMD) support: simulation mode
        "webnn": "SIMULATION", # WebNN support: simulation mode
        "webgpu": "SIMULATION" # WebGPU support: simulation mode
    },
    "llava_next": { # LLaVA-Next model family (high priority)
        "cpu": "REAL",        # CPU support: fully implemented
        "cuda": "REAL",       # CUDA support: fully implemented
        "openvino": "SIMULATION", # OpenVINO support: simulation mode
        "mps": "SIMULATION",  # MPS (Apple) support: simulation mode
        "rocm": "SIMULATION", # ROCm (AMD) support: simulation mode
        "webnn": "SIMULATION", # WebNN support: simulation mode
        "webgpu": "SIMULATION" # WebGPU support: simulation mode
    },
    "xclip": { # XCLIP model family (high priority)
        "cpu": "REAL",        # CPU support: fully implemented
        "cuda": "REAL",       # CUDA support: fully implemented
        "openvino": "REAL",   # OpenVINO support: fully implemented
        "mps": "REAL",        # MPS (Apple) support: fully implemented
        "rocm": "REAL",       # ROCm (AMD) support: fully implemented
        "webnn": "SIMULATION", # WebNN support: simulation mode
        "webgpu": "SIMULATION" # WebGPU support: simulation mode
    },
    
    # Large model families with multiple variants
    "qwen2": { # Qwen2 model family (high priority)
        "cpu": "REAL",        # CPU support: fully implemented
        "cuda": "REAL",       # CUDA support: fully implemented
        "openvino": "SIMULATION", # OpenVINO support: simulation mode
        "mps": "SIMULATION",  # MPS (Apple) support: simulation mode
        "rocm": "SIMULATION", # ROCm (AMD) support: simulation mode
        "webnn": "SIMULATION", # WebNN support: simulation mode
        "webgpu": "SIMULATION" # WebGPU support: simulation mode
    }
}

# MODALITY_TYPES for proper hardware support mapping
MODALITY_TYPES = {
    "text": ["bert", "gpt2", "t5", "roberta", "distilbert", "bart", "llama", "mistral", "phi", 
             "mixtral", "gemma", "qwen2", "deepseek", "falcon", "mpt", "chatglm", "bloom", 
             "command-r", "orca3", "olmo", "starcoder", "codellama"],
    "vision": ["vit", "deit", "swin", "convnext", "resnet", "dinov2", "detr", "sam", "segformer", 
               "mask2former", "conditional_detr", "dino", "zoedepth", "depth-anything", "yolos"],
    "audio": ["wav2vec2", "whisper", "hubert", "clap", "audioldm2", "musicgen", "bark", 
              "encodec", "univnet", "speecht5", "qwen2-audio"],
    "multimodal": ["clip", "llava", "blip", "flava", "owlvit", "git", "pali-gemma", "idefics",
                   "llava-next", "flamingo", "blip2", "kosmos-2", "siglip", "chinese-clip", 
                   "instructblip", "qwen2-vl", "cogvlm2", "vilt", "imagebind"],
    "video": ["xclip", "videomae", "vivit", "movinet", "videobert", "videogpt"]
}

# Create backup of a file
def backup_file(file_path):
    """Create a backup of a file."""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    backup_path = file_path.with_suffix(f".py.bak_{timestamp}")
    shutil.copy2(file_path, backup_path)
    print(f"Created backup of {file_path} at {backup_path}")
    return backup_path

# Enhanced hardware detection code if template_hardware_detection.py is not available
def enhanced_hardware_detection_code():
    """Return an enhanced hardware detection code block."""
    return """
# Hardware Detection with Web Platform Support
import os
import sys
import importlib.util
import logging
from typing import Dict, Any, List

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger("hardware_detection")

# Try to import torch first (needed for CUDA/ROCm/MPS)
try:
    import torch
    HAS_TORCH = True
except ImportError:
    from unittest.mock import MagicMock
    torch = MagicMock()
    HAS_TORCH = False
    logger.warning("torch not available, using mock")

# Initialize hardware capability flags
HAS_CUDA = False
HAS_ROCM = False
HAS_MPS = False
HAS_OPENVINO = False
HAS_WEBNN = False
HAS_WEBGPU = False

# CUDA detection
if HAS_TORCH:
    HAS_CUDA = torch.cuda.is_available()
    
    # ROCm detection
    if HAS_CUDA and hasattr(torch, '_C') and hasattr(torch._C, '_rocm_version'):
        HAS_ROCM = True
    elif 'ROCM_HOME' in os.environ:
        HAS_ROCM = True
    
    # Apple MPS detection
    if hasattr(torch, "mps") and hasattr(torch.mps, "is_available"):
        HAS_MPS = torch.mps.is_available()

# OpenVINO detection
HAS_OPENVINO = importlib.util.find_spec("openvino") is not None

# WebNN detection (browser API or simulation)
HAS_WEBNN = (
    importlib.util.find_spec("webnn") is not None or 
    importlib.util.find_spec("webnn_js") is not None or
    "WEBNN_AVAILABLE" in os.environ or
    "WEBNN_ENABLED" in os.environ or
    "WEBNN_SIMULATION" in os.environ
)

# WebGPU detection (browser API or simulation)
HAS_WEBGPU = (
    importlib.util.find_spec("webgpu") is not None or
    importlib.util.find_spec("wgpu") is not None or
    "WEBGPU_AVAILABLE" in os.environ or
    "WEBGPU_ENABLED" in os.environ or
    "WEBGPU_SIMULATION" in os.environ
)

# Web platform optimizations
HAS_WEBGPU_COMPUTE_SHADERS = (
    "WEBGPU_COMPUTE_SHADERS_ENABLED" in os.environ or
    "WEBGPU_COMPUTE_SHADERS" in os.environ
)

HAS_PARALLEL_LOADING = (
    "WEB_PARALLEL_LOADING_ENABLED" in os.environ or
    "PARALLEL_LOADING_ENABLED" in os.environ
)

HAS_SHADER_PRECOMPILE = (
    "WEBGPU_SHADER_PRECOMPILE_ENABLED" in os.environ or
    "WEBGPU_SHADER_PRECOMPILE" in os.environ
)

# Hardware detection function for comprehensive hardware info
def detect_all_hardware():
    \"\"\"Detect available hardware platforms on the current system.\"\"\"
    capabilities = {
        "cpu": {
            "detected": True,
            "version": None,
            "count": os.cpu_count()
        },
        "cuda": {
            "detected": False,
            "version": None,
            "device_count": 0,
            "devices": []
        },
        "mps": {
            "detected": False,
            "device": None
        },
        "openvino": {
            "detected": False,
            "version": None,
            "devices": []
        },
        "rocm": {
            "detected": False,
            "version": None,
            "device_count": 0
        },
        "webnn": {
            "detected": False,
            "simulation": True
        },
        "webgpu": {
            "detected": False,
            "simulation": True,
            "compute_shaders": HAS_WEBGPU_COMPUTE_SHADERS,
            "parallel_loading": HAS_PARALLEL_LOADING,
            "shader_precompile": HAS_SHADER_PRECOMPILE
        }
    }
    
    # CUDA capabilities
    if HAS_TORCH and HAS_CUDA:
        capabilities["cuda"]["detected"] = True
        capabilities["cuda"]["device_count"] = torch.cuda.device_count()
        capabilities["cuda"]["version"] = torch.version.cuda if hasattr(torch.version, "cuda") else None
        
        # Get device info
        for i in range(torch.cuda.device_count()):
            capabilities["cuda"]["devices"].append({
                "id": i,
                "name": torch.cuda.get_device_name(i),
                "total_memory_mb": torch.cuda.get_device_properties(i).total_memory / (1024 * 1024)
            })
    
    # MPS capabilities (Apple Silicon)
    capabilities["mps"]["detected"] = HAS_MPS
    if HAS_MPS:
        import platform
        capabilities["mps"]["device"] = platform.processor()
    
    # OpenVINO capabilities
    capabilities["openvino"]["detected"] = HAS_OPENVINO
    if HAS_OPENVINO:
        try:
            import openvino
            capabilities["openvino"]["version"] = openvino.__version__ if hasattr(openvino, "__version__") else "Unknown"
            
            # Get available devices
            try:
                # Try new API first (recommended since 2025.0)
                try:
                    from openvino import Core
                except ImportError:
                    # Fall back to legacy API
                    from openvino.runtime import Core
                
                core = Core()
                devices = core.available_devices
                capabilities["openvino"]["devices"] = devices
            except:
                pass
        except ImportError:
            pass
    
    # ROCm capabilities
    capabilities["rocm"]["detected"] = HAS_ROCM
    if HAS_ROCM:
        capabilities["rocm"]["device_count"] = torch.cuda.device_count() if HAS_CUDA else 0
        if hasattr(torch, "version") and hasattr(torch.version, "hip"):
            capabilities["rocm"]["version"] = torch.version.hip
    
    # WebNN capabilities
    capabilities["webnn"]["detected"] = HAS_WEBNN
    capabilities["webnn"]["simulation"] = not (
        importlib.util.find_spec("webnn") is not None or 
        "WEBNN_AVAILABLE" in os.environ
    )
    
    # WebGPU capabilities
    capabilities["webgpu"]["detected"] = HAS_WEBGPU
    capabilities["webgpu"]["simulation"] = not (
        importlib.util.find_spec("webgpu") is not None or 
        importlib.util.find_spec("wgpu") is not None or 
        "WEBGPU_AVAILABLE" in os.environ
    )
    
    return capabilities

# Get hardware capabilities
HW_CAPABILITIES = detect_all_hardware()

# For convenience in conditional code
HAS_HARDWARE_DETECTION = True
"""
    
def create_hardware_template_integration(file_path, text, vision, audio, multimodal, video):
    """Create a specific hardware template for each modality."""
    modality_template = f"""
# Enhanced Hardware Templates - Auto-generated with fix_generator_hardware_support.py

# Text Model Template (BERT, T5, LLAMA, etc.)
text_hardware_template = \"\"\"
{text[:200]}
...
\"\"\"

# Vision Model Template (ViT, CLIP, DETR, etc.)
vision_hardware_template = \"\"\"
{vision[:200]}
...
\"\"\"

# Audio Model Template (Whisper, WAV2VEC2, CLAP, etc.)
audio_hardware_template = \"\"\"
{audio[:200]}
...
\"\"\"

# Multimodal Model Template (LLAVA, LLAVA-Next, etc.)
multimodal_hardware_template = \"\"\"
{multimodal[:200]}
...
\"\"\"

# Video Model Template (XCLIP, etc.)
video_hardware_template = \"\"\"
{video[:200]}
...
\"\"\"

# Map model categories to templates
hardware_template_map = {{
    "text": text_hardware_template,
    "vision": vision_hardware_template,
    "audio": audio_hardware_template,
    "multimodal": multimodal_hardware_template,
    "video": video_hardware_template
}}

# Key Models Map - Maps key model prefixes to proper categories
key_models_mapping = {{
    "bert": "text", 
    "gpt2": "text",
    "t5": "text",
    "llama": "text",
    "vit": "vision",
    "clip": "vision",
    "whisper": "audio",
    "wav2vec2": "audio",
    "clap": "audio",
    "detr": "vision",
    "llava": "multimodal",
    "llava_next": "multimodal",
    "qwen2": "text",
    "xclip": "video"
}}

# Hardware support matrix for key models
KEY_MODEL_HARDWARE_MAP = {{
    # Text models
    "bert": {{ 
        "cpu": "REAL",        # CPU support: fully implemented
        "cuda": "REAL",       # CUDA support: fully implemented
        "openvino": "REAL",   # OpenVINO support: fully implemented
        "mps": "REAL",        # MPS (Apple Silicon) support: fully implemented
        "rocm": "REAL",       # ROCm (AMD) support: fully implemented
        "webnn": "REAL",      # WebNN support: fully implemented
        "webgpu": "REAL"      # WebGPU support: fully implemented
    }},
    "t5": {{ 
        "cpu": "REAL",        # CPU support: fully implemented
        "cuda": "REAL",       # CUDA support: fully implemented
        "openvino": "REAL",   # OpenVINO support: fully implemented
        "mps": "REAL",        # MPS (Apple Silicon) support: fully implemented
        "rocm": "REAL",       # ROCm (AMD) support: fully implemented
        "webnn": "REAL",      # WebNN support: fully implemented
        "webgpu": "REAL"      # WebGPU support: fully implemented
    }},
    "llama": {{ 
        "cpu": "REAL",        # CPU support: fully implemented
        "cuda": "REAL",       # CUDA support: fully implemented
        "openvino": "REAL",   # OpenVINO support: fully implemented
        "mps": "REAL",        # MPS (Apple) support: fully implemented
        "rocm": "REAL",       # ROCm (AMD) support: fully implemented
        "webnn": "SIMULATION", # WebNN support: simulation mode
        "webgpu": "SIMULATION" # WebGPU support: simulation mode
    }},
    "vit": {{ 
        "cpu": "REAL",        # CPU support: fully implemented
        "cuda": "REAL",       # CUDA support: fully implemented
        "openvino": "REAL",   # OpenVINO support: fully implemented
        "mps": "REAL",        # MPS (Apple) support: fully implemented
        "rocm": "REAL",       # ROCm (AMD) support: fully implemented
        "webnn": "REAL",      # WebNN support: fully implemented 
        "webgpu": "REAL"      # WebGPU support: fully implemented
    }},
    "clip": {{ 
        "cpu": "REAL",        # CPU support: fully implemented
        "cuda": "REAL",       # CUDA support: fully implemented
        "openvino": "REAL",   # OpenVINO support: fully implemented
        "mps": "REAL",        # MPS (Apple) support: fully implemented
        "rocm": "REAL",       # ROCm (AMD) support: fully implemented
        "webnn": "REAL",      # WebNN support: fully implemented
        "webgpu": "REAL"      # WebGPU support: fully implemented
    }},
    "detr": {{ 
        "cpu": "REAL",        # CPU support: fully implemented 
        "cuda": "REAL",       # CUDA support: fully implemented
        "openvino": "REAL",   # OpenVINO support: fully implemented
        "mps": "REAL",        # MPS (Apple) support: fully implemented
        "rocm": "REAL",       # ROCm (AMD) support: fully implemented
        "webnn": "SIMULATION", # WebNN support: simulation mode
        "webgpu": "SIMULATION" # WebGPU support: simulation mode
    }},
    "clap": {{ 
        "cpu": "REAL",        # CPU support: fully implemented
        "cuda": "REAL",       # CUDA support: fully implemented
        "openvino": "REAL",   # OpenVINO support: fully implemented
        "mps": "REAL",        # MPS (Apple) support: fully implemented
        "rocm": "REAL",       # ROCm (AMD) support: fully implemented
        "webnn": "SIMULATION", # WebNN support: simulation mode
        "webgpu": "SIMULATION" # WebGPU support: simulation mode
    }},
    "wav2vec2": {{ 
        "cpu": "REAL",        # CPU support: fully implemented
        "cuda": "REAL",       # CUDA support: fully implemented
        "openvino": "REAL",   # OpenVINO support: fully implemented
        "mps": "REAL",        # MPS (Apple) support: fully implemented
        "rocm": "REAL",       # ROCm (AMD) support: fully implemented
        "webnn": "SIMULATION", # WebNN support: simulation mode
        "webgpu": "SIMULATION" # WebGPU support: simulation mode
    }},
    "whisper": {{ 
        "cpu": "REAL",        # CPU support: fully implemented
        "cuda": "REAL",       # CUDA support: fully implemented
        "openvino": "REAL",   # OpenVINO support: fully implemented
        "mps": "REAL",        # MPS (Apple) support: fully implemented
        "rocm": "REAL",       # ROCm (AMD) support: fully implemented
        "webnn": "SIMULATION", # WebNN support: simulation mode
        "webgpu": "SIMULATION" # WebGPU support: simulation mode
    }},
    "llava": {{ 
        "cpu": "REAL",        # CPU support: fully implemented
        "cuda": "REAL",       # CUDA support: fully implemented
        "openvino": "SIMULATION", # OpenVINO support: simulation mode
        "mps": "SIMULATION",   # MPS (Apple) support: simulation mode
        "rocm": "SIMULATION",  # ROCm (AMD) support: simulation mode
        "webnn": "SIMULATION", # WebNN support: simulation mode
        "webgpu": "SIMULATION" # WebGPU support: simulation mode
    }},
    "llava_next": {{ 
        "cpu": "REAL",        # CPU support: fully implemented
        "cuda": "REAL",       # CUDA support: fully implemented
        "openvino": "SIMULATION", # OpenVINO support: simulation mode
        "mps": "SIMULATION",  # MPS (Apple) support: simulation mode
        "rocm": "SIMULATION", # ROCm (AMD) support: simulation mode
        "webnn": "SIMULATION", # WebNN support: simulation mode
        "webgpu": "SIMULATION" # WebGPU support: simulation mode
    }},
    "xclip": {{ 
        "cpu": "REAL",        # CPU support: fully implemented
        "cuda": "REAL",       # CUDA support: fully implemented
        "openvino": "REAL",   # OpenVINO support: fully implemented
        "mps": "REAL",        # MPS (Apple) support: fully implemented
        "rocm": "REAL",       # ROCm (AMD) support: fully implemented
        "webnn": "SIMULATION", # WebNN support: simulation mode
        "webgpu": "SIMULATION" # WebGPU support: simulation mode
    }},
    "qwen2": {{ 
        "cpu": "REAL",        # CPU support: fully implemented
        "cuda": "REAL",       # CUDA support: fully implemented
        "openvino": "SIMULATION", # OpenVINO support: simulation mode
        "mps": "SIMULATION",  # MPS (Apple) support: simulation mode
        "rocm": "SIMULATION", # ROCm (AMD) support: simulation mode
        "webnn": "SIMULATION", # WebNN support: simulation mode
        "webgpu": "SIMULATION" # WebGPU support: simulation mode
    }}
}}

# Function to detect modality from model name
def detect_model_modality(model_name):
    \"\"\"Detect which modality a model belongs to based on its name.\"\"\"
    # Check key models first
    model_base = model_name.split("-")[0].lower() if "-" in model_name else model_name.lower()
    
    # Direct mapping from key models
    if model_base in key_models_mapping:
        return key_models_mapping[model_base]
    
    # Check for common patterns in model names
    model_lower = model_name.lower()
    
    # Text models
    if any(text_model in model_lower for text_model in {text_models}):
        return "text"
    
    # Vision models
    if any(vision_model in model_lower for vision_model in {vision_models}):
        return "vision"
    
    # Audio models
    if any(audio_model in model_lower for audio_model in {audio_models}):
        return "audio"
    
    # Multimodal models
    if any(mm_model in model_lower for mm_model in {multimodal_models}):
        return "multimodal"
    
    # Video models
    if any(video_model in model_lower for video_model in {video_models}):
        return "video"
    
    # Default to text as fallback
    return "text"

# Function to get hardware template for a model
def get_hardware_template_for_model(model_name):
    \"\"\"Get the appropriate hardware template for a model.\"\"\"
    modality = detect_model_modality(model_name)
    return hardware_template_map.get(modality, text_hardware_template)

# Function to get hardware map for a model
def get_hardware_map_for_model(model_name):
    \"\"\"Get the appropriate hardware map for a model.\"\"\"
    # Check if this is a known key model
    model_base = model_name.split("-")[0].lower() if "-" in model_name else model_name.lower()
    
    # Direct mapping from key models
    if model_base in KEY_MODEL_HARDWARE_MAP:
        return KEY_MODEL_HARDWARE_MAP[model_base]
    
    # If not a key model, use modality to create default map
    modality = detect_model_modality(model_name)
    
    # Default hardware map based on modality
    default_map = {
        "text": {
            "cpu": "REAL", "cuda": "REAL", "openvino": "REAL", 
            "mps": "REAL", "rocm": "REAL", "webnn": "REAL", "webgpu": "REAL"
        },
        "vision": {
            "cpu": "REAL", "cuda": "REAL", "openvino": "REAL", 
            "mps": "REAL", "rocm": "REAL", "webnn": "REAL", "webgpu": "REAL"
        },
        "audio": {
            "cpu": "REAL", "cuda": "REAL", "openvino": "REAL", 
            "mps": "REAL", "rocm": "REAL", "webnn": "SIMULATION", "webgpu": "SIMULATION"
        },
        "multimodal": {
            "cpu": "REAL", "cuda": "REAL", "openvino": "SIMULATION", 
            "mps": "SIMULATION", "rocm": "SIMULATION", "webnn": "SIMULATION", "webgpu": "SIMULATION"
        },
        "video": {
            "cpu": "REAL", "cuda": "REAL", "openvino": "REAL", 
            "mps": "REAL", "rocm": "REAL", "webnn": "SIMULATION", "webgpu": "SIMULATION"
        }
    }
    
    return default_map.get(modality, default_map["text"])
"""

    # Create template lists for different modalities
    text_models = str([m.lower() for m in MODALITY_TYPES["text"][:8]])
    vision_models = str([m.lower() for m in MODALITY_TYPES["vision"][:8]])
    audio_models = str([m.lower() for m in MODALITY_TYPES["audio"][:8]])
    multimodal_models = str([m.lower() for m in MODALITY_TYPES["multimodal"][:8]])
    video_models = str([m.lower() for m in MODALITY_TYPES["video"][:5]])
    
    # Format the template with model lists
    formatted_template = modality_template.format(
        text_models=text_models,
        vision_models=vision_models,
        audio_models=audio_models,
        multimodal_models=multimodal_models,
        video_models=video_models
    )
    
    # Write to file
    template_file = TEST_DIR / "hardware_template_integration.py"
    with open(template_file, 'w') as f:
        f.write(formatted_template)
    
    print(f"Created hardware template integration file at {template_file}")
    return template_file

# Generate web platform optimization code
def generate_web_platform_optimization_code():
    """Generate code for the March 2025 web platform optimizations."""
    optimization_code = """
# Web Platform Optimizations - March 2025
# These optimizations are enabled by environment variables:
# - WEBGPU_COMPUTE_SHADERS_ENABLED: Enables compute shader optimizations for audio models
# - WEB_PARALLEL_LOADING_ENABLED: Enables parallel loading for multimodal models
# - WEBGPU_SHADER_PRECOMPILE_ENABLED: Enables shader precompilation for faster startup

def apply_web_platform_optimizations(model_type, implementation_type=None):
    \"\"\"
    Apply web platform optimizations based on model type and environment settings.
    
    Args:
        model_type: Type of model (audio, multimodal, etc.)
        implementation_type: Implementation type (WebNN, WebGPU)
        
    Returns:
        Dict of optimization settings
    \"\"\"
    optimizations = {
        "compute_shaders": False,
        "parallel_loading": False,
        "shader_precompile": False
    }
    
    # Check for optimization environment flags
    compute_shaders_enabled = (
        os.environ.get("WEBGPU_COMPUTE_SHADERS_ENABLED", "0") == "1" or
        os.environ.get("WEBGPU_COMPUTE_SHADERS", "0") == "1"
    )
    
    parallel_loading_enabled = (
        os.environ.get("WEB_PARALLEL_LOADING_ENABLED", "0") == "1" or
        os.environ.get("PARALLEL_LOADING_ENABLED", "0") == "1"
    )
    
    shader_precompile_enabled = (
        os.environ.get("WEBGPU_SHADER_PRECOMPILE_ENABLED", "0") == "1" or
        os.environ.get("WEBGPU_SHADER_PRECOMPILE", "0") == "1"
    )
    
    # Enable all optimizations flag
    if os.environ.get("WEB_ALL_OPTIMIZATIONS", "0") == "1":
        compute_shaders_enabled = True
        parallel_loading_enabled = True
        shader_precompile_enabled = True
    
    # Only apply WebGPU compute shaders for audio models
    if compute_shaders_enabled and implementation_type == "WebGPU" and model_type == "audio":
        optimizations["compute_shaders"] = True
    
    # Only apply parallel loading for multimodal models
    if parallel_loading_enabled and model_type == "multimodal":
        optimizations["parallel_loading"] = True
    
    # Apply shader precompilation for most model types with WebGPU
    if shader_precompile_enabled and implementation_type == "WebGPU":
        optimizations["shader_precompile"] = True
    
    return optimizations

def detect_browser_for_optimizations():
    \"\"\"
    Detect browser type for optimizations, particularly for Firefox WebGPU compute shader optimizations.
    
    Returns:
        Dict with browser information
    \"\"\"
    # Start with default (simulation environment)
    browser_info = {
        "is_browser": False,
        "browser_type": "unknown",
        "is_firefox": False,
        "is_chrome": False,
        "is_edge": False,
        "is_safari": False,
        "supports_compute_shaders": False,
        "workgroup_size": [128, 1, 1]  # Default workgroup size
    }
    
    # Try to detect browser environment
    try:
        import js
        if hasattr(js, 'navigator'):
            browser_info["is_browser"] = True
            user_agent = js.navigator.userAgent.lower()
            
            # Detect browser type
            if "firefox" in user_agent:
                browser_info["browser_type"] = "firefox"
                browser_info["is_firefox"] = True
                browser_info["supports_compute_shaders"] = True
                browser_info["workgroup_size"] = [256, 1, 1]  # Firefox optimized workgroup size
            elif "chrome" in user_agent:
                browser_info["browser_type"] = "chrome"
                browser_info["is_chrome"] = True
                browser_info["supports_compute_shaders"] = True
            elif "edg" in user_agent:
                browser_info["browser_type"] = "edge"
                browser_info["is_edge"] = True
                browser_info["supports_compute_shaders"] = True
            elif "safari" in user_agent:
                browser_info["browser_type"] = "safari"
                browser_info["is_safari"] = True
                browser_info["supports_compute_shaders"] = False  # Safari has limited compute shader support
    except (ImportError, AttributeError):
        # Not in a browser environment
        pass
    
    # Check environment variables for browser simulation
    if os.environ.get("SIMULATE_FIREFOX", "0") == "1":
        browser_info["browser_type"] = "firefox"
        browser_info["is_firefox"] = True
        browser_info["supports_compute_shaders"] = True
        browser_info["workgroup_size"] = [256, 1, 1]
    
    return browser_info
"""
    return optimization_code

# Apply hardware support to a generator file
def update_generator_hardware_support(file_path, force=False):
    """Update a generator file with proper hardware support."""
    if not file_path.exists():
        print(f"File not found: {file_path}")
        return False
    
    # Create backup
    backup_path = backup_file(file_path)
    
    try:
        # Read content
        with open(file_path, 'r') as f:
            content = f.read()
        
        # Check if hardware detection is already present and we're not forcing update
        if not force and "HAS_HARDWARE_DETECTION" in content and "HAS_WEBGPU" in content:
            print(f"Hardware detection already present in {file_path} (use --force to update anyway)")
            return True
        
        # Get hardware detection code
        if HAS_HARDWARE_TEMPLATE:
            hardware_detection = generate_hardware_detection_code()
        else:
            hardware_detection = enhanced_hardware_detection_code()
        
        # Add web platform optimization code
        web_optimizations = generate_web_platform_optimization_code()
        combined_code = hardware_detection + "\n" + web_optimizations
        
        # Find a suitable insertion point after imports
        import_section_end = content.find("\n\n", content.find("import "))
        if import_section_end == -1:
            import_section_end = content.find("import ") + 100  # Rough estimate
        
        # Insert hardware detection code
        new_content = content[:import_section_end] + "\n" + combined_code + content[import_section_end:]
        
        # Add the KEY_MODEL_HARDWARE_CONFIG if it's not already present
        if "KEY_MODEL_HARDWARE_MAP" not in new_content and "KEY_MODEL_HARDWARE_CONFIG" not in new_content:
            # Find a good place to insert the configuration
            config_pos = new_content.find("def main(")
            if config_pos == -1:
                config_pos = new_content.find("if __name__ ==")
            
            if config_pos != -1:
                # Insert before the main function or __name__ check
                config_str = "\n# Define key models that need special handling with hardware backends\n"
                config_str += "KEY_MODEL_HARDWARE_MAP = " + str(KEY_MODEL_HARDWARE_CONFIG) + "\n\n"
                
                new_content = new_content[:config_pos] + config_str + new_content[config_pos:]
        
        # Add hardware initialization methods if appropriate
        if "init_cpu" in new_content and "init_cuda" in new_content and HAS_HARDWARE_TEMPLATE:
            # Find a suitable insertion point for hardware init methods
            init_methods_match = re.search(r'def\s+init_cpu\s*\(', new_content)
            if init_methods_match:
                init_methods_pos = init_methods_match.start()
                # Get all init methods
                hardware_init_methods = generate_hardware_init_methods()
                # Replace the existing methods with new ones
                init_end_match = re.search(r'def\s+(?!init_)[a-zA-Z0-9_]+\s*\(', new_content[init_methods_pos:])
                if init_end_match:
                    init_end_pos = init_methods_pos + init_end_match.start()
                    new_content = new_content[:init_methods_pos] + hardware_init_methods + new_content[init_end_pos:]
        
        # Create handler creation methods if needed
        if "create_cpu_" in new_content and "create_cuda_" in new_content and HAS_HARDWARE_TEMPLATE:
            # Find a suitable insertion point for creation methods
            creation_methods_match = re.search(r'def\s+create_cpu_[a-zA-Z0-9_]+\s*\(', new_content)
            if creation_methods_match:
                creation_methods_pos = creation_methods_match.start()
                # Get all creation methods
                creation_methods_code = generate_creation_methods()
                # Replace the existing methods with new ones
                creation_end_match = re.search(r'def\s+(?!create_)[a-zA-Z0-9_]+\s*\(', new_content[creation_methods_pos:])
                if creation_end_match:
                    creation_end_pos = creation_methods_pos + creation_end_match.start()
                    new_content = new_content[:creation_methods_pos] + creation_methods_code + new_content[creation_end_pos:]
        
        # Update platform detection for web
        if "detect_model_modality" in new_content:
            # Check if we need to update the modality detection
            if "def detect_model_modality" in new_content and "multimodal" not in new_content:
                modality_match = re.search(r'def\s+detect_model_modality.*?\)', new_content)
                if modality_match:
                    # Extract sample templates for different modalities
                    text_template = "Default text template"
                    vision_template = "Default vision template"
                    audio_template = "Default audio template"
                    multimodal_template = "Default multimodal template"
                    video_template = "Default video template"
                    
                    # Create hardware template integration
                    template_file = create_hardware_template_integration(
                        file_path, 
                        text_template, 
                        vision_template, 
                        audio_template, 
                        multimodal_template, 
                        video_template
                    )
                    
                    # Add import for hardware template integration
                    import_pos = new_content.find("import ")
                    import_statement = f"# Import hardware template integration\ntry:\n    from hardware_template_integration import detect_model_modality, get_hardware_template_for_model\n    HAS_HARDWARE_TEMPLATES = True\nexcept ImportError:\n    HAS_HARDWARE_TEMPLATES = False\n\n"
                    new_content = new_content[:import_pos] + import_statement + new_content[import_pos:]
        
        # Write updated content
        with open(file_path, 'w') as f:
            f.write(new_content)
        
        print(f"Successfully updated hardware support in {file_path}")
        return True
        
    except Exception as e:
        print(f"Error updating {file_path}: {e}")
        # Restore from backup
        shutil.copy2(backup_path, file_path)
        print(f"Restored {file_path} from backup")
        return False

def update_web_platform_files(force=False):
    """Update web platform files with optimization support."""
    success_count = 0
    
    for web_file in WEB_PLATFORM_FILES:
        if web_file.exists():
            print(f"Updating web platform file: {web_file}")
            
            # Create backup
            backup_path = backup_file(web_file)
            
            try:
                # Read content
                with open(web_file, 'r') as f:
                    content = f.read()
                
                # Check if optimizations are already present and we're not forcing update
                if not force and "WEBGPU_COMPUTE_SHADERS_ENABLED" in content and "WEB_PARALLEL_LOADING_ENABLED" in content:
                    print(f"Web platform optimizations already present in {web_file} (use --force to update anyway)")
                    success_count += 1
                    continue
                
                # Add March 2025 web optimizations
                has_changed = False
                
                # Add compute shader optimization
                if "class WebGPUInference" in content and "compute_shaders" not in content:
                    compute_shader_code = """
    def _init_compute_shaders(self):
        \"\"\"Initialize compute shaders for audio processing acceleration\"\"\"
        # Check if compute shaders are enabled
        compute_shaders_enabled = (
            os.environ.get("WEBGPU_COMPUTE_SHADERS_ENABLED", "0") == "1" or
            os.environ.get("WEBGPU_COMPUTE_SHADERS", "0") == "1"
        )
        
        if not compute_shaders_enabled:
            return False
            
        # Detect browser for optimal workgroup size
        is_firefox = False
        try:
            import js
            if hasattr(js, 'navigator'):
                user_agent = js.navigator.userAgent.lower()
                is_firefox = "firefox" in user_agent
        except (ImportError, AttributeError):
            # Environment variable override for testing
            is_firefox = os.environ.get("SIMULATE_FIREFOX", "0") == "1"
        
        # Set workgroup size based on browser
        # Firefox performs best with 256x1x1 workgroups
        # Chrome/Edge perform best with 128x2x1 workgroups
        if is_firefox:
            self.workgroup_size = [256, 1, 1]
            logger.info("Using Firefox-optimized compute shader workgroup size: 256x1x1")
        else:
            self.workgroup_size = [128, 2, 1]
            logger.info("Using standard compute shader workgroup size: 128x2x1")
            
        self.use_compute_shaders = True
        return True
"""
                    # Find insertion point for compute shader method
                    init_pos = content.find("def __init__")
                    if init_pos != -1:
                        next_method_pos = content.find("def ", init_pos + 10)
                        if next_method_pos != -1:
                            content = content[:next_method_pos] + compute_shader_code + content[next_method_pos:]
                            has_changed = True
                
                # Add parallel loading optimization
                if "def load_model" in content and "parallel_loading" not in content:
                    parallel_loading_code = """
    def _enable_parallel_loading(self):
        \"\"\"Enable parallel loading for multimodal models\"\"\"
        # Check if parallel loading is enabled
        parallel_loading_enabled = (
            os.environ.get("WEB_PARALLEL_LOADING_ENABLED", "0") == "1" or
            os.environ.get("PARALLEL_LOADING_ENABLED", "0") == "1"
        )
        
        if not parallel_loading_enabled:
            return False
        
        # Only enable for multimodal models
        if not hasattr(self, 'model_type') or self.model_type != 'multimodal':
            return False
            
        self.use_parallel_loading = True
        logger.info("Parallel loading enabled for multimodal model")
        return True
"""
                    # Find insertion point for parallel loading method
                    load_model_pos = content.find("def load_model")
                    if load_model_pos != -1:
                        prev_method_end = content.rfind("\n    def ", 0, load_model_pos)
                        if prev_method_end != -1:
                            content = content[:prev_method_end] + parallel_loading_code + content[prev_method_end:]
                            has_changed = True
                
                # Add shader precompilation optimization
                if "def compile_model" in content and "shader_precompile" not in content:
                    shader_precompile_code = """
    def _enable_shader_precompilation(self):
        \"\"\"Enable shader precompilation for faster startup\"\"\"
        # Check if shader precompilation is enabled
        shader_precompile_enabled = (
            os.environ.get("WEBGPU_SHADER_PRECOMPILE_ENABLED", "0") == "1" or
            os.environ.get("WEBGPU_SHADER_PRECOMPILE", "0") == "1"
        )
        
        if not shader_precompile_enabled:
            return False
            
        self.precompile_shaders = True
        logger.info("Shader precompilation enabled")
        return True
"""
                    # Find insertion point for shader precompilation method
                    compile_model_pos = content.find("def compile_model")
                    if compile_model_pos != -1:
                        prev_method_end = content.rfind("\n    def ", 0, compile_model_pos)
                        if prev_method_end != -1:
                            content = content[:prev_method_end] + shader_precompile_code + content[prev_method_end:]
                            has_changed = True
                
                # Add initialization in __init__ method
                if has_changed and "def __init__" in content:
                    init_method = re.search(r'def __init__\([^)]*\):.*?\n        [^\n]+', content, re.DOTALL)
                    if init_method:
                        init_pos = init_method.end()
                        init_code = "\n        # March 2025 Optimizations\n"
                        init_code += "        self.use_compute_shaders = False\n"
                        init_code += "        self.use_parallel_loading = False\n"
                        init_code += "        self.precompile_shaders = False\n"
                        init_code += "        self.workgroup_size = [128, 1, 1]  # Default workgroup size\n"
                        
                        content = content[:init_pos] + init_code + content[init_pos:]
                        
                        # Also add calls to initialization methods
                        setup_pattern = re.search(r'def setup\([^)]*\):[^}]*?return', content, re.DOTALL)
                        if setup_pattern:
                            setup_pos = setup_pattern.start() + setup_pattern.group(0).rfind("return")
                            setup_code = "\n        # Initialize optimizations\n"
                            setup_code += "        self._init_compute_shaders()\n"
                            setup_code += "        self._enable_parallel_loading()\n"
                            setup_code += "        self._enable_shader_precompilation()\n\n        "
                            
                            content = content[:setup_pos] + setup_code + content[setup_pos:]
                
                # Write updated content
                if has_changed:
                    with open(web_file, 'w') as f:
                        f.write(content)
                    
                    print(f"Successfully updated web platform optimizations in {web_file}")
                    success_count += 1
                else:
                    print(f"No changes needed for {web_file}")
                    success_count += 1
                
            except Exception as e:
                print(f"Error updating web platform file {web_file}: {e}")
                # Restore from backup
                shutil.copy2(backup_path, web_file)
                print(f"Restored {web_file} from backup")
        else:
            print(f"Web platform file not found: {web_file}")
    
    return success_count == len(WEB_PLATFORM_FILES)

def fix_all_generators(force=False):
    """Fix hardware support in all generator files."""
    success_count = 0
    
    for generator_file in GENERATOR_FILES:
        if generator_file.exists():
            success = update_generator_hardware_support(generator_file, force)
            if success:
                success_count += 1
        else:
            print(f"Generator file not found: {generator_file}")
    
    # Update web platform files
    web_success = update_web_platform_files(force)
    if web_success:
        print("Successfully updated all web platform files")
    else:
        print("Some web platform files could not be updated")
    
    print(f"\nSummary: Successfully updated {success_count} of {len(GENERATOR_FILES)} generator files")
    if web_success:
        print("Web platform files updated with March 2025 optimizations")
    
    return success_count == len(GENERATOR_FILES) and web_success

def main():
    """Main function."""
    parser = argparse.ArgumentParser(description="Fix generator hardware support and web platform optimizations")
    parser.add_argument("--force", action="store_true", help="Force update even if hardware detection is already present")
    args = parser.parse_args()
    
    print("Fixing hardware support in test generators and web platform files...")
    print("This includes March 2025 web platform optimizations:")
    print("- WebGPU Compute Shader optimization for audio models")
    print("- Parallel Model Loading for multimodal models")
    print("- Shader Precompilation for faster startup")
    
    success = fix_all_generators(args.force)
    
    if success:
        print("\nAll generators and web platform files successfully updated!")
    else:
        print("\nSome generators or web platform files could not be updated. Check the logs above.")
    
    return 0 if success else 1

if __name__ == "__main__":
    sys.exit(main())