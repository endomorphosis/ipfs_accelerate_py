"""
Test comprehensive hardware coverage for all key HuggingFace model classes.

This script implements the test completion plan from CLAUDE.md to ensure
complete coverage of the 13 key HuggingFace model classes across all supported
hardware platforms.

Usage:
    python test_comprehensive_hardware_coverage.py --model [model_name] --hardware [hardware_platform]
    python test_comprehensive_hardware_coverage.py --all
    python test_comprehensive_hardware_coverage.py --phase 1
    python test_comprehensive_hardware_coverage.py --report
"""

import argparse
import json
import os
import sys
from datetime import datetime
from typing import Dict, List, Optional, Set, Tuple

# Key model classes for comprehensive testing
KEY_MODELS = {
    "bert": {
        "name": "BERT",
        "models": ["bert-base-uncased", "prajjwal1/bert-tiny"],
        "category": "embedding"
    },
    "t5": {
        "name": "T5",
        "models": ["t5-small", "google/t5-efficient-tiny"],
        "category": "text_generation"
    },
    "llama": {
        "name": "LLAMA",
        "models": ["facebook/opt-125m"],
        "category": "text_generation"
    },
    "clip": {
        "name": "CLIP",
        "models": ["openai/clip-vit-base-patch32"],
        "category": "vision_text"
    },
    "vit": {
        "name": "ViT",
        "models": ["google/vit-base-patch16-224"],
        "category": "vision"
    },
    "clap": {
        "name": "CLAP",
        "models": ["laion/clap-htsat-unfused"],
        "category": "audio_text"
    },
    "whisper": {
        "name": "Whisper",
        "models": ["openai/whisper-tiny"],
        "category": "audio"
    },
    "wav2vec2": {
        "name": "Wav2Vec2",
        "models": ["facebook/wav2vec2-base"],
        "category": "audio"
    },
    "llava": {
        "name": "LLaVA",
        "models": ["llava-hf/llava-1.5-7b-hf"],
        "category": "multimodal"
    },
    "llava_next": {
        "name": "LLaVA-Next",
        "models": ["llava-hf/llava-v1.6-34b-hf"],
        "category": "multimodal"
    },
    "xclip": {
        "name": "XCLIP",
        "models": ["microsoft/xclip-base-patch32"],
        "category": "video"
    },
    "qwen": {
        "name": "Qwen2/3",
        "models": ["Qwen/Qwen2-7B-Instruct", "Qwen/Qwen2-VL-Chat"],
        "category": "text_generation"
    },
    "detr": {
        "name": "DETR",
        "models": ["facebook/detr-resnet-50"],
        "category": "vision"
    }
}

# Hardware platforms for testing
HARDWARE_PLATFORMS = {
    "cpu": {
        "name": "CPU",
        "compatibility": set(KEY_MODELS.keys()),
        "flag": "--device cpu"
    },
    "cuda": {
        "name": "CUDA",
        "compatibility": set(KEY_MODELS.keys()),
        "flag": "--device cuda"
    },
    "rocm": {
        "name": "AMD ROCm",
        "compatibility": set(KEY_MODELS.keys()) - {"llava", "llava_next"},
        "flag": "--device rocm"
    },
    "mps": {
        "name": "Apple MPS",
        "compatibility": set(KEY_MODELS.keys()) - {"llava", "llava_next"},
        "flag": "--device mps"
    },
    "openvino": {
        "name": "OpenVINO",
        "compatibility": set(KEY_MODELS.keys()) - {"llava_next"},
        "flag": "--device openvino"
    },
    "webnn": {
        "name": "WebNN",
        "compatibility": {"bert", "t5", "clip", "vit"},
        "flag": "--web-platform webnn"
    },
    "webgpu": {
        "name": "WebGPU",
        "compatibility": {"bert", "t5", "clip", "vit"},
        "flag": "--web-platform webgpu"
    }
}

# Mock implementations that need to be replaced with real ones
MOCK_IMPLEMENTATIONS = [
    ("t5", "openvino"),
    ("clap", "openvino"),
    ("wav2vec2", "openvino"),
    ("llava", "openvino"),
    ("whisper", "webnn"),
    ("whisper", "webgpu"),
    ("qwen", "rocm"),
    ("qwen", "mps"),
    ("qwen", "openvino")
]

def get_hardware_compatibility_status() -> Dict:
    """
    Generate a comprehensive status report of hardware compatibility
    for all key model classes.
    
    Returns:
        Dict: Current status of hardware compatibility testing
    """
    status = {}
    
    for model_key, model_info in KEY_MODELS.items():
        model_status = {
            "name": model_info["name"],
            "models": model_info["models"],
            "category": model_info["category"],
            "hardware_compatibility": {}
        }
        
        for hw_key, hw_info in HARDWARE_PLATFORMS.items():
            is_compatible = model_key in hw_info["compatibility"]
            is_mocked = (model_key, hw_key) in MOCK_IMPLEMENTATIONS
            
            if is_compatible:
                if is_mocked:
                    status_code = "mock"
                else:
                    status_code = "real"
            else:
                status_code = "incompatible"
                
            model_status["hardware_compatibility"][hw_key] = {
                "status": status_code,
                "hardware_name": hw_info["name"]
            }
        
        status[model_key] = model_status
    
    return status

def generate_test_command(model_key: str, hardware: str) -> Optional[str]:
    """
    Generate the command to run a test for a specific model on a specific hardware.
    
    Args:
        model_key (str): Key for the model to test
        hardware (str): Hardware platform to test on
        
    Returns:
        Optional[str]: Command to run the test, or None if incompatible
    """
    if model_key not in KEY_MODELS:
        return None
    
    if hardware not in HARDWARE_PLATFORMS:
        return None
    
    if model_key not in HARDWARE_PLATFORMS[hardware]["compatibility"]:
        return None
    
    model_name = KEY_MODELS[model_key]["models"][0].split("/")[-1]
    hw_flag = HARDWARE_PLATFORMS[hardware]["flag"]
    
    # Basic test command
    command = f"python test/test_hardware_backend.py --model {model_name} {hw_flag}"
    
    # Add special flags for certain combinations
    if hardware in ["webnn", "webgpu"]:
        command += " --web-platform-test"
    
    return command

def generate_implementation_report() -> str:
    """
    Generate a report on the implementation status of all models
    across all hardware platforms.
    
    Returns:
        str: Markdown formatted report
    """
    status = get_hardware_compatibility_status()
    now = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    report = [
        f"# Comprehensive Hardware Test Coverage Report ({now})",
        "",
        "## Summary",
        ""
    ]
    
    # Count statistics
    total_combinations = 0
    implemented_combinations = 0
    mocked_combinations = 0
    
    for model_key, model_info in status.items():
        for hw_key, hw_status in model_info["hardware_compatibility"].items():
            total_combinations += 1
            if hw_status["status"] == "real":
                implemented_combinations += 1
            elif hw_status["status"] == "mock":
                mocked_combinations += 1
    
    incompatible_combinations = total_combinations - implemented_combinations - mocked_combinations
    
    report.extend([
        f"- Total model-hardware combinations: {total_combinations}",
        f"- Fully implemented combinations: {implemented_combinations} ({implemented_combinations/total_combinations*100:.1f}%)",
        f"- Mock implementations: {mocked_combinations} ({mocked_combinations/total_combinations*100:.1f}%)",
        f"- Incompatible combinations: {incompatible_combinations} ({incompatible_combinations/total_combinations*100:.1f}%)",
        "",
        "## Implementation Status by Model",
        ""
    ])
    
    # Generate per-model status
    for model_key, model_info in status.items():
        report.append(f"### {model_info['name']}")
        report.append("")
        report.append(f"- Category: {model_info['category']}")
        report.append(f"- Test Models: {', '.join(model_info['models'])}")
        report.append("")
        report.append("| Hardware | Status | Notes |")
        report.append("|----------|--------|-------|")
        
        for hw_key, hw_status in model_info["hardware_compatibility"].items():
            status_text = {
                "real": "✅ Implemented",
                "mock": "⚠️ Mock Implementation",
                "incompatible": "❌ Incompatible"
            }[hw_status["status"]]
            
            notes = ""
            if (model_key, hw_key) in MOCK_IMPLEMENTATIONS:
                notes = "Needs real implementation"
            
            report.append(f"| {hw_status['hardware_name']} | {status_text} | {notes} |")
        
        report.append("")
    
    # Generate implementation plan
    report.extend([
        "## Implementation Plan",
        "",
        "### Phase 1: Fix Mock Implementations",
        ""
    ])
    
    for model_key, hw_key in MOCK_IMPLEMENTATIONS:
        model_name = KEY_MODELS[model_key]["name"]
        hw_name = HARDWARE_PLATFORMS[hw_key]["name"]
        report.append(f"- Replace mock implementation of {model_name} on {hw_name}")
    
    report.extend([
        "",
        "### Phase 2: Add Missing Web Platform Tests",
        ""
    ])
    
    for model_key in ["xclip", "detr"]:
        model_name = KEY_MODELS[model_key]["name"]
        for hw_key in ["webnn", "webgpu"]:
            hw_name = HARDWARE_PLATFORMS[hw_key]["name"]
            report.append(f"- Investigate feasibility of {model_name} on {hw_name}")
    
    report.extend([
        "",
        "### Phase 3: Expand Multimodal Support",
        ""
    ])
    
    for model_key in ["llava", "llava_next"]:
        model_name = KEY_MODELS[model_key]["name"]
        for hw_key in ["rocm", "mps"]:
            hw_name = HARDWARE_PLATFORMS[hw_key]["name"]
            report.append(f"- Investigate feasibility of {model_name} on {hw_name}")

    return "\n".join(report)

def save_report(report: str, output_dir: str = "hardware_compatibility_reports") -> str:
    """
    Save the implementation report to a file.
    
    Args:
        report (str): Report content
        output_dir (str): Directory to save report
        
    Returns:
        str: Path to saved report
    """
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
    
    now = datetime.now().strftime("%Y%m%d_%H%M%S")
    report_path = os.path.join(output_dir, f"hardware_compatibility_report_{now}.md")
    
    with open(report_path, "w") as f:
        f.write(report)
    
    return report_path

def run_tests_for_phase(phase: int) -> List[str]:
    """
    Run tests for a specific phase of the implementation plan.
    
    Args:
        phase (int): Phase number to run tests for
        
    Returns:
        List[str]: List of commands that were executed
    """
    commands = []
    
    if phase == 1:
        # Phase 1: Fix mock implementations
        for model_key, hw_key in MOCK_IMPLEMENTATIONS:
            command = generate_test_command(model_key, hw_key)
            if command:
                commands.append(command)
                print(f"Running: {command}")
                # Uncomment to actually run the tests
                # os.system(command)
    
    elif phase == 2:
        # Phase 2: Expand multimodal support
        for model_key in ["llava", "llava_next"]:
            for hw_key in ["rocm", "mps"]:
                command = generate_test_command(model_key, hw_key)
                if command:
                    commands.append(command)
                    print(f"Running: {command}")
                    # Uncomment to actually run the tests
                    # os.system(command)
    
    elif phase == 3:
        # Phase 3: Web platform extension
        for model_key in ["xclip", "detr", "whisper"]:
            for hw_key in ["webnn", "webgpu"]:
                command = generate_test_command(model_key, hw_key)
                if command:
                    commands.append(command)
                    print(f"Running: {command}")
                    # Uncomment to actually run the tests
                    # os.system(command)
    
    return commands

def run_all_tests() -> List[str]:
    """
    Run tests for all compatible model-hardware combinations.
    
    Returns:
        List[str]: List of commands that were executed
    """
    commands = []
    
    for model_key in KEY_MODELS:
        for hw_key in HARDWARE_PLATFORMS:
            if model_key in HARDWARE_PLATFORMS[hw_key]["compatibility"]:
                command = generate_test_command(model_key, hw_key)
                if command:
                    commands.append(command)
                    print(f"Running: {command}")
                    # Uncomment to actually run the tests
                    # os.system(command)
    
    return commands

def run_tests_for_model(model_key: str) -> List[str]:
    """
    Run tests for a specific model across all compatible hardware platforms.
    
    Args:
        model_key (str): Key for the model to test
        
    Returns:
        List[str]: List of commands that were executed
    """
    commands = []
    
    if model_key not in KEY_MODELS:
        print(f"Unknown model: {model_key}")
        print(f"Available models: {', '.join(KEY_MODELS.keys())}")
        return commands
    
    for hw_key in HARDWARE_PLATFORMS:
        if model_key in HARDWARE_PLATFORMS[hw_key]["compatibility"]:
            command = generate_test_command(model_key, hw_key)
            if command:
                commands.append(command)
                print(f"Running: {command}")
                # Uncomment to actually run the tests
                # os.system(command)
    
    return commands

def run_tests_for_hardware(hw_key: str) -> List[str]:
    """
    Run tests for a specific hardware platform across all compatible models.
    
    Args:
        hw_key (str): Key for the hardware platform to test
        
    Returns:
        List[str]: List of commands that were executed
    """
    commands = []
    
    if hw_key not in HARDWARE_PLATFORMS:
        print(f"Unknown hardware platform: {hw_key}")
        print(f"Available platforms: {', '.join(HARDWARE_PLATFORMS.keys())}")
        return commands
    
    for model_key in KEY_MODELS:
        if model_key in HARDWARE_PLATFORMS[hw_key]["compatibility"]:
            command = generate_test_command(model_key, hw_key)
            if command:
                commands.append(command)
                print(f"Running: {command}")
                # Uncomment to actually run the tests
                # os.system(command)
    
    return commands

def main():
    parser = argparse.ArgumentParser(description="Test comprehensive hardware coverage for HuggingFace models")
    group = parser.add_mutually_exclusive_group()
    group.add_argument("--all", action="store_true", help="Run tests for all compatible model-hardware combinations")
    group.add_argument("--phase", type=int, choices=[1, 2, 3, 4, 5], help="Run tests for a specific phase of the implementation plan")
    group.add_argument("--model", help="Run tests for a specific model across all compatible hardware platforms")
    group.add_argument("--hardware", help="Run tests for a specific hardware platform across all compatible models")
    group.add_argument("--report", action="store_true", help="Generate and save an implementation report")
    args = parser.parse_args()
    
    if args.all:
        commands = run_all_tests()
        print(f"\nExecuted {len(commands)} test commands")
    
    elif args.phase:
        commands = run_tests_for_phase(args.phase)
        print(f"\nExecuted {len(commands)} test commands for Phase {args.phase}")
    
    elif args.model:
        commands = run_tests_for_model(args.model)
        print(f"\nExecuted {len(commands)} test commands for model {args.model}")
    
    elif args.hardware:
        commands = run_tests_for_hardware(args.hardware)
        print(f"\nExecuted {len(commands)} test commands for hardware {args.hardware}")
    
    elif args.report:
        report = generate_implementation_report()
        report_path = save_report(report)
        print(f"Generated implementation report: {report_path}")
        print("\nReport Preview:")
        print("=" * 80)
        print("\n".join(report.split("\n")[:20]) + "\n...")
    
    else:
        parser.print_help()

if __name__ == "__main__":
    main()