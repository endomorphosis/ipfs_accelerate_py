# HuggingFace Model Test Coverage Report

**Generated:** 2025-03-22 01:42:35

## Coverage Summary

- **Total models from HF:** 6
- **Implemented (from HF list):** 6 (100.0%)
- **Additional models implemented:** 351
- **Missing models:** 0

## Coverage by Architecture

| Architecture | Implemented | Missing |
| --- | --- | --- |
| audio | 9 (100.0%) | 0 |
| decoder_only | 10 (100.0%) | 0 |
| encoder_decoder | 9 (100.0%) | 0 |
| encoder_only | 15 (100.0%) | 0 |
| multimodal | 12 (100.0%) | 0 |
| uncategorized | 297 (100.0%) | 0 |
| vision | 5 (100.0%) | 0 |

## Implemented Models

The following models have been implemented and tested:

### Audio
- clap
- encodec
- hubert
- unispeech
- unispeech_sat
- wav2vec2
- wav2vec2_bert
- wav2vec2_conformer
- whisper

### Decoder Only
- falcon
- falcon_mamba
- gpt2
- gpt2_minimal
- gpt_neo
- gptj
- llama
- mistral
- mistral_nemo
- mistral_next

### Encoder Decoder
- bart
- led
- m2m_100
- mbart
- pegasus
- pegasus_x
- t5
- t5_minimal
- t5_small

### Encoder Only
- albert
- bert
- bert_base_uncased
- bert_copy
- bert_generation
- bert_minimal
- convbert
- electra
- roberta
- roberta_prelayernorm
- vit
- vit_hybrid
- vit_mae
- vit_minimal
- vit_msn

### Multimodal
- blip
- blip_2
- clip
- clip_text_model
- clip_vision_model
- flava
- idefics
- llava
- llava_next
- llava_next_video
- llava_onevision
- paligemma

### Uncategorized
- \
- __help
- __list_only
- __model
- align
- altclip
- audio
- audio_spectrogram_transformer
- audioldm2
- autoformer
- bark
- big_bird
- bigbird
- bigbird_pegasus
- biogpt
- bit
- blenderbot
- blenderbot_small
- blip2
- bloom
- bridgetower
- bros
- camembert
- canine
- chameleon
- chinese_clip
- chinese_clip_vision_model
- claude3_haiku
- clipseg
- clvp
- cm3
- code_llama
- codegen
- codellama
- cogvlm2
- cohere
- command_r
- conditional_detr
- convnextv2
- cpmant
- ctrl
- cvt
- dac
- data2vec
- data2vec-audio
- data2vec-text
- data2vec-vision
- data2vec_audio
- data2vec_text
- data2vec_vision
- dbrx
- dbrx_instruct
- deberta
- deberta_v2
- decision_transformer
- decoder_only
- deepseek
- deepseek_coder
- deepseek_distil
- deepseek_r1
- deepseek_r1_distil
- deepseek_vision
- deformable_detr
- depth_anything
- deta
- detr
- dinat
- dino
- dinov2
- distilbert
- distilroberta_base
- donut
- donut_swin
- dpr
- dpt
- efficientformer
- efficientnet
- encoder_decoder
- encoder_only
- ernie
- ernie_m
- esm
- fastspeech2_conformer
- flamingo
- flan
- flan-t5
- flan_t5
- flaubert
- florence
- fnet
- focalnet
- fsmt
- funnel
- fuyu
- gemma
- gemma2
- gemma3
- git
- glm
- glpn
- gpt-j
- gpt-neo
- gpt-neox
- gpt_bigcode
- gpt_j
- gpt_neox
- gpt_neox_japanese
- gpt_sw3
- gptsan_japanese
- granite
- granitemoe
- graphormer
- graphsage
- grounding_dino
- groupvit
- hiera
- ibert
- idefics2
- idefics3
- imagebind
- imagegpt
- informer
- instructblip
- instructblipvideo
- jamba
- jetmoe
- jukebox
- kosmos_2
- layoutlm
- layoutlmv2
- layoutlmv3
- levit
- lilt
- longformer
- longt5
- luke
- lxmert
- mamba
- mamba2
- marian
- markuplm
- mask2former
- maskformer
- maskformer_swin
- mctct
- mega
- megatron_bert
- mgp_str
- mimi
- mixtral
- mllama
- mlp-mixer
- mlp_mixer
- mobilebert
- mobilenet_v1
- mobilenet_v2
- mobilevit
- mobilevitv2
- moshi
- mpnet
- mpt
- mra
- mt5
- multimodal
- musicgen
- musicgen_melody
- mvp
- nat
- nemotron
- nezha
- nllb_moe
- nougat
- nystromformer
- olmo
- olmoe
- omdet_turbo
- oneformer
- open_llama
- openai_gpt
- opt
- optimized_model
- orca3
- owlv2
- owlvit
- patchtsmixer
- patchtst
- perceiver
- persimmon
- phi
- phi3
- phi4
- phimoe
- pix2struct
- pixtral
- plbart
- poolformer
- pop2piano
- prophetnet
- pvt
- pvt_v2
- qdqbert
- qwen2
- qwen2_audio
- qwen2_audio_encoder
- qwen2_moe
- qwen2_vl
- qwen3
- qwen3_moe
- qwen3_vl
- rag
- realm
- recurrent_gemma
- reformer
- regnet
- rembert
- resnet
- retribert
- roc_bert
- roformer
- rt_detr
- rt_detr_resnet
- rwkv
- seamless_m4t
- seamless_m4t_v2
- segformer
- seggpt
- sew
- sew_d
- siglip
- siglip_vision_model
- speech-to-text
- speech-to-text-2
- speech_encoder_decoder
- speech_to_text
- speech_to_text_2
- speecht5
- splinter
- squeezebert
- stablelm
- starcoder2
- superpoint
- swiftformer
- swin2sr
- swinv2
- switch_transformers
- table_transformer
- tapas
- time_series_transformer
- timesformer
- timm_backbone
- tinyllama
- trajectory_transformer
- transfo-xl
- transfo_xl
- trocr
- trocr_base
- trocr_large
- tvlt
- tvp
- udop
- ulip
- umt5
- univnet
- upernet
- usm
- van
- video_llava
- videomae
- vilt
- vinvl
- vipllava
- vision
- vision-text-dual-encoder
- vision_encoder_decoder
- vision_t5
- vision_text_dual_encoder
- visual_bert
- vitdet
- vitmatte
- vits
- vivit
- vqgan
- wav2vec2-conformer
- wavlm
- xclip
- xglm
- xlm
- xlm-roberta
- xlm_prophetnet
- xlm_roberta
- xlm_roberta_xl
- xlnet
- xmod
- yolos
- yoso
- zamba
- zoedepth

### Vision
- beit
- convnext
- deit
- sam
- swin

## Missing Models

The following models still need to be implemented: