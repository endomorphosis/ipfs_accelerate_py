#!/usr/bin/env python3
"""
Hardware Detection Module for Test Generators

This module provides reliable detection of hardware capabilities for the test generator templates.
It ensures that all generated tests will have consistent hardware detection code.
"""

import os
import importlib.util
import logging
from typing import Dict, Any, List

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger("hardware_detection")

def generate_hardware_detection_code() -> str:
    """
    Generate hardware detection code to be inserted into test templates.
    Returns Python code as a string that properly detects hardware capabilities.
    """
    detection_code = """
# Hardware Detection
import os
import importlib.util

# Try to import torch first (needed for CUDA/ROCm/MPS)
try:
    import torch
    HAS_TORCH = True
except ImportError:
    from unittest.mock import MagicMock
    torch = MagicMock()
    HAS_TORCH = False
    logger.warning("torch not available, using mock")

# Initialize hardware capability flags
HAS_CUDA = False
HAS_ROCM = False
HAS_MPS = False
HAS_OPENVINO = False
HAS_WEBNN = False
HAS_WEBGPU = False

# CUDA detection
if HAS_TORCH:
    HAS_CUDA = torch.cuda.is_available()
    
    # ROCm detection
    if HAS_CUDA and hasattr(torch, '_C') and hasattr(torch._C, '_rocm_version'):
        HAS_ROCM = True
    elif 'ROCM_HOME' in os.environ:
        HAS_ROCM = True
    
    # Apple MPS detection
    if hasattr(torch, "mps") and hasattr(torch.mps, "is_available"):
        HAS_MPS = torch.mps.is_available()

# OpenVINO detection
HAS_OPENVINO = importlib.util.find_spec("openvino") is not None

# WebNN detection (browser API)
HAS_WEBNN = (
    importlib.util.find_spec("webnn") is not None or 
    importlib.util.find_spec("webnn_js") is not None or
    "WEBNN_AVAILABLE" in os.environ or
    "WEBNN_SIMULATION" in os.environ
)

# WebGPU detection (browser API)
HAS_WEBGPU = (
    importlib.util.find_spec("webgpu") is not None or
    importlib.util.find_spec("wgpu") is not None or
    "WEBGPU_AVAILABLE" in os.environ or
    "WEBGPU_SIMULATION" in os.environ
)

# Hardware detection function for comprehensive hardware info
def check_hardware():
    \"\"\"Check available hardware and return capabilities.\"\"\"
    capabilities = {
        "cpu": True,
        "cuda": False,
        "cuda_version": None,
        "cuda_devices": 0,
        "mps": False,
        "openvino": False,
        "rocm": False,
        "webnn": False,
        "webgpu": False
    }
    
    # CUDA capabilities
    if HAS_TORCH and HAS_CUDA:
        capabilities["cuda"] = True
        capabilities["cuda_devices"] = torch.cuda.device_count()
        capabilities["cuda_version"] = torch.version.cuda
    
    # MPS capabilities (Apple Silicon)
    capabilities["mps"] = HAS_MPS
    
    # OpenVINO capabilities
    capabilities["openvino"] = HAS_OPENVINO
    
    # ROCm capabilities
    capabilities["rocm"] = HAS_ROCM
    
    # WebNN capabilities
    capabilities["webnn"] = HAS_WEBNN
    
    # WebGPU capabilities
    capabilities["webgpu"] = HAS_WEBGPU
    
    return capabilities

# Get hardware capabilities
HW_CAPABILITIES = check_hardware()
"""
    
    return detection_code

# Safe version for template string literals
def generate_hardware_detection_template(as_string_literal=False) -> str:
    """
    Generate hardware detection code suitable for template files.
    
    Args:
        as_string_literal: If True, escape special characters for string literals
    
    Returns:
        Python code as a string that properly detects hardware capabilities
    """
    code = generate_hardware_detection_code()
    
    # If needed for string literals, escape special characters
    if as_string_literal:
        code = code.replace('\\', '\\\\')
        code = code.replace('"', '\\"')
        code = code.replace("'", "\\'")
    
    return code

def generate_hardware_init_methods() -> str:
    """Generate initialization methods for different hardware platforms."""
    init_methods = """
    def init_cpu(self, model_name=None, **kwargs):
        \"\"\"Initialize model for CPU inference.\"\"\"
        model_name = model_name or self.model_id
        
        try:
            # Initialize tokenizer
            tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)
            
            # Initialize model
            model = transformers.AutoModel.from_pretrained(model_name)
            model.eval()
            
            # Create handler function
            def handler(text_input, **kwargs):
                try:
                    # Process with tokenizer
                    if isinstance(text_input, list):
                        inputs = tokenizer(text_input, padding=True, truncation=True, return_tensors="pt")
                    else:
                        inputs = tokenizer(text_input, return_tensors="pt")
                    
                    # Run inference
                    with torch.no_grad():
                        outputs = model(**inputs)
                    
                    return {
                        "output": outputs,
                        "implementation_type": "CPU",
                        "model": model_name
                    }
                except Exception as e:
                    logger.error(f"Error in CPU handler: {e}")
                    return {
                        "output": f"Error: {str(e)}",
                        "implementation_type": "ERROR",
                        "error": str(e),
                        "model": model_name
                    }
            
            # Create queue for batch processing
            queue = asyncio.Queue(64)
            batch_size = getattr(self, 'batch_size', 8)
            
            # Return all components
            return model, tokenizer, handler, queue, batch_size
            
        except Exception as e:
            logger.error(f"Error initializing model on CPU: {str(e)}")
            traceback.print_exc()
            return None, None, None, None, 1
    
    def init_cuda(self, model_name=None, device="cuda:0", **kwargs):
        \"\"\"Initialize model for CUDA inference.\"\"\"
        model_name = model_name or self.model_id
        
        # Check for CUDA availability
        if not HAS_CUDA:
            logger.warning("CUDA not available, falling back to CPU")
            return self.init_cpu(model_name)
            
        try:
            logger.info(f"Initializing {model_name} with CUDA on {device}")
            
            # Initialize tokenizer
            tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)
            
            # Initialize model and move to GPU
            model = transformers.AutoModel.from_pretrained(model_name)
            model.to(device)
            model.eval()
            
            # Create handler function
            def handler(text_input, **kwargs):
                try:
                    # Process with tokenizer
                    if isinstance(text_input, list):
                        inputs = tokenizer(text_input, padding=True, truncation=True, return_tensors="pt")
                    else:
                        inputs = tokenizer(text_input, return_tensors="pt")
                    
                    # Move inputs to GPU
                    inputs = {k: v.to(device) for k, v in inputs.items()}
                    
                    # Run inference
                    with torch.no_grad():
                        outputs = model(**inputs)
                    
                    return {
                        "output": outputs,
                        "implementation_type": "CUDA",
                        "device": device,
                        "model": model_name
                    }
                except Exception as e:
                    logger.error(f"Error in CUDA handler: {e}")
                    return {
                        "output": f"Error: {str(e)}",
                        "implementation_type": "ERROR",
                        "error": str(e),
                        "model": model_name
                    }
            
            # Create queue for batch processing
            queue = asyncio.Queue(64)
            batch_size = getattr(self, 'batch_size', 8)
            
            # Return all components
            return model, tokenizer, handler, queue, batch_size
            
        except Exception as e:
            logger.error(f"Error initializing model with CUDA: {str(e)}")
            logger.warning("Falling back to CPU implementation")
            return self.init_cpu(model_name)
    
    def init_rocm(self, model_name=None, device="hip", **kwargs):
        \"\"\"Initialize model for ROCm (AMD GPU) inference.\"\"\"
        model_name = model_name or self.model_id
        
        # Check for ROCm/HIP availability
        if not HAS_ROCM:
            logger.warning("ROCm/HIP not available, falling back to CPU")
            return self.init_cpu(model_name)
            
        try:
            logger.info(f"Initializing {model_name} with ROCm/HIP on {device}")
            
            # Initialize tokenizer
            tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)
            
            # Initialize model
            model = transformers.AutoModel.from_pretrained(model_name)
            
            # Move model to AMD GPU
            model.to(device)
            model.eval()
            
            # Create handler function
            def handler(text_input, **kwargs):
                try:
                    # Process with tokenizer
                    if isinstance(text_input, list):
                        inputs = tokenizer(text_input, padding=True, truncation=True, return_tensors="pt")
                    else:
                        inputs = tokenizer(text_input, return_tensors="pt")
                    
                    # Move inputs to GPU
                    inputs = {k: v.to(device) for k, v in inputs.items()}
                    
                    # Run inference
                    with torch.no_grad():
                        outputs = model(**inputs)
                    
                    return {
                        "output": outputs,
                        "implementation_type": "ROCM",
                        "device": device,
                        "model": model_name
                    }
                except Exception as e:
                    logger.error(f"Error in ROCm handler: {e}")
                    return {
                        "output": f"Error: {str(e)}",
                        "implementation_type": "ERROR",
                        "error": str(e),
                        "model": model_name
                    }
            
            # Create queue
            queue = asyncio.Queue(64)
            batch_size = getattr(self, 'batch_size', 8)
            
            # Return components
            return model, tokenizer, handler, queue, batch_size
            
        except Exception as e:
            logger.error(f"Error initializing model with ROCm: {str(e)}")
            logger.warning("Falling back to CPU implementation")
            return self.init_cpu(model_name)
    
    def init_openvino(self, model_name=None, device="CPU", **kwargs):
        \"\"\"Initialize model for OpenVINO inference.\"\"\"
        model_name = model_name or self.model_id
        
        # Check for OpenVINO
        if not HAS_OPENVINO:
            logger.warning("OpenVINO not available, falling back to CPU")
            return self.init_cpu(model_name)
        
        try:
            logger.info(f"Initializing {model_name} with OpenVINO on {device}")
            
            # Try to use optimum.intel if available
            try:
                # Import openvino and handle API changes
                import openvino
                
                # Try new API first (recommended since 2025.0)
                try:
                    from openvino import Core
                except (ImportError, AttributeError):
                    # Fall back to legacy API
                    from openvino.runtime import Core
                
                # Import optimum.intel for transformer models
                from optimum.intel import OVModelForSequenceClassification
                
                # Time tokenizer loading
                tokenizer_load_start = time.time()
                tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)
                tokenizer_load_time = time.time() - tokenizer_load_start
                
                # Time model loading
                model_load_start = time.time()
                model = OVModelForSequenceClassification.from_pretrained(model_name, export=True)
                model_load_time = time.time() - model_load_start
                
                # Create handler function
                def handler(text_input, **kwargs):
                    # Process with tokenizer
                    if isinstance(text_input, list):
                        inputs = tokenizer(text_input, padding=True, truncation=True, return_tensors="pt")
                    else:
                        inputs = tokenizer(text_input, return_tensors="pt")
                    
                    # Run inference
                    outputs = model(**inputs)
                    
                    return {
                        "output": outputs,
                        "implementation_type": "optimum.intel",
                        "model": model_name
                    }
                
                # Create queue
                queue = asyncio.Queue(64)
                batch_size = getattr(self, 'batch_size', 8)
                
                # Return components
                return model, tokenizer, handler, queue, batch_size
                
            except ImportError:
                logger.warning("optimum.intel not available, using direct OpenVINO conversion")
                
                # Initialize tokenizer
                tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)
                
                # Load model directly with transformers first
                pt_model = transformers.AutoModel.from_pretrained(model_name)
                
                # We'll use a simplified approach for this implementation
                # Instead of full OpenVINO conversion, we'll wrap the PyTorch model
                class SimpleOVWrapper:
                    def __init__(self, pt_model):
                        self.pt_model = pt_model
                        
                    def __call__(self, **kwargs):
                        with torch.no_grad():
                            return self.pt_model(**kwargs)
                
                model = SimpleOVWrapper(pt_model)
                
                # Create handler function
                def handler(text_input, **kwargs):
                    # Process with tokenizer
                    if isinstance(text_input, list):
                        inputs = tokenizer(text_input, padding=True, truncation=True, return_tensors="pt")
                    else:
                        inputs = tokenizer(text_input, return_tensors="pt")
                    
                    # Run inference
                    outputs = model(**inputs)
                    
                    return {
                        "output": outputs,
                        "implementation_type": "openvino_direct",
                        "model": model_name
                    }
                
                # Create queue
                queue = asyncio.Queue(64)
                batch_size = getattr(self, 'batch_size', 8)
                
                # Return components
                return model, tokenizer, handler, queue, batch_size
                
        except Exception as e:
            logger.error(f"Error initializing OpenVINO: {str(e)}")
            traceback.print_exc()
            # Fall back to CPU
            logger.warning("Falling back to CPU implementation")
            return self.init_cpu(model_name)
    
    def init_mps(self, model_name=None, device="mps", **kwargs):
        \"\"\"Initialize model for MPS (Apple Silicon) inference.\"\"\"
        model_name = model_name or self.model_id
        
        # Check for MPS availability
        if not HAS_MPS:
            logger.warning("MPS not available, falling back to CPU")
            return self.init_cpu(model_name)
            
        try:
            logger.info(f"Initializing {model_name} with MPS on {device}")
            
            # Initialize tokenizer
            tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)
            
            # Initialize model and move to MPS
            model = transformers.AutoModel.from_pretrained(model_name)
            model.to(device)
            model.eval()
            
            # Create handler function
            def handler(text_input, **kwargs):
                try:
                    # Process with tokenizer
                    if isinstance(text_input, list):
                        inputs = tokenizer(text_input, padding=True, truncation=True, return_tensors="pt")
                    else:
                        inputs = tokenizer(text_input, return_tensors="pt")
                    
                    # Move inputs to MPS
                    inputs = {k: v.to(device) for k, v in inputs.items()}
                    
                    # Run inference
                    with torch.no_grad():
                        outputs = model(**inputs)
                    
                    return {
                        "output": outputs,
                        "implementation_type": "MPS",
                        "device": device,
                        "model": model_name
                    }
                except Exception as e:
                    logger.error(f"Error in MPS handler: {e}")
                    return {
                        "output": f"Error: {str(e)}",
                        "implementation_type": "ERROR",
                        "error": str(e),
                        "model": model_name
                    }
            
            # Create queue
            queue = asyncio.Queue(64)
            batch_size = getattr(self, 'batch_size', 8)
            
            # Return components
            return model, tokenizer, handler, queue, batch_size
            
        except Exception as e:
            logger.error(f"Error initializing model with MPS: {str(e)}")
            logger.warning("Falling back to CPU implementation")
            return self.init_cpu(model_name)
    
    def init_webnn(self, model_name=None, **kwargs):
        \"\"\"Initialize model for WebNN inference.
        
        WebNN support requires browser environment or dedicated WebNN runtime.
        This implementation provides the necessary adapter functions for web usage.
        \"\"\"
        model_name = model_name or self.model_id
        
        # For WebNN, actual execution happens in browser environment
        # This method prepares the necessary adapters
        
        # Create a simple mock for direct testing
        processor = None
        
        try:
            # Get the tokenizer as processor
            processor = transformers.AutoTokenizer.from_pretrained(model_name)
        except Exception as e:
            logger.warning(f"Could not load tokenizer: {str(e)}")
            # Create mock tokenizer
            class MockTokenizer:
                def __call__(self, text, **kwargs):
                    return {"input_ids": [1, 2, 3, 4, 5]}
                
                def decode(self, token_ids, **kwargs):
                    return "WebNN mock output"
                    
            processor = MockTokenizer()
        
        # Create adapter
        model = None  # No model object needed, execution happens in browser
        
        # Create test data attributes for the test
        self.test_webnn_text = "This is a test sentence for WebNN"
        self.test_batch_webnn = ["First WebNN test.", "Second WebNN test."]
        
        # Handler for WebNN
        def handler(text_input, **kwargs):
            # This handler is called from Python side to prepare for WebNN execution
            # It should return the necessary data for the browser to execute the model
            
            # Process input
            if isinstance(text_input, str):
                # For API simulation/testing, return mock output
                return {
                    "output": "WebNN mock output for text model",
                    "implementation_type": "WebNN_READY",
                    "input_text": text_input,
                    "model": model_name,
                    "test_data": self.test_webnn_text
                }
            elif isinstance(text_input, list):
                # Batch processing
                return {
                    "output": ["WebNN mock output for text model"] * len(text_input),
                    "implementation_type": "WebNN_READY",
                    "input_batch": text_input,
                    "model": model_name,
                    "test_batch_data": self.test_batch_webnn
                }
            else:
                return {
                    "error": "Unsupported input format for WebNN",
                    "implementation_type": "WebNN_ERROR"
                }
        
        # Create queue and batch_size
        queue = asyncio.Queue(64)
        batch_size = 1  # Single item processing for WebNN typically
        
        return model, processor, handler, queue, batch_size
    
    def init_webgpu(self, model_name=None, **kwargs):
        \"\"\"Initialize model for WebGPU inference.
        
        WebGPU support requires browser environment or dedicated WebGPU runtime.
        This implementation provides the necessary adapter functions for web usage.
        \"\"\"
        model_name = model_name or self.model_id
        
        # For WebGPU, actual execution happens in browser environment
        # This method prepares the necessary adapters
        
        # Create a simple mock for direct testing
        processor = None
        
        try:
            # Get the tokenizer as processor
            processor = transformers.AutoTokenizer.from_pretrained(model_name)
        except Exception as e:
            logger.warning(f"Could not load tokenizer: {str(e)}")
            # Create mock tokenizer
            class MockTokenizer:
                def __call__(self, text, **kwargs):
                    return {"input_ids": [1, 2, 3, 4, 5]}
                
                def decode(self, token_ids, **kwargs):
                    return "WebGPU mock output"
                    
            processor = MockTokenizer()
        
        # Create adapter
        model = None  # No model object needed, execution happens in browser
        
        # Create test data attributes for the test
        self.test_webgpu_text = "This is a test sentence for WebGPU"
        self.test_batch_webgpu = ["First WebGPU test.", "Second WebGPU test."]
        
        # Handler for WebGPU
        def handler(text_input, **kwargs):
            # This handler is called from Python side to prepare for WebGPU execution
            # It should return the necessary data for the browser to execute the model
            
            # Process input
            if isinstance(text_input, str):
                # For API simulation/testing, return mock output
                return {
                    "output": "WebGPU mock output for text model",
                    "implementation_type": "WebGPU_READY",
                    "input_text": text_input,
                    "model": model_name,
                    "test_data": self.test_webgpu_text
                }
            elif isinstance(text_input, list):
                # Batch processing
                return {
                    "output": ["WebGPU mock output for text model"] * len(text_input),
                    "implementation_type": "WebGPU_READY",
                    "input_batch": text_input,
                    "model": model_name,
                    "test_batch_data": self.test_batch_webgpu
                }
            else:
                return {
                    "error": "Unsupported input format for WebGPU",
                    "implementation_type": "WebGPU_ERROR"
                }
        
        # Create queue and batch_size
        queue = asyncio.Queue(64)
        batch_size = 1  # Single item processing for WebGPU typically
        
        return model, processor, handler, queue, batch_size
"""
    
    return init_methods

def generate_creation_methods() -> str:
    """Generate handler creation methods for different hardware platforms."""
    creation_methods = """
    def create_cpu_handler(self, endpoint_model, device, hardware_label, endpoint=None, processor=None):
        \"\"\"Create a CPU handler for the endpoint.\"\"\"
        # Initialize for CPU
        model, tokenizer, handler_func, queue, batch_size = self.init_cpu(endpoint_model)
        return handler_func
        
    def create_cuda_handler(self, endpoint_model, device, hardware_label, endpoint=None, processor=None):
        \"\"\"Create a CUDA handler for the endpoint.\"\"\"
        # Check CUDA availability
        if not HAS_CUDA:
            logger.warning("CUDA not available, falling back to CPU")
            return self.create_cpu_handler(endpoint_model, "cpu", "cpu", endpoint, processor)
        
        # Initialize for CUDA
        model, tokenizer, handler_func, queue, batch_size = self.init_cuda(endpoint_model, device)
        return handler_func
        
    def create_rocm_handler(self, endpoint_model, device, hardware_label, endpoint=None, processor=None):
        \"\"\"Create a ROCm handler for the endpoint.\"\"\"
        # Check ROCm availability
        if not HAS_ROCM:
            logger.warning("ROCm not available, falling back to CPU")
            return self.create_cpu_handler(endpoint_model, "cpu", "cpu", endpoint, processor)
        
        # Initialize for ROCm
        model, tokenizer, handler_func, queue, batch_size = self.init_rocm(endpoint_model, device)
        return handler_func
        
    def create_openvino_handler(self, endpoint_model, openvino_label, endpoint=None, processor=None):
        \"\"\"Create an OpenVINO handler for the endpoint.\"\"\"
        # Check OpenVINO availability
        if not HAS_OPENVINO:
            logger.warning("OpenVINO not available, falling back to CPU")
            return self.create_cpu_handler(endpoint_model, "cpu", "cpu", endpoint, processor)
        
        # Initialize for OpenVINO
        model, tokenizer, handler_func, queue, batch_size = self.init_openvino(endpoint_model)
        return handler_func
        
    def create_mps_handler(self, endpoint_model, device, hardware_label, endpoint=None, processor=None):
        \"\"\"Create an MPS handler for the endpoint.\"\"\"
        # Check MPS availability
        if not HAS_MPS:
            logger.warning("MPS not available, falling back to CPU")
            return self.create_cpu_handler(endpoint_model, "cpu", "cpu", endpoint, processor)
        
        # Initialize for MPS
        model, tokenizer, handler_func, queue, batch_size = self.init_mps(endpoint_model, device)
        return handler_func
        
    def create_webnn_handler(self, endpoint_model, webnn_device, endpoint=None, processor=None):
        \"\"\"Create a WebNN handler for the endpoint.\"\"\"
        # Check WebNN availability or provide simulation
        if not HAS_WEBNN and "WEBNN_SIMULATION" not in os.environ:
            logger.warning("WebNN not available, falling back to CPU with simulation")
        
        # Initialize for WebNN (simulation or real)
        model, processor, handler_func, queue, batch_size = self.init_webnn(endpoint_model)
        return handler_func
        
    def create_webgpu_handler(self, endpoint_model, webgpu_device, endpoint=None, processor=None):
        \"\"\"Create a WebGPU handler for the endpoint.\"\"\"
        # Check WebGPU availability or provide simulation
        if not HAS_WEBGPU and "WEBGPU_SIMULATION" not in os.environ:
            logger.warning("WebGPU not available, falling back to CPU with simulation")
        
        # Initialize for WebGPU (simulation or real)
        model, processor, handler_func, queue, batch_size = self.init_webgpu(endpoint_model)
        return handler_func
"""
    
    return creation_methods

if __name__ == "__main__":
    # Print the hardware detection code
    print("Hardware Detection Code Template:")
    print(generate_hardware_detection_code())
    
    # Create a file with the template
    output_file = os.path.join(os.path.dirname(__file__), "hardware_detection_template.py")
    with open(output_file, "w") as f:
        f.write(generate_hardware_detection_code())
    
    print(f"\nTemplate saved to: {output_file}")