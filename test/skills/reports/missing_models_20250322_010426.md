# HuggingFace Model Coverage Report

**Date:** 2025-03-22 01:04:28

## Summary

- **Total models tracked:** 198
- **Implemented models:** 114 (57.6%)
- **Missing models:** 84 (42.4%)

## Coverage by Architecture

### Decoder-only Models

- **Total decoder-only models:** 33
- **Implemented:** 9 (27.3%)
- **Missing:** 24

### Encoder-decoder Models

- **Total encoder-decoder models:** 16
- **Implemented:** 9 (56.2%)
- **Missing:** 7

### Encoder-only Models

- **Total encoder-only models:** 31
- **Implemented:** 13 (41.9%)
- **Missing:** 18

### Multimodal Models

- **Total multimodal models:** 17
- **Implemented:** 6 (35.3%)
- **Missing:** 11

### Speech Models

- **Total speech models:** 6
- **Implemented:** 2 (33.3%)
- **Missing:** 4

### Unknown Models

- **Total unknown models:** 61
- **Implemented:** 61 (100.0%)
- **Missing:** 0

### Vision Models

- **Total vision models:** 23
- **Implemented:** 10 (43.5%)
- **Missing:** 13

### Vision-text Models

- **Total vision-text models:** 11
- **Implemented:** 4 (36.4%)
- **Missing:** 7

## Coverage by Priority

### Critical Priority Models

- **Total critical models:** 32
- **Implemented:** 28 (87.5%)
- **Missing:** 4

### High Priority Models

- **Total high models:** 27
- **Implemented:** 9 (33.3%)
- **Missing:** 18

### Medium Priority Models

- **Total medium models:** 139
- **Implemented:** 77 (55.4%)
- **Missing:** 62

## Implementation Roadmap

### Critical Priority Models

These models should be implemented first due to their importance and widespread use:

- `gpt-j` (decoder-only)
- `flan-t5` (encoder-decoder)
- `xlm-roberta` (encoder-only)
- `vision-text-dual-encoder` (vision-text)

### High Priority Models

These models should be implemented next:

- `codellama` (decoder-only)
- `gpt-neo` (decoder-only)
- `gpt-neox` (decoder-only)
- `qwen2` (decoder-only)
- `qwen3` (decoder-only)
- `longt5` (encoder-decoder)
- `pegasus-x` (encoder-decoder)
- `luke` (encoder-only)
- `mpnet` (encoder-only)
- `fuyu` (multimodal)
- `kosmos-2` (multimodal)
- `llava-next` (multimodal)
- `video-llava` (multimodal)
- `bark` (speech)
- `mobilenet-v2` (vision)
- `blip-2` (vision-text)
- `chinese-clip` (vision-text)
- `clipseg` (vision-text)

### Medium Priority Models

These models can be implemented after critical and high priority models:

#### Decoder-only Models

- `codegen`
- `command-r`
- `gemma2`
- `gemma3`
- `llama-3`
- `mamba`
- `mistral-next`
- `nemotron`
- `olmo`
- `olmoe`
- `openai-gpt`
- `persimmon`
- `phi3`
- `phi4`
- `recurrent-gemma`
- `rwkv`
- `stablelm`
- `starcoder2`

#### Encoder-decoder Models

- `m2m-100`
- `seamless-m4t`
- `switch-transformers`
- `umt5`

#### Encoder-only Models

- `convbert`
- `data2vec-text`
- `deberta-v2`
- `esm`
- `flaubert`
- `ibert`
- `megatron-bert`
- `mobilebert`
- `mra`
- `nezha`
- `nystromformer`
- `splinter`
- `xlm`
- `xlm-roberta-xl`
- `xmod`

#### Multimodal Models

- `idefics2`
- `idefics3`
- `llava-next-video`
- `mllama`
- `qwen2-vl`
- `qwen3-vl`
- `siglip`

#### Speech Models

- `speech-to-text`
- `speech-to-text-2`
- `wav2vec2-conformer`

#### Vision Models

- `beit3`
- `conditional-detr`
- `convnextv2`
- `cvt`
- `depth-anything`
- `dinat`
- `dino`
- `imagegpt`
- `mobilenet-v1`
- `swinv2`
- `van`
- `vitdet`

#### Vision-text Models

- `instructblip`
- `vision-encoder-decoder`
- `xclip`

## Implemented Models

These models already have test implementations:

### Decoder-only Models

- `bloom`
- `falcon`
- `gpt2`
- `gpt_j`
- `llama`
- `mistral`
- `mixtral`
- `mpt`
- `phi`

### Encoder-decoder Models

- `bart`
- `flan_t5`
- `led`
- `marian`
- `mbart`
- `mt5`
- `pegasus`
- `speecht5`
- `t5`

### Encoder-only Models

- `albert`
- `bert`
- `camembert`
- `deberta`
- `deberta_v2`
- `distilbert`
- `electra`
- `hubert`
- `rembert`
- `roberta`
- `squeezebert`
- `wav2vec2_bert`
- `xlm_roberta`

### Multimodal Models

- `git`
- `llava`
- `paligemma`
- `pix2struct`
- `video_llava`
- `vision_text_dual_encoder`

### Speech Models

- `wav2vec2`
- `whisper`

### Unknown Models

- `align`
- `audio`
- `bigbird`
- `bit`
- `blenderbot`
- `canine`
- `clap`
- `ctrl`
- `data2vec`
- `data2vec_audio`
- `data2vec_text`
- `data2vec_vision`
- `decoder_only`
- `detr`
- `donut`
- `dpt`
- `efficientnet`
- `encodec`
- `encoder_decoder`
- `encoder_only`
- `ernie`
- `flamingo`
- `flava`
- `florence`
- `funnel`
- `gemma`
- `gpt_neo`
- `gpt_neox`
- `gptj`
- `idefics`
- `imagebind`
- `layoutlm`
- `longformer`
- `mask2former`
- `mlp-mixer`
- `multimodal`
- `musicgen`
- `opt`
- `prophetnet`
- `reformer`
- `regnet`
- `roformer`
- `sam`
- `seamless_m4t`
- `segformer`
- `sew`
- `speech_to_text`
- `speech_to_text_2`
- `tapas`
- `transfo-xl`
- `trocr_base`
- `trocr_large`
- `unispeech`
- `usm`
- `vilt`
- `vinvl`
- `vision`
- `vision_encoder_decoder`
- `wavlm`
- `xlnet`
- `yolos`

### Vision Models

- `beit`
- `convnext`
- `deit`
- `dinov2`
- `levit`
- `mobilevit`
- `poolformer`
- `resnet`
- `swin`
- `vit`

### Vision-text Models

- `blip`
- `blip_2`
- `chinese_clip`
- `clip`