#!/usr/bin/env python3

# Import hardware detection capabilities if available
try:
    from hardware_detection import (
        HAS_CUDA, HAS_ROCM, HAS_OPENVINO, HAS_MPS, HAS_WEBNN, HAS_WEBGPU,
        detect_all_hardware
    )
    HAS_HARDWARE_DETECTION = True
except ImportError:
    HAS_HARDWARE_DETECTION = False
    # We'll detect hardware manually as fallback
"""
Test implementation for t5-small

This file provides a standardized test interface for t5-small models
across different hardware backends (CPU, CUDA, OpenVINO, Apple, Qualcomm).

Generated by template_test_generator.py - 2025-03-01T21:53:18.156230
"""

import os
import sys
import json
import time
import datetime
import traceback
from unittest.mock import patch, MagicMock

# Add parent directory to path for imports
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# Third-party imports
import numpy as np

# Try/except pattern for optional dependencies
try:
    import torch
    TORCH_AVAILABLE = True
except ImportError:
    torch = MagicMock()
    TORCH_AVAILABLE = False
    print("Warning: torch not available, using mock implementation")

try:
    import transformers
    TRANSFORMERS_AVAILABLE = True
except ImportError:
    transformers = MagicMock()
    TRANSFORMERS_AVAILABLE = False
    print("Warning: transformers not available, using mock implementation")

# Model type: t5-small
# Primary task: text-generation
# All tasks: text-generation

class hf_t5_small:
    """
    T5-small implementation.
    
    This class provides standardized interfaces for working with t5-small models
    across different hardware backends (CPU, CUDA, OpenVINO, Apple, Qualcomm).
    """
    
    def __init__(self, resources=None, metadata=None):
        """Initialize the t5-small model.
        
        Args:
            resources (dict): Dictionary of shared resources (torch, transformers, etc.)
            metadata (dict): Configuration metadata
        """
        self.resources = resources or {
            "torch": torch,
            "numpy": np,
            "transformers": transformers
        }
        self.metadata = metadata or {}
        
        # Handler creation methods
        self.create_cpu_text_embedding_endpoint_handler = self.create_cpu_text_embedding_endpoint_handler
        self.create_cuda_text_embedding_endpoint_handler = self.create_cuda_text_embedding_endpoint_handler
        self.create_openvino_text_embedding_endpoint_handler = self.create_openvino_text_embedding_endpoint_handler
        self.create_apple_text_embedding_endpoint_handler = self.create_apple_text_embedding_endpoint_handler
        self.create_qualcomm_text_embedding_endpoint_handler = self.create_qualcomm_text_embedding_endpoint_handler
        
        # Initialization methods
        self.init = self.init_cpu  # Default to CPU
        self.init_cpu = self.init_cpu
        self.init_cuda = self.init_cuda
        self.init_openvino = self.init_openvino
        self.init_apple = self.init_apple
        self.init_qualcomm = self.init_qualcomm
        
        # Test methods
        self.__test__ = self.__test__
        
        # Hardware-specific utilities
        self.snpe_utils = None  # Qualcomm SNPE utils
        return None


    def init_webgpu(self, model_name=None):
        """Initialize text model for WebGPU inference using transformers.js simulation."""
        try:
            print("Initializing WebGPU for text model")
            model_name = model_name or self.model_name
            
            # Check for WebGPU support
            webgpu_support = False
            try:
                # In browser environments, check for WebGPU API
                import js
                if hasattr(js, 'navigator') and hasattr(js.navigator, 'gpu'):
                    webgpu_support = True
                    print("WebGPU API detected in browser environment")
            except ImportError:
                # Not in a browser environment
                pass
                
            # Create queue for inference requests
            import asyncio
            queue = asyncio.Queue(16)
            
            if not webgpu_support:
                # Create a WebGPU simulation using CPU implementation for text models
                print("Using WebGPU/transformers.js simulation for text model")
                
                # Initialize with CPU for simulation
                endpoint, processor, _, _, batch_size = self.init_cpu(model_name=model_name)
                
                # Wrap the CPU function to simulate WebGPU/transformers.js
    def webgpu_handler(text_input, **kwargs):
                    try:
                        # Process input with tokenizer
                        if isinstance(text_input, list):
                            inputs = processor(text_input, padding=True, truncation=True, return_tensors="pt")
                        else:
                            inputs = processor(text_input, return_tensors="pt")
                        
                        # Run inference
                        with torch.no_grad():
                            outputs = endpoint(**inputs)
                        
                        # Add WebGPU-specific metadata to match transformers.js
                        return {
                            "output": outputs,
                            "implementation_type": "SIMULATION_WEBGPU_TRANSFORMERS_JS",
                            "model": model_name,
                            "backend": "webgpu-simulation",
                            "device": "webgpu",
                            "transformers_js": {
                                "version": "2.9.0",  # Simulated version
                                "quantized": False,
                                "format": "float32",
                                "backend": "webgpu"
                            }
                        }
                    except Exception as e:
                        print(f"Error in WebGPU simulation handler: {e}")
                        return {
                            "output": f"Error: {str(e)}",
                            "implementation_type": "ERROR",
                            "error": str(e),
                            "model": model_name
                        }
                
                return endpoint, processor, webgpu_handler, queue, batch_size
            else:
                # Use actual WebGPU implementation when available
                # (This would use transformers.js in browser environments)
                print("Using native WebGPU implementation with transformers.js")
                
                # Since WebGPU API access depends on browser environment,
                # implementation details would involve JS interop
                
                # Create mock implementation for now (replace with real implementation)
                return None, None, lambda x: {"output": "Native WebGPU output", "implementation_type": "WEBGPU_TRANSFORMERS_JS"}, queue, 1
                
        except Exception as e:
            print(f"Error initializing WebGPU: {e}")
            # Fallback to a minimal mock
            import asyncio
            queue = asyncio.Queue(16)
            return None, None, lambda x: {"output": "Mock WebGPU output", "implementation_type": "MOCK_WEBGPU"}, queue, 1

    def init_webnn(self, model_name=None):
        """Initialize text model for WebNN inference."""
        try:
            print("Initializing WebNN for text model")
            model_name = model_name or self.model_name
            
            # Check for WebNN support
            webnn_support = False
            try:
                # In browser environments, check for WebNN API
                import js
                if hasattr(js, 'navigator') and hasattr(js.navigator, 'ml'):
                    webnn_support = True
                    print("WebNN API detected in browser environment")
            except ImportError:
                # Not in a browser environment
                pass
                
            # Create queue for inference requests
            import asyncio
            queue = asyncio.Queue(16)
            
            if not webnn_support:
                # Create a WebNN simulation using CPU implementation for text models
                print("Using WebNN simulation for text model")
                
                # Initialize with CPU for simulation
                endpoint, processor, _, _, batch_size = self.init_cpu(model_name=model_name)
                
                # Wrap the CPU function to simulate WebNN
    def webnn_handler(text_input, **kwargs):
                    try:
                        # Process input with tokenizer
                        if isinstance(text_input, list):
                            inputs = processor(text_input, padding=True, truncation=True, return_tensors="pt")
                        else:
                            inputs = processor(text_input, return_tensors="pt")
                        
                        # Run inference
                        with torch.no_grad():
                            outputs = endpoint(**inputs)
                        
                        # Add WebNN-specific metadata
                        return {
                            "output": outputs,
                            "implementation_type": "SIMULATION_WEBNN",
                            "model": model_name,
                            "backend": "webnn-simulation",
                            "device": "cpu"
                        }
                    except Exception as e:
                        print(f"Error in WebNN simulation handler: {e}")
                        return {
                            "output": f"Error: {str(e)}",
                            "implementation_type": "ERROR",
                            "error": str(e),
                            "model": model_name
                        }
                
                return endpoint, processor, webnn_handler, queue, batch_size
            else:
                # Use actual WebNN implementation when available
                # (This would use the WebNN API in browser environments)
                print("Using native WebNN implementation")
                
                # Since WebNN API access depends on browser environment,
                # implementation details would involve JS interop
                
                # Create mock implementation for now (replace with real implementation)
                return None, None, lambda x: {"output": "Native WebNN output", "implementation_type": "WEBNN"}, queue, 1
                
        except Exception as e:
            print(f"Error initializing WebNN: {e}")
            # Fallback to a minimal mock
            import asyncio
            queue = asyncio.Queue(16)
            return None, None, lambda x: {"output": "Mock WebNN output", "implementation_type": "MOCK_WEBNN"}, queue, 1

    def init_rocm(self, model_name, model_type, device_label="rocm:0", **kwargs):
        """Initialize model for ROCm inference.
        
        Args:
            model_name (str): Model identifier
            model_type (str): Type of model ('text-generation', etc.)
            device_label (str): GPU device ('rocm:0', 'rocm:1', etc.)
            
        Returns:
            Tuple of (endpoint, processor, handler, queue, batch_size)
        """
        try:
            import asyncio
            
            # Create processor and endpoint
            processor = self._create_mock_processor()
            endpoint = self._create_mock_endpoint()
            
            # Move to ROCm
            endpoint = endpoint.to(device_label)
            
            # Create handler
            handler = self.create_rocm_text_embedding_endpoint_handler(
                endpoint_model=model_name,
                device=device_label,
                hardware_label=device_label,
                endpoint=endpoint,
                tokenizer=processor,
                is_real_impl=True,
                batch_size=4
            )
            
            # Create queue
            queue = asyncio.Queue(32)
            batch_size = 4  # Default to larger batch size for ROCm
            
            return endpoint, processor, handler, queue, batch_size
        except Exception as e:
            print(f"Error initializing ROCm model: {e}")
            traceback.print_exc()
            
            # Return mock components on error
            import asyncio
            handler = lambda x: {"output": "Mock ROCm output", "input": x, "implementation_type": "MOCK"}
            return None, None, handler, asyncio.Queue(32), 2

    

    def init_mps(self, model_name, model_type, device_label="mps:0", **kwargs):
        """Initialize model for MPS inference.
        
        Args:
            model_name (str): Model identifier
            model_type (str): Type of model ('text-generation', etc.)
            device_label (str): GPU device ('mps:0', 'mps:1', etc.)
            
        Returns:
            Tuple of (endpoint, processor, handler, queue, batch_size)
        """
        try:
            import asyncio
            
            # Create processor and endpoint
            processor = self._create_mock_processor()
            endpoint = self._create_mock_endpoint()
            
            # Move to MPS
            endpoint = endpoint.to(device_label)
            
            # Create handler
            handler = self.create_mps_text_embedding_endpoint_handler(
                endpoint_model=model_name,
                device=device_label,
                hardware_label=device_label,
                endpoint=endpoint,
                tokenizer=processor,
                is_real_impl=True,
                batch_size=4
            )
            
            # Create queue
            queue = asyncio.Queue(32)
            batch_size = 4  # Default to larger batch size for MPS
            
            return endpoint, processor, handler, queue, batch_size
        except Exception as e:
            print(f"Error initializing MPS model: {e}")
            traceback.print_exc()
            
            # Return mock components on error
            import asyncio
            handler = lambda x: {"output": "Mock MPS output", "input": x, "implementation_type": "MOCK"}
            return None, None, handler, asyncio.Queue(32), 2

    
    def _create_mock_processor(self):
        """Create a mock processor/tokenizer for testing."""
        class MockProcessor:
            def __init__(self):
                self.vocab_size = 30000
                
            def __call__(self, text, **kwargs):
                # Handle both single strings and batches
                if isinstance(text, str):
                    batch_size = 1
                else:
                    batch_size = len(text)
                    
                return {
                    "input_ids": torch.ones((batch_size, 10), dtype=torch.long),
                    "attention_mask": torch.ones((batch_size, 10), dtype=torch.long)
                }
                
            def decode(self, token_ids, **kwargs):
                return "Decoded text from mock processor"
        
        return MockProcessor()

    def _create_mock_endpoint(self):
        """Create a mock endpoint/model for testing."""
        class MockEndpoint:
            def __init__(self):
                self.config = type('obj', (object,), {
                    'hidden_size': 768,
                    'max_position_embeddings': 512
                })
                
            def eval(self):
                return self
                
            def to(self, device):
                return self
                
            def __call__(self, **kwargs):
                # Handle inputs
                batch_size = kwargs.get("input_ids").shape[0]
                seq_len = kwargs.get("input_ids").shape[1]
                
                # Create mock output
                output = type('obj', (object,), {})
                output.last_hidden_state = torch.rand((batch_size, seq_len, 768))
                
                return output
        
        return MockEndpoint()

            def init_cpu(self, model_name, model_type, device="cpu", **kwargs):
        """Initialize model for CPU inference.
        
        Args:
            model_name (str): Model identifier
            model_type (str): Type of model ('text-generation', etc.)
            device (str): CPU identifier ('cpu')
            
        Returns:
            Tuple of (endpoint, processor, handler, queue, batch_size)
        """
        try:
            import asyncio
            
            # Create processor and endpoint
            processor = self._create_mock_processor()
            endpoint = self._create_mock_endpoint()
            
            # Create handler
            handler = self.create_cpu_text_embedding_endpoint_handler(
                endpoint_model=model_name,
                device=device,
                hardware_label="cpu",
                endpoint=endpoint,
                tokenizer=processor
            )
            
            # Create queue
            queue = asyncio.Queue(32)
            batch_size = 1
            
            return endpoint, processor, handler, queue, batch_size
        except Exception as e:
            print(f"Error initializing CPU model: {e}")
            traceback.print_exc()
            
            # Return mock components on error
            import asyncio
            handler = lambda x: {"output": "Mock CPU output", "input": x, "implementation_type": "MOCK"}
            return None, None, handler, asyncio.Queue(32), 1

            def init_cuda(self, model_name, model_type, device_label="cuda:0", **kwargs):
        """Initialize model for CUDA inference.
        
        Args:
            model_name (str): Model identifier
            model_type (str): Type of model ('text-generation', etc.)
            device_label (str): GPU device ('cuda:0', 'cuda:1', etc.)
            
        Returns:
            Tuple of (endpoint, processor, handler, queue, batch_size)
        """
        try:
            import asyncio
            
            # Create processor and endpoint
            processor = self._create_mock_processor()
            endpoint = self._create_mock_endpoint()
            
            # Move to CUDA
            endpoint = endpoint.to(device_label)
            
            # Create handler
            handler = self.create_cuda_text_embedding_endpoint_handler(
                endpoint_model=model_name,
                device=device_label,
                hardware_label=device_label,
                endpoint=endpoint,
                tokenizer=processor,
                is_real_impl=True,
                batch_size=4
            )
            
            # Create queue
            queue = asyncio.Queue(32)
            batch_size = 4  # Default to larger batch size for CUDA
            
            return endpoint, processor, handler, queue, batch_size
        except Exception as e:
            print(f"Error initializing CUDA model: {e}")
            traceback.print_exc()
            
            # Return mock components on error
            import asyncio
            handler = lambda x: {"output": "Mock CUDA output", "input": x, "implementation_type": "MOCK"}
            return None, None, handler, asyncio.Queue(32), 2

            def init_openvino(self, model_name, model_type, device="CPU", **kwargs):
        """Initialize model for OpenVINO inference.
        
        Args:
            model_name (str): Model identifier
            model_type (str): Type of model ('text-generation', etc.)
            device (str): OpenVINO device ('CPU', 'GPU', etc.)
            
        Returns:
            Tuple of (endpoint, processor, handler, queue, batch_size)
        """
        try:
            import asyncio
            import numpy as np
            
            # Create processor and endpoint (OpenVINO-specific)
            processor = self._create_mock_processor()
            
            # Create OpenVINO-style endpoint
            class MockOpenVINOModel:
                def infer(self, inputs):
                    batch_size = 1
                    seq_len = 10
                    if isinstance(inputs, dict) and 'input_ids' in inputs:
                        if hasattr(inputs['input_ids'], 'shape'):
                            batch_size = inputs['input_ids'].shape[0]
                            if len(inputs['input_ids'].shape) > 1:
                                seq_len = inputs['input_ids'].shape[1]
                    
                    # Return OpenVINO-style output
                    return {"last_hidden_state": np.random.rand(batch_size, seq_len, 768).astype(np.float32)}
            
            endpoint = MockOpenVINOModel()
            
            # Create handler
            handler = self.create_openvino_text_embedding_endpoint_handler(
                endpoint_model=model_name,
                tokenizer=processor,
                openvino_label=device,
                endpoint=endpoint
            )
            
            # Create queue
            queue = asyncio.Queue(64)
            batch_size = 1
            
            return endpoint, processor, handler, queue, batch_size
        except Exception as e:
            print(f"Error initializing OpenVINO model: {e}")
            traceback.print_exc()
            
            # Return mock components on error
            import asyncio
            handler = lambda x: {"output": "Mock OpenVINO output", "input": x, "implementation_type": "MOCK"}
            return None, None, handler, asyncio.Queue(64), 1

    def init_apple(self, model_name, model_type, device="mps", **kwargs):
        """Initialize model for Apple Silicon (M1/M2/M3) inference.
        
        Args:
            model_name (str): Model identifier
            model_type (str): Type of model ('text-generation', etc.)
            device (str): Device identifier ('mps')
            
        Returns:
            Tuple of (endpoint, processor, handler, queue, batch_size)
        """
        try:
            import asyncio
            
            # Create processor and endpoint
            processor = self._create_mock_processor()
            endpoint = self._create_mock_endpoint()
            
            # Move to MPS
            if TORCH_AVAILABLE and hasattr(torch, 'mps') and hasattr(torch.mps, 'is_available') and torch.mps.is_available():
                endpoint = endpoint.to('mps')
            
            # Create handler
            handler = self.create_apple_text_embedding_endpoint_handler(
                endpoint_model=model_name,
                apple_label=device,
                endpoint=endpoint,
                tokenizer=processor
            )
            
            # Create queue
            queue = asyncio.Queue(32)
            batch_size = 2
            
            return endpoint, processor, handler, queue, batch_size
        except Exception as e:
            print(f"Error initializing Apple Silicon model: {e}")
            traceback.print_exc()
            
            # Return mock components on error
            import asyncio
            handler = lambda x: {"output": "Mock Apple Silicon output", "input": x, "implementation_type": "MOCK"}
            return None, None, handler, asyncio.Queue(32), 2

    def init_qualcomm(self, model_name, model_type, device="qualcomm", **kwargs):
        """Initialize model for Qualcomm AI inference.
        
        Args:
            model_name (str): Model identifier
            model_type (str): Type of model ('text-generation', etc.)
            device (str): Device identifier ('qualcomm')
            
        Returns:
            Tuple of (endpoint, processor, handler, queue, batch_size)
        """
        try:
            import asyncio
            import numpy as np
            
            # Create processor
            processor = self._create_mock_processor()
            
            # Create Qualcomm-style endpoint
            class MockQualcommModel:
                def execute(self, inputs):
                    batch_size = 1
                    seq_len = 10
                    if isinstance(inputs, dict) and 'input_ids' in inputs:
                        if hasattr(inputs['input_ids'], 'shape'):
                            batch_size = inputs['input_ids'].shape[0]
                            if len(inputs['input_ids'].shape) > 1:
                                seq_len = inputs['input_ids'].shape[1]
                    
                    # Return Qualcomm-style output
                    return {"output": np.random.rand(batch_size, seq_len, 768).astype(np.float32)}
            
            endpoint = MockQualcommModel()
            
            # Create handler
            handler = self.create_qualcomm_text_embedding_endpoint_handler(
                endpoint_model=model_name,
                qualcomm_label=device,
                endpoint=endpoint,
                tokenizer=processor
            )
            
            # Create queue
            queue = asyncio.Queue(32)
            batch_size = 1
            
            return endpoint, processor, handler, queue, batch_size
        except Exception as e:
            print(f"Error initializing Qualcomm model: {e}")
            traceback.print_exc()
            
            # Return mock components on error
            import asyncio
            handler = lambda x: {"output": "Mock Qualcomm output", "input": x, "implementation_type": "MOCK"}
            return None, None, handler, asyncio.Queue(32), 1

    # Handler creation methods
                def create_cpu_text_embedding_endpoint_handler(self, endpoint_model, device, hardware_label, endpoint=None, tokenizer=None):
        """Create a handler function for CPU inference.
        
        Args:
            endpoint_model: Model name
            device: Device to run on ('cpu')
            hardware_label: Label for the endpoint
            endpoint: Model endpoint
            tokenizer: Tokenizer for the model
            
        Returns:
            A handler function that accepts text input and returns embeddings
        """
        # Create a handler that works with the endpoint and tokenizer
        def handler(text_input):
            try:
                # This should match how the actual handler would process data
                import torch
                
                # Create mock output with appropriate structure
                batch_size = 1 if isinstance(text_input, str) else len(text_input)
                tensor_output = torch.rand((batch_size, 768))  # Standard embedding size
                
                # Return dictionary with tensor and metadata instead of adding attributes to tensor
                return {
                    "tensor": tensor_output,
                    "implementation_type": "MOCK",
                    "device": "cpu",
                    "model": endpoint_model
                }
            except Exception as e:
                print(f"Error in CPU handler: {e}")
                # Return a simple dict on error
                return {"output": "Error in CPU handler", "implementation_type": "MOCK"}
                
        return handler

                def create_cuda_text_embedding_endpoint_handler(self, endpoint_model, device, hardware_label, endpoint=None, tokenizer=None, is_real_impl=False, batch_size=1):
        """Create a handler function for CUDA inference.
        
        Args:
            endpoint_model: Model name
            device: Device to run on ('cuda:0', etc.)
            hardware_label: Label for the endpoint
            endpoint: Model endpoint
            tokenizer: Tokenizer for the model
            is_real_impl: Whether this is a real implementation
            batch_size: Batch size for processing
            
        Returns:
            A handler function that accepts text input and returns embeddings
        """
        # Create a handler that works with the endpoint and tokenizer
        def handler(text_input):
            try:
                # This should match how the actual handler would process data
                import torch
                
                # Create mock output with appropriate structure
                batch_size = 1 if isinstance(text_input, str) else len(text_input)
                tensor_output = torch.rand((batch_size, 768))  # Standard embedding size
                
                # Return dictionary with tensor and metadata instead of adding attributes to tensor
                return {
                    "tensor": tensor_output,
                    "implementation_type": "MOCK",
                    "device": device,
                    "model": endpoint_model,
                    "is_cuda": True
                }
            except Exception as e:
                print(f"Error in CUDA handler: {e}")
                # Return a simple dict on error
                return {"output": "Error in CUDA handler", "implementation_type": "MOCK"}
                
        return handler

                def create_openvino_text_embedding_endpoint_handler(self, endpoint_model, tokenizer, openvino_label, endpoint=None):
        """Create a handler function for OpenVINO inference.
        
        Args:
            endpoint_model: Model name
            tokenizer: Tokenizer for the model
            openvino_label: Label for the endpoint
            endpoint: OpenVINO model endpoint
            
        Returns:
            A handler function that accepts text input and returns embeddings
        """
        # Create a handler that works with the endpoint and tokenizer
        def handler(text_input):
            try:
                # This should match how the actual handler would process data
                import torch
                
                # Create mock output with appropriate structure
                batch_size = 1 if isinstance(text_input, str) else len(text_input)
                tensor_output = torch.rand((batch_size, 768))  # Standard embedding size
                
                # Return dictionary with tensor and metadata instead of adding attributes to tensor
                return {
                    "tensor": tensor_output,
                    "implementation_type": "MOCK",
                    "device": "OpenVINO",
                    "model": endpoint_model,
                    "is_openvino": True
                }
            except Exception as e:
                print(f"Error in OpenVINO handler: {e}")
                # Return a simple dict on error
                return {"output": "Error in OpenVINO handler", "implementation_type": "MOCK"}
                
        return handler

    def create_apple_text_embedding_endpoint_handler(self, endpoint_model, apple_label, endpoint=None, tokenizer=None):
        """Create a handler function for Apple Silicon inference.
        
        Args:
            endpoint_model: Model name
            apple_label: Label for the endpoint
            endpoint: Model endpoint
            tokenizer: Tokenizer for the model
            
        Returns:
            A handler function that accepts text input and returns embeddings
        """
        # Create a handler that works with the endpoint and tokenizer
        def handler(text_input):
            try:
                # This should match how the actual handler would process data
                import torch
                
                # Create mock output with appropriate structure
                batch_size = 1 if isinstance(text_input, str) else len(text_input)
                tensor_output = torch.rand((batch_size, 768))  # Standard embedding size
                
                # Return dictionary with tensor and metadata instead of adding attributes to tensor
                return {
                    "tensor": tensor_output,
                    "implementation_type": "MOCK",
                    "device": "MPS",
                    "model": endpoint_model,
                    "is_mps": True
                }
            except Exception as e:
                print(f"Error in Apple Silicon handler: {e}")
                # Return a simple dict on error
                return {"output": "Error in Apple Silicon handler", "implementation_type": "MOCK"}
                
        return handler

    def create_qualcomm_text_embedding_endpoint_handler(self, endpoint_model, qualcomm_label, endpoint=None, tokenizer=None):
        """Create a handler function for Qualcomm AI inference.
        
        Args:
            endpoint_model: Model name
            qualcomm_label: Label for the endpoint
            endpoint: Model endpoint
            tokenizer: Tokenizer for the model
            
        Returns:
            A handler function that accepts text input and returns embeddings
        """
        # Create a handler that works with the endpoint and tokenizer
        def handler(text_input):
            try:
                # This should match how the actual handler would process data
                import torch
                
                # Create mock output with appropriate structure
                batch_size = 1 if isinstance(text_input, str) else len(text_input)
                tensor_output = torch.rand((batch_size, 768))  # Standard embedding size
                
                # Return dictionary with tensor and metadata instead of adding attributes to tensor
                return {
                    "tensor": tensor_output,
                    "implementation_type": "MOCK",
                    "device": "Qualcomm",
                    "model": endpoint_model,
                    "is_qualcomm": True
                }
            except Exception as e:
                print(f"Error in Qualcomm handler: {e}")
                # Return a simple dict on error
                return {"output": "Error in Qualcomm handler", "implementation_type": "MOCK"}
                
        return handler

    def __test__(self):
        """Run tests for this model implementation."""
        results = {}
        examples = []
        
        # Test on CPU
        try:
            print("Testing t5-small on CPU...")
            endpoint, processor, handler, queue, batch_size = self.init_cpu(
                model_name="test-t5-small-model",
                model_type="text-generation"
            )
            
            # Test with simple input
            input_text = "This is a test input for t5-small"
            output = handler(input_text)
            
            # Record results
            examples.append({
                "platform": "CPU",
                "input": input_text,
                "output_type": f"container: {str(type(output))}, tensor: {str(type(output.get('tensor', output)))}",
                "implementation_type": output.get("implementation_type", "UNKNOWN")
            })
            
            results["cpu_test"] = "Success"
        except Exception as e:
            print(f"Error testing on CPU: {e}")
            traceback.print_exc()
            results["cpu_test"] = f"Error: {str(e)}"
        
        # Test on CUDA if available
        if TORCH_AVAILABLE and hasattr(torch, 'cuda') and torch.cuda.is_available():
            try:
                print("Testing t5-small on CUDA...")
                endpoint, processor, handler, queue, batch_size = self.init_cuda(
                    model_name="test-t5-small-model",
                    model_type="text-generation"
                )
                
                # Test with simple input
                input_text = "This is a test input for t5-small on CUDA"
                output = handler(input_text)
                
                # Record results
                examples.append({
                    "platform": "CUDA",
                    "input": input_text,
                    "output_type": f"container: {str(type(output))}, tensor: {str(type(output.get('tensor', output)))}",
                    "implementation_type": output.get("implementation_type", "UNKNOWN")
                })
                
                results["cuda_test"] = "Success"
            except Exception as e:
                print(f"Error testing on CUDA: {e}")
                traceback.print_exc()
                results["cuda_test"] = f"Error: {str(e)}"
        else:
            results["cuda_test"] = "CUDA not available"
        
        # Return test results
        return {
            "results": results,
            "examples": examples,
            "timestamp": datetime.datetime.now().isoformat()
        }

# Helper function to run the test
def run_test():
    """Run a simple test of the t5-small implementation."""
    print(f"Testing t5-small implementation...")
    
    # Create instance
    model = hf_t5_small()
    
    # Run test
    test_results = model.__test__()
    
    # Print results
    print("\nTest Results:")
    for platform, result in test_results["results"].items():
        print(f"- {platform}: {result}")
    
    print("\nExamples:")
    for example in test_results["examples"]:
        print(f"- Platform: {example['platform']}")
        print(f"  Input: {example['input']}")
        print(f"  Output Type: {example['output_type']}")
        print(f"  Implementation: {example['implementation_type']}")
        print("")
    
    return test_results

if __name__ == "__main__":
    run_test()
