#!/usr/bin/env python3
"""
Test Real WebNN and WebGPU Implementations

This script tests the real WebNN and WebGPU implementations 
by running them in actual browsers with hardware acceleration.

Usage:
    python test_real_web_implementation.py --platform webgpu --browser chrome
    python test_real_web_implementation.py --platform webnn --browser edge
    
    # Run in visible mode (not headless)
    python test_real_web_implementation.py --platform webgpu --browser chrome --no-headless
    
    # Test with transformers.js bridge
    python test_real_web_implementation.py --platform transformers_js --browser chrome
"""

import os
import sys
import json
import time
import asyncio
import logging
import argparse
from pathlib import Path

# Setup logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Import implementations
try:
    from fixed_web_platform.real_webgpu_connection import RealWebGPUConnection
    from fixed_web_platform.real_webnn_connection import RealWebNNConnection
    logger.info("Successfully imported RealWebImplementation - using REAL hardware acceleration when available")
except ImportError:
    logger.error("Failed to import RealWebImplementation - trying fallback to old implementation")
    try:
        from fixed_web_platform.webgpu_implementation import RealWebGPUImplementation as RealWebGPUConnection
        from fixed_web_platform.webnn_implementation import RealWebNNImplementation as RealWebNNConnection
    except ImportError:
        logger.error("Failed to import fallback implementation")
        sys.exit(1)

# Import transformers.js bridge if available
try:
    from transformers_js_integration import TransformersJSBridge
    logger.info("Successfully imported TransformersJSBridge - can use transformers.js for real inference")
except ImportError:
    logger.warning("TransformersJSBridge not available - transformers.js integration disabled")
    TransformersJSBridge = None

# Import WebPlatformImplementation for compatibility
try:
    from implement_real_webnn_webgpu import WebPlatformImplementation, RealWebPlatformIntegration
except ImportError:
    logger.warning("WebPlatformImplementation not available - using direct connection implementation")
    WebPlatformImplementation = None
    RealWebPlatformIntegration = None

async def test_webgpu_implementation(browser_name="chrome", headless=False, model_name="bert-base-uncased"):
    """Test WebGPU implementation.
    
    Args:
        browser_name: Browser to use (chrome, firefox, edge, safari)
        headless: Whether to run in headless mode
        model_name: Model to test
        
    Returns:
        0 for success, 1 for failure
    """
    # Create implementation
    impl = RealWebGPUConnection(browser_name=browser_name, headless=headless)
    
    try:
        # Initialize
        logger.info("Initializing WebGPU implementation")
        success = await impl.initialize()
        if not success:
            logger.error("Failed to initialize WebGPU implementation")
            return 1
        
        # Get feature support
        features = impl.get_feature_support()
        logger.info(f"WebGPU feature support: {json.dumps(features, indent=2)}")
        
        # Initialize model
        logger.info(f"Initializing model: {model_name}")
        model_info = await impl.initialize_model(model_name, model_type="text")
        if not model_info:
            logger.error(f"Failed to initialize model: {model_name}")
            await impl.shutdown()
            return 1
        
        logger.info(f"Model info: {json.dumps(model_info, indent=2)}")
        
        # Run inference
        logger.info(f"Running inference with model: {model_name}")
        result = await impl.run_inference(model_name, "This is a test input for model inference.")
        if not result:
            logger.error(f"Failed to run inference with model: {model_name}")
            await impl.shutdown()
            return 1
        
        # Check if simulation was used
        is_simulation = result.get('is_simulation', True)
        using_transformers_js = result.get('using_transformers_js', False)
        implementation_type = result.get('implementation_type', 'UNKNOWN')
        
        if is_simulation:
            logger.warning("Using SIMULATION mode for WebGPU inference - this is not a real implementation")
        else:
            logger.info("Using REAL WebGPU hardware acceleration")
            
        if using_transformers_js:
            logger.info("Using transformers.js for model inference")
            
        logger.info(f"Implementation type: {implementation_type}")
        logger.info(f"Inference result: {json.dumps(result, indent=2)}")
        
        # Shutdown
        await impl.shutdown()
        logger.info("WebGPU implementation test completed successfully")
        
        # Report success/failure
        if is_simulation:
            logger.warning("Test completed but used SIMULATION mode instead of real WebGPU")
            return 2  # Partial success
        
        return 0
        
    except Exception as e:
        logger.error(f"Error testing WebGPU implementation: {e}")
        if impl:
            await impl.shutdown()
        return 1

async def test_webnn_implementation(browser_name="edge", headless=False, model_name="bert-base-uncased"):
    """Test WebNN implementation.
    
    Args:
        browser_name: Browser to use (chrome, firefox, edge, safari)
        headless: Whether to run in headless mode
        model_name: Model to test
        
    Returns:
        0 for success, 1 for failure
    """
    # Create implementation - WebNN works best with Edge
    impl = RealWebNNConnection(browser_name=browser_name, headless=headless)
    
    try:
        # Initialize
        logger.info("Initializing WebNN implementation")
        success = await impl.initialize()
        if not success:
            logger.error("Failed to initialize WebNN implementation")
            return 1
        
        # Get feature support
        features = impl.get_feature_support()
        logger.info(f"WebNN feature support: {json.dumps(features, indent=2)}")
        
        # Get backend info if available
        if hasattr(impl, 'get_backend_info'):
            backend_info = impl.get_backend_info()
            logger.info(f"WebNN backend info: {json.dumps(backend_info, indent=2)}")
        
        # Initialize model
        logger.info(f"Initializing model: {model_name}")
        model_info = await impl.initialize_model(model_name, model_type="text")
        if not model_info:
            logger.error(f"Failed to initialize model: {model_name}")
            await impl.shutdown()
            return 1
        
        logger.info(f"Model info: {json.dumps(model_info, indent=2)}")
        
        # Run inference
        logger.info(f"Running inference with model: {model_name}")
        result = await impl.run_inference(model_name, "This is a test input for model inference.")
        if not result:
            logger.error(f"Failed to run inference with model: {model_name}")
            await impl.shutdown()
            return 1
        
        # Check if simulation was used
        is_simulation = result.get('is_simulation', True)
        using_transformers_js = result.get('using_transformers_js', False)
        implementation_type = result.get('implementation_type', 'UNKNOWN')
        
        if is_simulation:
            logger.warning("Using SIMULATION mode for WebNN inference - this is not a real implementation")
        else:
            logger.info("Using REAL WebNN hardware acceleration")
            
        if using_transformers_js:
            logger.info("Using transformers.js for model inference")
            
        logger.info(f"Implementation type: {implementation_type}")
        logger.info(f"Inference result: {json.dumps(result, indent=2)}")
        
        # Shutdown
        await impl.shutdown()
        logger.info("WebNN implementation test completed successfully")
        
        # Report success/failure
        if is_simulation:
            logger.warning("Test completed but used SIMULATION mode instead of real WebNN")
            return 2  # Partial success
        
        return 0
        
    except Exception as e:
        logger.error(f"Error testing WebNN implementation: {e}")
        if impl:
            await impl.shutdown()
        return 1

async def test_transformers_js_implementation(browser_name="chrome", headless=False, model_name="bert-base-uncased"):
    """Test transformers.js implementation.
    
    Args:
        browser_name: Browser to use (chrome, firefox, edge, safari)
        headless: Whether to run in headless mode
        model_name: Model to test
        
    Returns:
        0 for success, 1 for failure
    """
    if TransformersJSBridge is None:
        logger.error("TransformersJSBridge is not available")
        return 1
    
    # Create implementation
    bridge = TransformersJSBridge(browser_name=browser_name, headless=headless)
    
    try:
        # Start bridge
        logger.info("Starting transformers.js bridge")
        success = await bridge.start()
        if not success:
            logger.error("Failed to start transformers.js bridge")
            return 1
        
        # Get features
        if bridge.features:
            logger.info(f"Browser features: {json.dumps(bridge.features, indent=2)}")
        
        # Initialize model
        logger.info(f"Initializing model: {model_name}")
        success = await bridge.initialize_model(model_name, model_type="text")
        if not success:
            logger.error(f"Failed to initialize model: {model_name}")
            await bridge.stop()
            return 1
        
        # Run inference
        logger.info(f"Running inference with model: {model_name}")
        start_time = time.time()
        result = await bridge.run_inference(model_name, "This is a test input for transformers.js.")
        inference_time = (time.time() - start_time) * 1000  # ms
        
        if not result:
            logger.error(f"Failed to run inference with model: {model_name}")
            await bridge.stop()
            return 1
        
        logger.info(f"Inference completed in {inference_time:.2f} ms")
        logger.info(f"Inference result: {json.dumps(result, indent=2)}")
        
        # Check if there was an error
        if result.get('error'):
            logger.error(f"Error in inference: {result.get('error')}")
            await bridge.stop()
            return 1
        
        # Get metrics from result
        metrics = result.get('metrics', {})
        if metrics:
            logger.info(f"Inference metrics: {json.dumps(metrics, indent=2)}")
        
        # Stop bridge
        await bridge.stop()
        logger.info("Transformers.js test completed successfully")
        
        return 0
        
    except Exception as e:
        logger.error(f"Error testing transformers.js implementation: {e}")
        if bridge:
            await bridge.stop()
        return 1

async def test_visual_implementation(browser_name="chrome", headless=False, platform="webgpu"):
    """Test visual implementation with image input.
    
    Args:
        browser_name: Browser to use (chrome, firefox, edge, safari)
        headless: Whether to run in headless mode
        platform: Platform to test (webgpu, webnn)
        
    Returns:
        0 for success, 1 for failure
    """
    # Determine image path
    image_path = os.path.abspath("test.jpg")
    if not os.path.exists(image_path):
        logger.error(f"Test image not found: {image_path}")
        return 1
    
    # Create implementation
    if platform == "webgpu":
        impl = RealWebGPUImplementation(browser_name=browser_name, headless=headless)
    else:  # webnn
        impl = RealWebNNImplementation(browser_name=browser_name, headless=headless)
    
    try:
        # Initialize
        logger.info(f"Initializing {platform} implementation")
        success = await impl.initialize()
        if not success:
            logger.error(f"Failed to initialize {platform} implementation")
            return 1
        
        # Initialize model for vision task
        model_name = "vit-base-patch16-224" if platform == "webgpu" else "resnet-50"
        
        logger.info(f"Initializing vision model: {model_name}")
        model_info = await impl.initialize_model(model_name, model_type="vision")
        if not model_info:
            logger.error(f"Failed to initialize vision model: {model_name}")
            await impl.shutdown()
            return 1
        
        # Prepare image input
        image_input = {"image": image_path}
        
        # Run inference
        logger.info(f"Running vision inference with model: {model_name}")
        result = await impl.run_inference(model_name, image_input)
        if not result:
            logger.error(f"Failed to run vision inference with model: {model_name}")
            await impl.shutdown()
            return 1
        
        # Check if simulation was used
        is_simulation = result.get('is_simulation', True)
        
        if is_simulation:
            logger.warning(f"Using SIMULATION mode for {platform} vision inference")
        else:
            logger.info(f"Using REAL {platform} hardware acceleration for vision model")
        
        logger.info(f"Vision inference result: {json.dumps(result, indent=2)}")
        
        # Shutdown
        await impl.shutdown()
        logger.info(f"{platform} vision implementation test completed successfully")
        
        # Report success/failure
        if is_simulation:
            return 2  # Partial success
        
        return 0
        
    except Exception as e:
        logger.error(f"Error testing {platform} vision implementation: {e}")
        await impl.shutdown()
        return 1

async def main_async(args):
    """Main async function."""
    # Set log level
    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)
    else:
        logging.getLogger().setLevel(logging.INFO)
    
    # Set platform-specific browser defaults
    if args.platform == "webnn" and args.browser == "chrome" and not args.browser_specified:
        logger.info("WebNN works best with Edge browser. Switching to Edge...")
        args.browser = "edge"
    
    # Print test configuration
    print(f"\n=== Testing {args.platform.upper()} Implementation ===")
    print(f"Browser: {args.browser}")
    print(f"Headless: {args.headless}")
    print(f"Model: {args.model}")
    print(f"Test Type: {args.test_type}")
    print("===================================\n")
    
    # Determine which tests to run
    if args.platform == "webgpu":
        if args.test_type == "text":
            return await test_webgpu_implementation(
                browser_name=args.browser, 
                headless=args.headless,
                model_name=args.model
            )
        elif args.test_type == "vision":
            return await test_visual_implementation(
                browser_name=args.browser,
                headless=args.headless,
                platform="webgpu"
            )
    elif args.platform == "webnn":
        if args.test_type == "text":
            return await test_webnn_implementation(
                browser_name=args.browser, 
                headless=args.headless,
                model_name=args.model
            )
        elif args.test_type == "vision":
            return await test_visual_implementation(
                browser_name=args.browser,
                headless=args.headless,
                platform="webnn"
            )
    elif args.platform == "transformers_js":
        # Test transformers.js implementation
        return await test_transformers_js_implementation(
            browser_name=args.browser,
            headless=args.headless,
            model_name=args.model
        )
    elif args.platform == "both":
        # Run both WebGPU and WebNN tests sequentially
        logger.info("Testing WebGPU implementation...")
        webgpu_result = await test_webgpu_implementation(
            browser_name=args.browser,
            headless=args.headless,
            model_name=args.model
        )
        
        # For WebNN, prefer Edge browser
        webnn_browser = "edge" if not args.browser_specified else args.browser
        logger.info(f"\nTesting WebNN implementation with {webnn_browser} browser...")
        webnn_result = await test_webnn_implementation(
            browser_name=webnn_browser,
            headless=args.headless,
            model_name=args.model
        )
        
        # Return worst result (0 = success, 1 = failure, 2 = partial success)
        return max(webgpu_result, webnn_result)
    
    # Unknown platform
    logger.error(f"Unknown platform: {args.platform}")
    return 1

def main():
    """Main function."""
    parser = argparse.ArgumentParser(description="Test real WebNN and WebGPU implementations")
    parser.add_argument("--browser", choices=["chrome", "firefox", "edge", "safari"], default="chrome",
                      help="Browser to use")
    parser.add_argument("--platform", choices=["webgpu", "webnn", "transformers_js", "both"], default="webgpu",
                      help="Platform to test")
    parser.add_argument("--headless", action="store_true", default=True,
                      help="Run in headless mode (default: True)")
    parser.add_argument("--no-headless", action="store_false", dest="headless",
                      help="Run in visible mode (not headless)")
    parser.add_argument("--model", type=str, default="bert-base-uncased",
                      help="Model to test")
    parser.add_argument("--test-type", choices=["text", "vision"], default="text",
                      help="Type of test to run")
    parser.add_argument("--verbose", action="store_true",
                      help="Enable verbose logging")
    
    args = parser.parse_args()
    
    # Keep track of whether browser was explicitly specified
    args.browser_specified = '--browser' in sys.argv
    
    # Run async main function
    if sys.version_info >= (3, 7):
        result = asyncio.run(main_async(args))
    else:
        # For Python 3.6 or lower
        loop = asyncio.get_event_loop()
        result = loop.run_until_complete(main_async(args))
    
    # Return appropriate exit code
    if result == 0:
        print("\n✅ Test completed successfully with REAL hardware acceleration")
        logger.info("Test completed successfully with REAL hardware acceleration")
    elif result == 2:
        print("\n⚠️ Test completed with SIMULATION, not real hardware acceleration")
        logger.warning("Test completed with SIMULATION, not real hardware acceleration")
    else:
        print("\n❌ Test failed")
        logger.error("Test failed")
    
    return result

if __name__ == "__main__":
    sys.exit(main())