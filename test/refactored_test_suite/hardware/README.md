# Hardware Backend Support for IPFS Accelerate Test Suite

This module provides comprehensive hardware backend support for the IPFS Accelerate test suite. It enables model testing across a variety of hardware platforms, including:

- **CPU**: Always available baseline backend for all tests
- **CUDA**: NVIDIA GPUs via PyTorch
- **ROCm**: AMD GPUs via PyTorch+ROCm
- **MPS**: Apple Metal Performance Shaders for Apple Silicon
- **OpenVINO**: Intel hardware (CPUs, GPUs, VPUs, FPGAs) via OpenVINO
- **QNN**: Qualcomm Neural Network for mobile devices via Qualcomm QNN SDK

## Features

- **Automatic Hardware Detection**: Detects available hardware backends on the system
- **Device Optimization**: Selects optimal device based on availability and model architecture
- **Model Compatibility**: Provides compatibility information for model architectures on different backends
- **Architecture-Specific Recommendations**: Recommends best backends for each model architecture
- **Fallback Mechanisms**: Gracefully falls back to CPU when requested hardware is unavailable
- **Device Settings**: Provides optimal settings for each hardware backend

## Usage

### Hardware Detection

To list all available hardware backends on your system:

```bash
python run_test_generation.py --list-devices
```

### Hardware Compatibility Matrix

To view a hardware compatibility matrix for different model architectures:

```bash
python run_test_generation.py --hardware-compatibility
```

### Test Generation with Specific Hardware

Generate tests for a specific model targeting a specific hardware backend:

```bash
python run_test_generation.py --model "bert" --device "cuda"
```

Available device options:
- `cpu`: CPU (always available)
- `cuda`: NVIDIA GPUs
- `rocm`: AMD GPUs
- `mps`: Apple Metal (M1/M2/M3 chips)
- `openvino`: Intel hardware via OpenVINO
- `qnn`: Qualcomm Neural Network
- `auto`: Automatically select the best available device (default)

### Generate Tests for All High-Priority Models on Optimal Device

```bash
python run_test_generation.py --priority high --device auto
```

## Architecture-Specific Recommendations

Different model architectures perform better on different hardware backends:

- **Text Models (BERT, RoBERTa, etc.)**: CUDA > ROCm > MPS > CPU
- **Vision Models (ViT, Swin, etc.)**: CUDA > ROCm > MPS > OpenVINO > QNN > CPU
- **Speech Models (Whisper, Wav2Vec2)**: CUDA > ROCm > OpenVINO > QNN > MPS > CPU
- **Diffusion Models (Stable Diffusion)**: CUDA > ROCm > MPS > CPU
- **Mixture of Experts Models (Mixtral)**: CUDA > ROCm > CPU
- **State Space Models (Mamba)**: CUDA > ROCm > CPU

## Requirements

Each hardware backend requires specific dependencies:

- **CUDA**: PyTorch built with CUDA support
- **ROCm**: PyTorch built with ROCm support
- **MPS**: PyTorch â‰¥1.12 with MPS support
- **OpenVINO**: OpenVINO toolkit and optimum-intel package
- **QNN**: Qualcomm QNN SDK and compatible PyTorch

## Implementation Details

The hardware backend support is implemented through several key components:

1. **hardware_detection.py**: Central module for detecting and initializing hardware backends
2. **ModelTest class improvements**: Enhanced device handling in the base test class
3. **Template updates**: All templates now support additional hardware options
4. **Command-line interface**: Extended with hardware-specific options

## Debugging

If you encounter issues with specific hardware backends, you can enable verbose logging:

```bash
LOGLEVEL=DEBUG python run_test_generation.py --list-devices
```

For hardware-specific troubleshooting:

- **CUDA**: Check `nvidia-smi` and CUDA version compatibility with PyTorch
- **ROCm**: Verify ROCm installation with `rocminfo`
- **MPS**: Check PyTorch MPS availability with `python -c "import torch; print(torch.backends.mps.is_available())"`
- **OpenVINO**: Verify OpenVINO installation with `python -c "import openvino; print(openvino.__version__)"`
- **QNN**: Check QNN SDK installation and environment variables