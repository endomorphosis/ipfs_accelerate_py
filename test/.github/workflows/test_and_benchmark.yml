name: Test and Benchmark

on:
  # Run on schedule (nightly)
  schedule:
    - cron: '0 0 * * *'  # Run at midnight UTC every day
  
  # Run on push to main branch
  push:
    branches:
      - main
    paths:
      - 'test/**'
      - 'hardware_test_templates/**'
      - 'fixed_web_platform/**'
      - '.github/workflows/test_and_benchmark.yml'
  
  # Run on pull request to main branch
  pull_request:
    branches:
      - main
    paths:
      - 'test/**'
      - 'hardware_test_templates/**'
      - 'fixed_web_platform/**'
  
  # Allow manual trigger
  workflow_dispatch:
    inputs:
      models:
        description: 'Models to test (comma-separated)'
        required: false
        default: 'BAAI/bge-small-en-v1.5,prajjwal1/bert-tiny'
      hardware:
        description: 'Hardware platforms to test (comma-separated)'
        required: false
        default: 'cpu,cuda'
      report_format:
        description: 'Report format (markdown, html, json)'
        required: false
        default: 'markdown'

env:
  BENCHMARK_DB_PATH: ./benchmark_db.duckdb
  DEPRECATE_JSON_OUTPUT: 1
  PYTHONPATH: ${{ github.workspace }}

jobs:
  test:
    name: Run Tests and Benchmarks
    runs-on: ${{ matrix.os }}
    strategy:
      fail-fast: false
      matrix:
        os: [ubuntu-latest]
        python-version: ['3.10']
        
    steps:
      - name: Checkout code
        uses: actions/checkout@v3
        with:
          fetch-depth: 0
      
      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v4
        with:
          python-version: ${{ matrix.python-version }}
          cache: 'pip'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install duckdb pandas torch torchvision torchaudio transformers pytest pytest-cov
          pip install -r test/requirements_api.txt
      
      - name: Create database directory
        run: |
          mkdir -p benchmark_db
          touch benchmark_db/.gitkeep
      
      - name: Download previous benchmark database
        uses: actions/download-artifact@v3
        with:
          name: benchmark_db
          path: .
        continue-on-error: true
      
      - name: Run model tests
        run: |
          # Get models from input or use default
          MODELS="${{ github.event.inputs.models }}"
          if [ -z "$MODELS" ]; then
            MODELS="BAAI/bge-small-en-v1.5,prajjwal1/bert-tiny"
          fi
          
          # Get hardware from input or use default
          HARDWARE="${{ github.event.inputs.hardware }}"
          if [ -z "$HARDWARE" ]; then
            HARDWARE="cpu,cuda"
          fi
          
          # Run tests
          echo "Running tests with models: $MODELS and hardware: $HARDWARE"
          python test/test_ipfs_accelerate.py --models $MODELS --endpoints $HARDWARE
      
      - name: Run benchmarks
        run: |
          # Run benchmark suite
          python test/run_benchmark_with_db.py --models BAAI/bge-small-en-v1.5,prajjwal1/bert-tiny --hardware cpu
      
      - name: Generate compatibility matrix
        run: |
          python test/generate_compatibility_matrix.py --format markdown --output compatibility_matrix.md
      
      - name: Generate test report
        run: |
          # Get report format from input or use default
          REPORT_FORMAT="${{ github.event.inputs.report_format }}"
          if [ -z "$REPORT_FORMAT" ]; then
            REPORT_FORMAT="markdown"
          fi
          
          # Generate report
          python test/test_ipfs_accelerate.py --report --format $REPORT_FORMAT --output test_report.$REPORT_FORMAT
      
      - name: Upload benchmark database
        uses: actions/upload-artifact@v3
        with:
          name: benchmark_db
          path: benchmark_db.duckdb
          retention-days: 90
      
      - name: Upload test report
        uses: actions/upload-artifact@v3
        with:
          name: test-report
          path: |
            test_report.*
            compatibility_matrix.md
          retention-days: 30
  
  deploy-report:
    name: Deploy Test Reports
    needs: test
    runs-on: ubuntu-latest
    # Only run on main branch pushes or manual triggers
    if: github.ref == 'refs/heads/main' || github.event_name == 'workflow_dispatch'
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v3
      
      - name: Download test reports
        uses: actions/download-artifact@v3
        with:
          name: test-report
          path: reports
      
      - name: Setup Pages
        uses: actions/configure-pages@v3
      
      - name: Create report index
        run: |
          echo "# IPFS Accelerate Test Reports" > reports/index.md
          echo "" >> reports/index.md
          echo "Generated on: $(date)" >> reports/index.md
          echo "" >> reports/index.md
          echo "## Available Reports" >> reports/index.md
          echo "" >> reports/index.md
          echo "- [Test Report](test_report.md)" >> reports/index.md
          echo "- [Compatibility Matrix](compatibility_matrix.md)" >> reports/index.md
          
          # Convert markdown to HTML
          pip install markdown
          python -c "import markdown; import sys; open('reports/index.html', 'w').write(markdown.markdown(open('reports/index.md').read()))"
          
          # If HTML report exists, link to it
          if [ -f "reports/test_report.html" ]; then
            echo "- [HTML Test Report](test_report.html)" >> reports/index.md
          fi
      
      - name: Upload Pages artifact
        uses: actions/upload-pages-artifact@v1
        with:
          path: reports
      
      - name: Deploy to GitHub Pages
        id: deployment
        uses: actions/deploy-pages@v2