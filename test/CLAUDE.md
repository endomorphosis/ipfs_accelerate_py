# IPFS Accelerate Python Framework - Development Guide

> **ORGANIZATION UPDATE (March 10, 2025):**
>
> The codebase has been reorganized for better maintainability with the following top-level structure:
> 
> - All generator files moved to the top-level `generators/` directory (216 files) with subdirectories:
>   - `generators/benchmark_generators/`: Benchmark generation tools
>   - `generators/models/`: Model implementations and skills
>   - `generators/runners/`: Test runner scripts
>   - `generators/skill_generators/`: Skill generation tools
>   - `generators/template_generators/`: Template generation utilities
>   - `generators/templates/`: Template files for model generation
>   - `generators/test_generators/`: Test generation tools
>   - `generators/utils/`: Utility functions
>   - `generators/hardware/`: Hardware-specific generator tools
>
> - All database-related tools moved to the top-level `duckdb_api/` directory (83 files) with subdirectories:
>   - `duckdb_api/core/`: Core database functionality
>   - `duckdb_api/migration/`: Migration tools for JSON to database
>   - `duckdb_api/schema/`: Database schema definitions 
>   - `duckdb_api/utils/`: Utility functions for database operations
>   - `duckdb_api/visualization/`: Result visualization tools
>   - `duckdb_api/distributed_testing/`: Distributed testing framework components
>
> - Web platform implementations remain in `fixed_web_platform/` directory with subdirectories:
>   - `fixed_web_platform/unified_framework/`: Unified API for cross-browser WebNN/WebGPU
>   - `fixed_web_platform/wgsl_shaders/`: WebGPU Shading Language shader implementations
>
> - CI/CD workflow files moved from `test/.github/workflows/` to the standard `.github/workflows/` location
>
> **WEBGPU/WEBNN JAVASCRIPT SDK MIGRATION (MARCH 2025):**
>
> We have successfully migrated all WebGPU/WebNN implementations from `/fixed_web_platform/` to a dedicated `ipfs_accelerate_js` folder. This migration creates a clearer separation between JavaScript-based components and Python-based components and enables independent SDK development and deployment.
>
> The new JavaScript SDK structure follows a standardized NPM package layout with TypeScript declarations:
>
> ```
> ipfs_accelerate_js/
> â”œâ”€â”€ dist/           # Compiled output
> â”œâ”€â”€ src/            # Source code
> â”‚   â”œâ”€â”€ api_backends/     # API client implementations
> â”‚   â”œâ”€â”€ browser/          # Browser-specific optimizations
> â”‚   â”‚   â”œâ”€â”€ optimizations/    # Browser-specific optimization techniques
> â”‚   â”‚   â””â”€â”€ resource_pool/    # Resource pooling and management
> â”‚   â”œâ”€â”€ core/             # Core functionality 
> â”‚   â”œâ”€â”€ hardware/         # Hardware abstraction and detection
> â”‚   â”‚   â”œâ”€â”€ backends/         # WebGPU, WebNN backends
> â”‚   â”‚   â””â”€â”€ detection/        # Hardware capability detection
> â”‚   â”œâ”€â”€ model/            # Model implementations
> â”‚   â”‚   â”œâ”€â”€ audio/            # Audio models (Whisper, CLAP)
> â”‚   â”‚   â”œâ”€â”€ loaders/          # Model loading utilities
> â”‚   â”‚   â”œâ”€â”€ templates/        # Model templates
> â”‚   â”‚   â”œâ”€â”€ transformers/     # NLP models (BERT, T5, LLAMA)
> â”‚   â”‚   â””â”€â”€ vision/           # Vision models (ViT, CLIP, DETR)
> â”‚   â”œâ”€â”€ optimization/     # Performance optimization
> â”‚   â”‚   â”œâ”€â”€ memory/           # Memory optimization
> â”‚   â”‚   â””â”€â”€ techniques/       # Optimization techniques
> â”‚   â”œâ”€â”€ p2p/              # P2P integration
> â”‚   â”œâ”€â”€ quantization/     # Model quantization
> â”‚   â”‚   â””â”€â”€ techniques/       # Quantization techniques  
> â”‚   â”œâ”€â”€ react/            # React integration
> â”‚   â”œâ”€â”€ storage/          # Storage management
> â”‚   â”‚   â””â”€â”€ indexeddb/        # IndexedDB implementation
> â”‚   â”œâ”€â”€ tensor/           # Tensor operations
> â”‚   â”œâ”€â”€ utils/            # Utility functions
> â”‚   â””â”€â”€ worker/           # Web Workers
> â”‚       â”œâ”€â”€ wasm/             # WebAssembly support
> â”‚       â”œâ”€â”€ webgpu/           # WebGPU implementation
> â”‚       â”‚   â”œâ”€â”€ compute/          # Compute operations
> â”‚       â”‚   â”œâ”€â”€ pipeline/         # Pipeline management
> â”‚       â”‚   â””â”€â”€ shaders/          # WGSL shaders
> â”‚       â”‚       â”œâ”€â”€ chrome/           # Chrome-optimized shaders
> â”‚       â”‚       â”œâ”€â”€ edge/             # Edge-optimized shaders
> â”‚       â”‚       â”œâ”€â”€ firefox/          # Firefox-optimized shaders
> â”‚       â”‚       â”œâ”€â”€ model_specific/   # Model-specific shaders
> â”‚       â”‚       â””â”€â”€ safari/           # Safari-optimized shaders
> â”‚       â””â”€â”€ webnn/             # WebNN implementation
> â”œâ”€â”€ test/            # Test files
> â”‚   â”œâ”€â”€ browser/         # Browser-specific tests
> â”‚   â”œâ”€â”€ integration/     # Integration tests
> â”‚   â”œâ”€â”€ performance/     # Performance benchmarks
> â”‚   â””â”€â”€ unit/            # Unit tests
> â”œâ”€â”€ examples/        # Example applications
> â”‚   â”œâ”€â”€ browser/         # Browser examples
> â”‚   â”‚   â”œâ”€â”€ basic/           # Basic usage examples
> â”‚   â”‚   â”œâ”€â”€ advanced/        # Advanced examples
> â”‚   â”‚   â”œâ”€â”€ react/           # React integration examples
> â”‚   â”‚   â””â”€â”€ streaming/       # Streaming inference examples
> â”‚   â””â”€â”€ node/            # Node.js examples
> â””â”€â”€ docs/            # Documentation
>     â”œâ”€â”€ api/             # API reference
>     â”œâ”€â”€ architecture/    # Architecture guides
>     â”œâ”€â”€ examples/        # Example guides
>     â””â”€â”€ guides/          # User guides
> ```
>
> The migration completed on March 11, 2025, with the following achievements:
> - 790 files processed and migrated
> - 757 Python files converted to TypeScript
> - 33 JavaScript/WGSL files copied with appropriate organization
> - 11 browser-specific WGSL shaders properly organized
> - 0 conversion failures

## Current Focus Areas (Q2 2025):

- ðŸ”„ **WebGPU/WebNN Resource Pool Integration** (IN PROGRESS - 100% complete)
  - âœ… Enables concurrent execution of multiple AI models across heterogeneous browser backends
  - âœ… Creates browser-aware load balancing for model type optimization
  - âœ… Implements connection pooling for browser instance lifecycle management
  - âœ… Fault-tolerant cross-browser model sharding with recovery (COMPLETED)
  - âœ… Transaction-based state management for browser resources (COMPLETED)
  - âœ… Performance history tracking and trend analysis (COMPLETED)
  - âœ… Browser-specific optimizations based on performance history (COMPLETED - May 14, 2025)
  - âœ… Integration with Distributed Testing Framework for enhanced reliability (COMPLETED)
  - Target completion: May 25, 2025 (ahead of schedule)
  
- ðŸ”„ **Distributed Testing Framework** (IN PROGRESS - 25% complete)
  - âœ… Designed high-performance distributed test execution system (COMPLETED - May 8, 2025)
  - âœ… Initial implementation of core components (COMPLETED - May 12, 2025)
  - âœ… Created secure worker node registration and management system with JWT (COMPLETED - May 20, 2025)
  - ðŸ”„ Implementing intelligent result aggregation and analysis pipeline (IN PROGRESS - 30% complete)
  - ðŸ”² Develop adaptive load balancing for optimal test distribution (PLANNED - May 29-June 5, 2025) 
  - ðŸ”² Enhance support for heterogeneous hardware environments (PLANNED - June 5-12, 2025)
  - ðŸ”² Create fault tolerance system with automatic retries and fallbacks (PLANNED - June 12-19, 2025)
  - ðŸ”² Design comprehensive monitoring dashboard for distributed tests (PLANNED - June 19-26, 2025)
  - Target completion: June 26, 2025

- ðŸ”„ **Integration and Extensibility for Distributed Testing** (IN PROGRESS - 40% complete)
  - âœ… Plugin architecture for framework extensibility (COMPLETED - May 22, 2025)
  - âœ… WebGPU/WebNN Resource Pool Integration with fault tolerance (COMPLETED - May 22, 2025)
  - ðŸ”„ CI/CD system integrations (GitHub Actions, Jenkins, GitLab) (IN PROGRESS - 30% complete)
  - ðŸ”„ External system connectors via plugin interface (IN PROGRESS - 25% complete)
  - ðŸ”„ Standardized APIs with comprehensive documentation (IN PROGRESS - 15% complete)
  - ðŸ”² Custom scheduler extensibility through plugins (PLANNED - June 1-10, 2025)
  - ðŸ”² Notification system integration (PLANNED - June 10-17, 2025)
  - Target completion: July 10, 2025 (revised from July 31, 2025 due to accelerated progress)

- ðŸ”„ **WebGPU/WebNN Migration to ipfs_accelerate_js** (IN PROGRESS - 95% complete)
  - âœ… Created dedicated folder structure for JavaScript SDK components
  - âœ… Implemented clear separation between JavaScript and Python components
  - âœ… Organized code with proper module structure for better maintainability
  - âœ… Migrated 790 files including all core implementations (March 11, 2025)
  - âœ… Established browser-specific shader optimizations for Firefox, Chrome, and Safari
  - ðŸ”„ Final testing and import path validation (IN PROGRESS)
  - ðŸ”² JavaScript SDK package publishing and documentation
  - Target completion: April 2025 (accelerated from original Q3 2025 target)

- ðŸ“‹ **Advanced Visualization System** (PLANNED)
  - Design interactive 3D visualization components for multi-dimensional data (PLANNED - June 1-7, 2025)
  - Create dynamic hardware comparison heatmaps by model families (PLANNED - June 8-14, 2025)
  - Implement power efficiency visualization tools with interactive filters (PLANNED - June 15-21, 2025)
  - Develop animated visualizations for time-series performance data (PLANNED - June 22-28, 2025)
  - Create customizable dashboard system with saved configurations (PLANNED - June 29-July 5, 2025)
  - Target completion: July 15, 2025

- âœ… **Predictive Performance System** (COMPLETED - 100%)
  - âœ… Designed ML architecture for performance prediction on untested configurations
  - âœ… Developed comprehensive dataset from existing performance data
  - âœ… Created core ML model training pipeline with hyperparameter optimization
  - âœ… Implemented confidence scoring system for prediction reliability
  - âœ… Developed active learning pipeline for targeting high-value test configurations
  - âœ… Created integrated scoring system for uncertainty and diversity metrics
  - âœ… Implemented multi-model execution support with resource contention modeling
  - âœ… Completed multi-model resource pool integration for empirical validation
  - Completed May 11, 2025 (ahead of schedule)

- ðŸ“‹ **Simulation Accuracy and Validation Framework** (PLANNED - July 2025)
  - Design comprehensive simulation validation methodology
  - Implement simulation vs. real hardware comparison pipeline
  - Create statistical validation tools for simulation accuracy
  - Develop simulation calibration system based on real hardware results
  - Build automated detection for simulation drift over time
  - Target completion: October 15, 2025

## Key Features and Components

### Cross-Model Tensor Sharing

The Cross-Model Tensor Sharing system enables efficient sharing of tensors between multiple models, 
significantly improving memory efficiency and performance for multi-model workloads:

- **Shared Tensor Memory**: Multiple models can share the same tensor memory for common components
- **Reference Counting**: Intelligent memory management ensures tensors are only freed when no longer needed
- **Zero-Copy Tensor Views**: Create views into tensors without duplicating memory
- **Browser Storage Types**: Support for different tensor storage formats (CPU, WebGPU, WebNN)
- **Automatic Memory Optimization**: Identifies and frees unused tensors to reduce memory footprint
- **Intelligent Sharing Patterns**: Automatically identifies which models can share tensors

### Performance Benefits

- **Memory Reduction**: Up to 30% memory reduction for common multi-model workflows
- **Inference Speedup**: Up to 30% faster inference when reusing cached embeddings
- **Increased Throughput**: Higher throughput when running multiple related models
- **Browser Resource Efficiency**: More efficient use of limited browser memory resources

### Tensor Sharing Types

The system automatically identifies compatible model combinations for sharing:

| Tensor Type | Compatible Models | Description |
|-------------|------------------|-------------|
| text_embedding | BERT, T5, LLAMA, BART | Text embeddings for NLP models |
| vision_embedding | ViT, CLIP, DETR | Vision embeddings for image models |
| audio_embedding | Whisper, Wav2Vec2, CLAP | Audio embeddings for speech/audio models |
| vision_text_joint | CLIP, LLaVA, BLIP | Joint embeddings for multimodal models |
| audio_text_joint | CLAP, Whisper-Text | Joint embeddings for audio-text models |

### WebNN and WebGPU Benchmarking Tools

The framework includes comprehensive tools for benchmarking real WebNN and WebGPU implementations in browsers with clear distinction between real hardware acceleration and simulation mode:

```bash
# Run WebGPU benchmarks with Chrome
python benchmark_real_webnn_webgpu.py --webgpu --chrome

# Run WebNN benchmarks with Edge (best WebNN support)
python benchmark_real_webnn_webgpu.py --webnn --edge

# Run audio model benchmarks with Firefox (best for compute shaders)
python benchmark_real_webnn_webgpu.py --audio --firefox

# Benchmark with quantization (8-bit)
python benchmark_real_webnn_webgpu.py --text --bits 8

# Benchmark with mixed precision (4-bit)
python benchmark_real_webnn_webgpu.py --text --bits 4 --mixed-precision

# Run comprehensive benchmarks across multiple models
python benchmark_real_webnn_webgpu.py --comprehensive

# Store results in database
python benchmark_real_webnn_webgpu.py --text --db-path ./benchmark_db.duckdb

# Generate HTML report
python benchmark_real_webnn_webgpu.py --text --output-format html

# Check browser capabilities for WebNN/WebGPU support
python check_browser_webnn_webgpu.py --browser firefox

# Fix WebNN/WebGPU benchmarking issues
python fix_real_webnn_webgpu_benchmarks.py --browser chrome --fix-all
```

### Browser-Specific Optimizations

Different browsers excel at different tasks:

| Browser | Best For | Features | Command Flag |
|---------|----------|----------|-------------|
| Firefox | Audio models | 20-25% better performance for Whisper, CLAP | `--browser firefox --optimize-audio` |
| Edge | WebNN models | Superior WebNN implementation | `--browser edge --platform webnn` |
| Chrome | Vision models | Solid all-around WebGPU support | `--browser chrome --platform webgpu` |

For detailed instructions, see:
- [WebNN/WebGPU Benchmark System](WEBNN_WEBGPU_BENCHMARK_README.md)
- [Real WebNN/WebGPU Implementation Update](REAL_WEBNN_WEBGPU_IMPLEMENTATION_UPDATE.md)

## Web Resource Pool Integration

The WebGPU/WebNN Resource Pool Integration enables concurrent execution of multiple AI models across heterogeneous browser backends. It dramatically improves throughput, reduces resource waste, and provides fine-grained control over browser-based hardware acceleration resources. The system is now being enhanced with fault tolerance features based on the recently completed distributed testing framework implementation.

### Key Features

- **Concurrent Model Execution**: Run multiple models simultaneously (3.5x throughput improvement)
- **Connection Pooling**: Efficiently manage browser connections with lifecycle management
- **Browser-Aware Load Balancing**: Distribute models to optimal browsers based on model type
- **Adaptive Resource Scaling**: Dynamically adjust resource allocation based on demand
- **Real-Time Monitoring**: Track resource utilization and performance metrics
- **Fault-Tolerant Model Sharding**: Distribute model execution across multiple browsers with failover capabilities
- **Cross-Browser Recovery**: Automatically recover from browser crashes or disconnections
- **Transaction-Based State Management**: Ensure consistent state across browser instances
- **Performance History Analysis**: Track and analyze performance trends to optimize resource allocation

### Using the Resource Pool with Fault Tolerance

```python
# Create resource pool integration with fault tolerance
from fixed_web_platform.resource_pool_bridge import ResourcePoolBridgeIntegration

integration = ResourcePoolBridgeIntegration(
    max_connections=4,
    browser_preferences={
        'audio': 'firefox',     # Firefox for audio models
        'vision': 'chrome',     # Chrome for vision models
        'text_embedding': 'edge' # Edge for embedding models
    },
    adaptive_scaling=True,
    enable_fault_tolerance=True,  # Enable fault tolerance features
    recovery_strategy='progressive',  # Use progressive recovery strategy
    state_sync_interval=5,  # Sync state every 5 seconds
    redundancy_factor=2  # Keep redundant copies for critical operations
)

# Initialize the integration
integration.initialize()

# Get model from resource pool with fault tolerance
model = integration.get_model(
    model_type='text_embedding',
    model_name='bert-base-uncased',
    hardware_preferences={'priority_list': ['webgpu', 'cpu']},
    fault_tolerance={
        'recovery_timeout': 30,  # Maximum recovery time in seconds
        'state_persistence': True,  # Persist state between sessions
        'failover_strategy': 'immediate'  # Immediate failover on error
    }
)

# Run inference with automatic recovery
try:
    result = model(inputs)
except BrowserError as e:
    # Automatic recovery will be attempted based on configured strategy
    # If successful, the operation will continue
    print(f"Recovered from: {e}")
```

### Cross-Browser Model Sharding

The new fault-tolerant cross-browser model sharding feature allows running large models by distributing them across multiple browser instances:

```python
# Set up sharded model execution
from fixed_web_platform.model_sharding import ShardedModelExecution

sharded_execution = ShardedModelExecution(
    model_name="llama-13b",
    sharding_strategy="layer_balanced",  # Distribute model by layers
    num_shards=3,  # Split across 3 browser instances
    fault_tolerance_level="high",  # High level of fault tolerance
    recovery_strategy="coordinated",  # Synchronized recovery
    connection_pool=integration.connection_pool  # Use existing pool
)

# Initialize sharded execution
sharded_execution.initialize()

# Run inference on sharded model with automatic recovery
result = sharded_execution.run_inference(inputs)
```

### Performance History and Analysis

The performance history tracking feature allows optimization based on historical data:

```python
# Access performance history
history = integration.get_performance_history(
    model_type="text_embedding",
    time_range="7d",  # Last 7 days
    metrics=["latency", "throughput", "browser_utilization"]
)

# Analyze trends and get recommendations
recommendations = integration.analyze_performance_trends(history)

# Apply recommendations automatically
integration.apply_performance_optimizations(recommendations)
```

### Running Tests

```bash
# Test resource pool with fault tolerance features
python test_web_resource_pool.py --models bert,vit,whisper --fault-tolerance

# Test cross-browser model sharding with recovery
python test_web_resource_pool.py --test-sharding --recovery-tests

# Test concurrent model execution with fault injection
python test_web_resource_pool.py --concurrent-models --fault-injection --models bert,vit,whisper

# Run stress test with high concurrency and simulated failures
python test_web_resource_pool.py --stress-test --simulate-failures --duration 120

# Test transaction-based state management
python test_web_resource_pool.py --test-state-management --sync-interval 5

# Run comprehensive fault tolerance benchmark
python benchmark_resource_pool_fault_tolerance.py --comprehensive
```

For detailed documentation, see:
- [WEB_RESOURCE_POOL_INTEGRATION.md](WEB_RESOURCE_POOL_INTEGRATION.md) - Comprehensive guide
- [WEBNN_WEBGPU_DATABASE_INTEGRATION.md](WEBNN_WEBGPU_DATABASE_INTEGRATION.md) - Database integration details

## Documentation and Reference

For a complete overview of all available documentation, refer to:
- [DOCUMENTATION_INDEX.md](DOCUMENTATION_INDEX.md) - Comprehensive index of all project documentation with categorization

Major documentation categories include:
- [PHASE16_COMPLETION_REPORT.md](PHASE16_COMPLETION_REPORT.md) - Comprehensive report on the completed Phase 16 implementation
- [HARDWARE_BENCHMARKING_GUIDE.md](HARDWARE_BENCHMARKING_GUIDE.md) - Main hardware benchmarking documentation
- [BENCHMARK_DATABASE_GUIDE.md](BENCHMARK_DATABASE_GUIDE.md) - Benchmark database architecture and usage
- [WEB_PLATFORM_INTEGRATION_GUIDE.md](WEB_PLATFORM_INTEGRATION_GUIDE.md) - Web platform integration guide
- [REAL_WEBNN_WEBGPU_IMPLEMENTATION_UPDATE.md](REAL_WEBNN_WEBGPU_IMPLEMENTATION_UPDATE.md) - Latest WebNN/WebGPU implementation
- [WEBNN_WEBGPU_BENCHMARK_README.md](WEBNN_WEBGPU_BENCHMARK_README.md) - Overview of WebNN/WebGPU benchmark system
- [WEBNN_WEBGPU_DATABASE_INTEGRATION.md](WEBNN_WEBGPU_DATABASE_INTEGRATION.md) - How WebNN/WebGPU integrates with DuckDB
- [WEB_RESOURCE_POOL_INTEGRATION.md](WEB_RESOURCE_POOL_INTEGRATION.md) - Resource pool integration with web platform
- [TEMPLATE_INHERITANCE_GUIDE.md](TEMPLATE_INHERITANCE_GUIDE.md) - Template inheritance system documentation