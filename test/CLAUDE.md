# IPFS Accelerate Python Framework - Development Guide

## Current Focus: Advanced Hardware Benchmarking and Database Consolidation (Updated March 2025)
## Enhanced Feature: Added Qualcomm AI Engine Support (Updated March 2025)

### Project Status Overview

The project has successfully completed 16 phases of implementation, focusing on test-driven development, hardware compatibility, model optimization, cross-platform support, and data management. Key accomplishments include:

- ‚úÖ Complete development pipeline for test and skillset generators
- ‚úÖ Comprehensive hardware detection and compatibility system
- ‚úÖ Advanced resource management system with hardware awareness
- ‚úÖ Web platform integration (WebNN and WebGPU)
- ‚úÖ Model family classification and compatibility matrix 
- ‚úÖ Integration testing and platform support
- ‚úÖ Advanced model compression and optimization
- ‚úÖ Complete hardware platform test coverage for key models
- ‚úÖ Test results database architecture and core components implemented (100% complete)
- ‚úÖ Historical data migration pipeline implemented (100% complete)
- ‚úÖ CI/CD integration for automated benchmark storage (100% complete)

### Current Focus: Phase 16 - Advanced Hardware Benchmarking and Database Consolidation (100% Complete)
### Updated Focus: Web Platform Integration and Framework (100% Complete)
### New Focus: Template-Based Generation System for 300+ HuggingFace Models (95% Complete)

#### Template-Based Generation System
- ‚úÖ Store templates for 300+ HuggingFace model classes in DuckDB (100% complete)
- ‚úÖ Develop database schema for storing templates, helpers, and dependencies (100% complete)
- ‚úÖ Create dynamic template retrieval system from database (100% complete)
- ‚úÖ Ensure cross-platform hardware compatibility in all templates (100% complete)
- ‚úÖ Implement hardware-aware template instantiation (100% complete)
- ‚úÖ Support template inheritance for model families (100% complete)
- ‚úÖ Migrate generators to use database templates instead of static files (95% complete)
- ‚úÖ Add template versioning and dependency tracking (100% complete)
- üîÑ Complete template validation system for all generators (95% complete)

#### Hardware Performance Work
- ‚úÖ Create comprehensive benchmark database for all model-hardware combinations (100% complete)
- ‚úÖ Implement comparative analysis reporting system for hardware performance (100% complete)
- ‚úÖ Create automated hardware selection based on benchmarking data (100% complete)
- ‚úÖ Implement training mode test coverage in addition to inference (100% complete)
- ‚úÖ Complete cross-platform test coverage for 13 key model classes (100% complete)
- ‚úÖ Develop specialized web platform tests for audio models (100% complete)
- ‚úÖ Implement distributed training test suite (100% complete)
- ‚úÖ Add performance prediction for model-hardware combinations (100% complete)

#### Database Restructuring Effort
- ‚úÖ Consolidate benchmark and test output JSON files into DuckDB/Parquet for efficient storage and querying (100% complete)
- ‚úÖ Design unified schema for all test result types (100% complete)
- ‚úÖ Develop data migration pipeline for historical test data (100% complete)
- ‚úÖ Create programmatic database interface for test runners (100% complete)
- ‚úÖ Build analysis and visualization tools on the new database (100% complete)
- ‚úÖ Integrate database with CI/CD pipeline for automatic result storage (100% complete)
- ‚úÖ Implement comprehensive data migration system with validation and tracking (100% complete)
- ‚úÖ Migrate all benchmark scripts to use DuckDB for storage and querying (100% complete)
- ‚úÖ Complete tool integration with all test runners (100% complete)
- ‚úÖ Develop advanced analytics dashboard with interactive visualizations (100% complete)
- ‚úÖ Complete CI/CD integration with GitHub Actions workflow (100% complete)
- ‚úÖ Set DEPRECATE_JSON_OUTPUT=1 as default in all benchmark scripts (COMPLETED - March 5, 2025)
- ‚úÖ Archive all legacy JSON files and complete migration to DuckDB (COMPLETED - March 5, 2025)
- ‚úÖ JSON output fully deprecated in favor of database storage for all benchmarks (COMPLETED - March 5, 2025)
- ‚úÖ Integrate test_ipfs_accelerate.py with DuckDB for all test results (COMPLETED - March 6, 2025)
- ‚úÖ Add advanced visualizations for hardware performance comparisons (COMPLETED - March 6, 2025)
- ‚úÖ Add power efficiency and thermal metrics for mobile/edge hardware (COMPLETED - March 6, 2025)

## Time-Series Performance Tracking (COMPLETED - March 25, 2025)

The framework now includes a comprehensive time-series performance tracking system with these features:

- Versioned test results with git commit and environment information
- Regression detection based on configurable thresholds
- Trend analysis with statistical methods
- Visualization capabilities for performance metrics
- Reporting in Markdown and HTML formats
- Notification system for detected regressions 

```bash
# Run a quick test of the time-series performance tracker
python test/run_time_series_performance.py --quick-test

# Run the full test suite
python test/run_time_series_performance.py --full-test

# Record a performance result
python test/time_series_performance.py record --model-id 1 --hardware-id 1 --batch-size 4 --throughput 125.7 --latency 8.2 --memory 1024 --power 180

# Set baselines for all model-hardware combinations
python test/time_series_performance.py baseline --all --days 7 --min-samples 3

# Detect regressions
python test/time_series_performance.py regression --days 14 --notify

# Analyze trends
python test/time_series_performance.py trend --metric throughput --days 30 --visualize

# Generate a performance report
python test/time_series_performance.py report --days 30 --format markdown --output performance_report.md
```

For detailed documentation, see [Time-Series Performance Tracking Guide](TIME_SERIES_PERFORMANCE_GUIDE.md).

## Comprehensive Benchmark Timing Report (COMPLETED - March 6, 2025)

The framework includes a comprehensive benchmark timing report generator that provides detailed analysis of performance metrics for all 13 model types across 8 hardware endpoints:

- Detailed latency, throughput, and memory usage metrics
- Cross-hardware platform performance comparison
- Visualizations for performance metrics (HTML and Markdown formats)
- Categorized model performance by type (text, vision, audio, multimodal)
- Data-driven optimization recommendations based on model categories
- Consistent DuckDB database schema for all benchmark data
- Support for sample data generation for testing and demos

## Comprehensive Benchmarks and Timing Data (UPDATED - April 8, 2025)

The framework includes full benchmark execution and timing data for all model types across all hardware platforms:

- Comprehensive benchmarks for all 13 model types across 8 hardware platforms
- Detailed performance metrics including latency, throughput, and memory usage
- Hardware compatibility matrix with optimization recommendations
- HTML and Markdown reports with detailed performance comparisons
- Interactive visualizations for comparing hardware platforms
- Power efficiency metrics for mobile/edge devices

```bash
# Execute comprehensive benchmarks using the new script (April 2025 Update)
python test/run_comprehensive_benchmarks.py

# Run specific models on specific hardware
python test/run_comprehensive_benchmarks.py --models bert,t5,vit --hardware cpu,cuda

# Specify batch sizes to test
python test/run_comprehensive_benchmarks.py --batch-sizes 1,4,16

# Force benchmarks on hardware that may not be available
python test/run_comprehensive_benchmarks.py --force-hardware rocm,webgpu

# List available hardware platforms
python test/run_comprehensive_benchmarks.py --list-available-hardware

# Run benchmarks on all supported hardware platforms (may use simulation)
python test/run_comprehensive_benchmarks.py --all-hardware

# Use full-sized models instead of smaller variants
python test/run_comprehensive_benchmarks.py --no-small-models

# Generate report in different formats
python test/run_comprehensive_benchmarks.py --report-format markdown

# Set a custom timeout for benchmarks
python test/run_comprehensive_benchmarks.py --timeout 1200  # 20 minutes

# Specify database path and output directory
python test/run_comprehensive_benchmarks.py --db-path ./benchmark_db.duckdb --output-dir ./benchmark_results

# Legacy method: Execute comprehensive benchmarks across all hardware platforms
python benchmark_all_key_models.py --output-dir ./benchmark_results

# Run with small model variants for faster testing
python benchmark_all_key_models.py --small-models --output-dir ./benchmark_results

# Generate comprehensive benchmark timing report in multiple formats
python benchmark_timing_report.py --generate --format html --output report.html
python benchmark_timing_report.py --generate --format markdown --output report.md

# Generate hardware compatibility matrix with visualization
python get_compatibility_matrix.py
```

```bash
# Generate comprehensive benchmark timing report in HTML format
python test/run_benchmark_timing_report.py --generate --format html

# Generate report in Markdown format
python test/run_benchmark_timing_report.py --generate --format markdown

# Specify custom output location and database path
python test/run_benchmark_timing_report.py --generate --format html --output report.html --db-path ./benchmark_db.duckdb

# Generate sample benchmark data for testing
python test/generate_sample_benchmarks.py --db ./benchmark_db.duckdb

# Run real benchmarks with database integration
python test/benchmark_all_key_models.py --small-models --db-path ./benchmark_db.duckdb --db-only

# Generate model-hardware performance report
python test/scripts/benchmark_db_query.py --sql "SELECT m.model_name, hp.hardware_type, AVG(pr.average_latency_ms) as avg_latency, AVG(pr.throughput_items_per_second) as avg_throughput FROM performance_results pr JOIN models m ON pr.model_id = m.model_id JOIN hardware_platforms hp ON pr.hardware_id = hp.hardware_id GROUP BY m.model_name, hp.hardware_type ORDER BY m.model_name, hp.hardware_type" --db ./benchmark_db.duckdb --format markdown --output performance_summary.md
```

The report includes specialized views for:
- Text models (BERT, T5, LLAMA, Qwen2)
- Vision models (ViT, DETR, XCLIP)
- Audio models (Whisper, Wav2Vec2, CLAP)
- Multimodal models (CLIP, LLaVA, LLaVA-Next)
- Memory-intensive vs compute-intensive models

Performance data is stored in the DuckDB database for efficient querying and visualization, with comprehensive metrics showing optimal hardware selection for each model category.

For detailed documentation, see [Benchmark Timing Report Guide](BENCHMARK_TIMING_REPORT_GUIDE.md).

## Hardware Compatibility Matrix

### Model Family-Based Compatibility Chart

| Model Family | CPU | CUDA | ROCm | MPS | OpenVINO | QNN | Samsung | WebNN | WebGPU | Notes |
|--------------|-----|------|------|-----|----------|-----|---------|-------|--------|-------|
| Embedding (BERT, etc.) | ‚úÖ High | ‚úÖ High | ‚úÖ High | ‚úÖ High | ‚úÖ High | ‚úÖ High | ‚úÖ High | ‚úÖ High | ‚úÖ High | Fully supported on all hardware |
| Text Generation (LLMs) | ‚úÖ Medium | ‚úÖ High | ‚úÖ Medium | ‚úÖ Medium | ‚úÖ Medium | ‚úÖ Medium | ‚úÖ Medium | ‚ö†Ô∏è Limited | ‚ö†Ô∏è Limited | Memory requirements critical |
| Vision (ViT, CLIP, etc.) | ‚úÖ Medium | ‚úÖ High | ‚úÖ High | ‚úÖ High | ‚úÖ High | ‚úÖ High | ‚úÖ High | ‚úÖ High | ‚úÖ High | Full cross-platform support |
| Audio (Whisper, etc.) | ‚úÖ Medium | ‚úÖ High | ‚úÖ Medium | ‚úÖ Medium | ‚úÖ Medium | ‚úÖ Medium | ‚úÖ High | ‚ö†Ô∏è Limited | ‚ö†Ô∏è Limited | CUDA preferred, Web simulation added |
| Multimodal (LLaVA, etc.) | ‚ö†Ô∏è Limited | ‚úÖ High | ‚ö†Ô∏è Limited | ‚ö†Ô∏è Limited | ‚ö†Ô∏è Limited | ‚ö†Ô∏è Limited | ‚ö†Ô∏è Limited | ‚ö†Ô∏è Limited | ‚ö†Ô∏è Limited | CUDA for production, others are limited |

### IPFS Acceleration Testing Features (Updated March 2025)

The framework now includes comprehensive IPFS acceleration testing with enhanced DuckDB integration, Qualcomm QNN, and WebGPU support:

1. **Database-First Storage**: Complete integration with DuckDB for efficient and reliable test results storage:
   ```bash
   # Store results only in database (no JSON files)
   python test_ipfs_accelerate.py --models "bert-base-uncased" --db-only
   
   # Use custom database path
   python test_ipfs_accelerate.py --db-path ./custom_benchmark.duckdb --models "bert-base-uncased"
   ```

2. **Qualcomm AI Engine Support**: Test with Qualcomm QNN hardware acceleration:
   ```bash
   # Test with Qualcomm QNN acceleration
   python test_ipfs_accelerate.py --qnn --models "bert-base-uncased"
   
   # Run with specific Qualcomm precision settings
   python test_ipfs_accelerate.py --qnn --precision int8 --models "bert-base-uncased"
   
   # Generate Qualcomm performance comparison report
   python test_ipfs_accelerate.py --qnn-analysis --models "bert-base-uncased,whisper-tiny" --format html
   ```

3. **WebGPU Support and Analysis**: Test and analyze browser-based GPU acceleration:
   ```bash
   # Test with WebGPU acceleration
   python test_ipfs_accelerate.py --webgpu --models "bert-base-uncased"
   
   # Generate WebGPU analysis report with shader metrics
   python test_ipfs_accelerate.py --webgpu-analysis --browser firefox --shader-metrics --format html
   
   # Generate comprehensive WebGPU performance analysis across browsers
   python test_ipfs_accelerate.py --webgpu-analysis --format html
   
   # Analyze compute shader optimizations (especially for audio models)
   python test_ipfs_accelerate.py --webgpu-analysis --compute-shader-optimization --browser firefox --format html
   ```

4. **Real-Time Database Integration**: Test results stored in database as they're generated:
   ```bash
   # Test multiple platforms with real-time database integration
   python test_ipfs_accelerate.py --models "bert-base-uncased" --qnn --webnn --webgpu --db-only
   ```

5. **Enhanced Visualization and Reporting**:
   - Interactive Plotly charts for performance comparisons
   - WebGPU shader compilation metrics visualization
   - Browser-specific WebGPU performance analysis
   - Model-specific optimization recommendations
   - Hardware compatibility heatmaps
   - Qualcomm power efficiency metrics for mobile/edge devices

6. **Comprehensive Reporting Options**:
   - General report: `--report`
   - IPFS acceleration report: `--ipfs-acceleration-report`
   - Acceleration comparison report: `--comparison-report` 
   - WebGPU analysis report: `--webgpu-analysis` 
   - Qualcomm performance report: `--qnn-analysis` (NEW!)

For detailed documentation on these features, see [IPFS_ACCELERATION_TESTING.md](IPFS_ACCELERATION_TESTING.md).

To generate an updated compatibility matrix with actual benchmark data, run:
```bash
# IMPORTANT: All benchmark results are now stored in DuckDB database, not JSON files
# Set database path with environment variable or parameter
export BENCHMARK_DB_PATH=./benchmark_db.duckdb

# Run benchmarks (results stored directly in database)
python test/benchmark_all_key_models.py --db-only

# Legacy approach (DEPRECATED - not recommended)
# python test/benchmark_all_key_models.py --output-dir ./benchmark_results
```

This will benchmark all 13 high-priority model classes across all available hardware platforms and generate a comprehensive compatibility matrix based on real performance data. All results will be stored directly in the DuckDB database for efficient querying and analysis.

### Key Model Test Coverage Status

| Model Class | Model Used | CPU | CUDA | ROCm | MPS | OpenVINO | Qualcomm | Samsung | WebNN | WebGPU | Notes |
|-------------|------------|-----|------|------|-----|----------|----------|---------|-------|--------|-------|
| BERT | bert-base-uncased, bert-tiny | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ | Complete coverage (March 6) |
| T5 | t5-small, t5-efficient-tiny | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ | Complete coverage (March 6) |
| LLAMA | opt-125m | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ | ‚ö†Ô∏è | ‚ö†Ô∏è | WebNN/WebGPU limited by memory |
| CLIP | Local test model | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ | Complete coverage |
| ViT | vit-base | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ | Complete coverage |
| CLAP | Local test model | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ | ‚ö†Ô∏è | ‚ö†Ô∏è | Web has limited audio support |
| Whisper | whisper-tiny | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ | ‚ö†Ô∏è | ‚ö†Ô∏è | Web audio challenges |
| Wav2Vec2 | Local test model | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ | ‚ö†Ô∏è | ‚ö†Ô∏è | Web audio challenges |
| LLaVA | llava-onevision-base | ‚úÖ | ‚úÖ | ‚ö†Ô∏è | ‚úÖ | ‚ö†Ô∏è | ‚ö†Ô∏è | ‚ö†Ô∏è | ‚ö†Ô∏è | ‚ö†Ô∏è | Memory intensive |
| LLaVA-Next | Local test model | ‚úÖ | ‚úÖ | ‚ö†Ô∏è | ‚úÖ | ‚ö†Ô∏è | ‚ö†Ô∏è | ‚ö†Ô∏è | ‚ö†Ô∏è | ‚ö†Ô∏è | Memory intensive |
| XCLIP | Local test model | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ | ‚ö†Ô∏è | ‚ö†Ô∏è | Limited video support in web |
| Qwen2/3 | qwen2, qwen3, qwen2_vl, qwen3_vl | ‚úÖ | ‚úÖ | ‚ö†Ô∏è | ‚ö†Ô∏è | ‚ö†Ô∏è | ‚ö†Ô∏è | ‚ö†Ô∏è | ‚ö†Ô∏è | ‚ö†Ô∏è | Memory constraints |
| DETR | Local test model | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ | ‚ö†Ô∏è | ‚ö†Ô∏è | Limited detection support |

## Essential Test Commands

### Template-Based Generation System
The framework uses a template-based approach stored in DuckDB to efficiently generate test files, skills, and benchmarks for 300+ HuggingFace model classes. This approach prevents the repository from containing thousands of individual files.

Key features:
- Templates for tests, skills, and benchmarks are stored in the DuckDB database
- Templates include helper functions and dependencies needed across models
- Generators retrieve templates from the database and instantiate them for specific models
- Cross-platform hardware compatibility is built into templates
- Each generator creates tests/skills/benchmarks on demand rather than storing static files

### MARCH 2025 UPDATE: Simplified Template System

A new simplified template system has been implemented that makes it easier to generate hardware-aware tests:

```bash
# Create a simple template database
python test/create_simple_template_db.py

# Validate templates in the database
python test/simple_template_validator.py --validate-db

# Generate a test with database templates
python test/simple_test_generator.py -g bert -t

# Generate a test with specific hardware platforms
python test/simple_test_generator.py -g vit -p cuda,qualcomm,webgpu -t

# Generate a test with Qualcomm AI Engine support
python test/simple_test_generator.py -g bert -p qualcomm -o test_bert_qualcomm.py

# Check all template system components
python test/run_template_system_check.py

# List all templates in the database
python test/simple_test_generator.py --list-templates

# Detect available hardware platforms
python test/simple_test_generator.py --detect-hardware
```

```bash
# Generate tests with database templates and cross-platform hardware compatibility
python test/merged_test_generator.py --model bert --cross-platform --hardware all --use-db-templates

# Generate tests for a specific model and hardware platforms using database templates
python test/integrated_skillset_generator.py --model bert --hardware cuda,openvino,webnn --use-db-templates

# Generate all 300+ HuggingFace model tests from database templates
python test/merged_test_generator.py --all-models --use-db-templates

# Update template database with hardware-specific templates
python test/template_database.py --update-templates --model-family bert

# Generate and store a new template in the database
python test/template_database.py --create-template --model-type llama --store-in-db

# List all available templates in the database
python test/template_database.py --list-templates

# Validate templates in the database
python test/template_database.py --validate-templates

# Generate all test files for a model family from templates
python test/merged_test_generator.py --family text-embedding --use-db-templates

# Run test generator with all improvements applied
python test/run_fixed_test_generator.py --model bert --use-db-templates --cross-platform

# Run test generator with all features enabled
python test/run_fixed_test_generator.py --model bert --enable-all

# Fix generator integration issues
python test/fix_template_integration.py --integrate-generator fixed_merged_test_generator.py

# Check template database integrity
python test/fix_template_integration.py --check-db
```

### Hardware-Aware Test Generation
```bash
# Generate tests with cross-platform hardware compatibility
python test/integrated_skillset_generator.py --model bert --cross-platform --hardware all

# Generate tests for specific hardware platforms only
python test/integrated_skillset_generator.py --model bert --hardware cuda,openvino,qnn,webnn

# Generate tests with the improved generator that supports all hardware platforms
python test/qualified_test_generator.py -g bert-base-uncased -p cpu,cuda,rocm,mps,openvino,qnn,webnn,webgpu -o test_bert_all_platforms.py

# Run hardware-specific template generation
python test/enhance_key_models_hardware_coverage.py --create-templates

# Update the test generator with hardware-aware templates
python test/update_test_generator_with_hardware_templates.py

# Run validation on hardware compatibility
python test/enhance_key_models_hardware_coverage.py --validate
```

### Phase 16 Hardware Integration
```bash
# Run hardware integration fixes on key model tests
./test/run_key_model_fixes.sh

# Fix hardware integration for specific models
python test/fix_hardware_integration.py --specific-models bert,t5,clip

# Fix all key model tests
python test/fix_hardware_integration.py --all-key-models

# Analyze hardware integration issues without fixing
python test/fix_hardware_integration.py --all-key-models --analyze-only --output-json hardware_analysis.json

# Test model generators with hardware-aware templates
python test/update_test_generator_with_hardware_templates.py

# Generate tests with cross-platform hardware compatibility
python test/integrated_skillset_generator.py --model bert --cross-platform --hardware all
```

### Hardware Testing
```bash
# Automated hardware selection for any model
python test/automated_hardware_selection.py --model [model_name] --batch-size [batch_size] --mode [inference|training]

# Select hardware for distributed training
python test/automated_hardware_selection.py --model [model_name] --distributed-config --gpu-count 8 --max-memory-gb 40

# Generate comprehensive hardware selection map
python test/automated_hardware_selection.py --create-map --output hardware_selection_map.json

# Analyze model performance across all available hardware
python test/automated_hardware_selection.py --model [model_name] --analyze --output analysis.json

# Detect available hardware platforms
python test/automated_hardware_selection.py --detect-hardware

# Comprehensive hardware detection and compatibility test
python test/test_comprehensive_hardware.py --test all

# Test hardware backends with specific model
python test/test_hardware_backend.py --backend [cpu|cuda|rocm|mps|openvino|qualcomm|webnn|webgpu|all] --model [model_name]

# Test resource pool with hardware awareness
python test/test_resource_pool.py --test hardware

# Test model family integration with web platform support
python test/test_resource_pool.py --test family --debug
```

### Web Platform Testing

```bash
# Run web platform integration tests
python test/test_model_integration.py

# Verify web platform integration is correct
python test/verify_web_platform_integration.py

# Generate a test with WebNN support
python test/merged_test_generator.py --generate bert --platform webnn

# Generate a test with WebGPU support
python test/merged_test_generator.py --generate vit --platform webgpu

# Run tests with database integration (DuckDB)
python test/run_web_platform_tests_with_db.py --models bert t5 vit --small-models --db-path ./benchmark_db.duckdb

# Use environment variable for database path
export BENCHMARK_DB_PATH=./benchmark_db.duckdb
python test/run_web_platform_tests_with_db.py --all-models --run-webgpu

# Run with browser automation
./run_web_platform_tests.sh --use-browser-automation --browser chrome python test/web_platform_test_runner.py --model bert

# Run WebNN tests with Edge browser
./run_web_platform_tests.sh --webnn-only --use-browser-automation --browser edge python test/web_platform_test_runner.py --model bert

# Run WebGPU tests with Firefox browser
./run_web_platform_tests.sh --webgpu-only --use-browser-automation --browser firefox python test/web_platform_test_runner.py --model vit

# Run browser tests with direct database storage
python test/web_platform_test_runner.py --model bert --platform webnn --browser edge

# Disable JSON output (database storage only)
export DEPRECATE_JSON_OUTPUT=1 python test/web_platform_test_runner.py --model vit --platform webgpu

# Run with enhanced WebGPU compute shaders with DB storage
python test/web_platform_test_runner.py --model whisper --platform webgpu --compute-shaders

# Use database for parallel model loading results
python test/run_web_platform_tests_with_db.py --models llava clip --parallel-loading

# Store shader compilation metrics in database
WEBGPU_SHADER_PRECOMPILE=1 python test/web_platform_test_runner.py --model vit

# Test all March 2025 optimizations at once (compute shaders, parallel loading, and shader precompilation)
python test/test_web_platform_optimizations.py --all-optimizations

# Combine multiple features with browser automation
./run_web_platform_tests.sh --use-browser-automation --browser chrome --enable-compute-shaders --enable-shader-precompile python test/web_platform_test_runner.py --model whisper

# Run comprehensive web platform integration tests with all optimizations
./run_web_platform_integration_tests.sh --all-optimizations --model clap

# Test specific models with selected optimizations
./run_web_platform_integration_tests.sh --models whisper,wav2vec2 --enable-compute-shaders --enable-shader-precompile

# Test multimodal models with parallel loading
./run_web_platform_integration_tests.sh --models clip,llava --enable-parallel-loading --enable-shader-precompile

# Run comprehensive tests for all models with all optimizations
./run_web_platform_integration_tests.sh --all-models --all-optimizations

# Run tests with database integration and browser automation
./run_web_platform_integration_tests.sh --model bert --use-browser-automation --browser edge --db-path ./benchmark_db.duckdb

# Generate web platform reports from database
python test/scripts/benchmark_db_query.py --report web_platform --format html --output web_report.html

# View advanced WebGPU features usage from database
python test/scripts/benchmark_db_query.py --report webgpu --format html --output webgpu_report.html

# Compare web vs native performance from database
python test/scripts/benchmark_db_query.py --sql "SELECT * FROM cross_platform_performance WHERE model_name='bert-base-uncased'" --format html

# Compare simulation vs real browser results
python test/scripts/benchmark_db_query.py --report simulation_vs_real --format html --output comparison.html
```

### March 2025 Web Platform Optimizations

The March 2025 release includes three major optimizations for web platform models:

```bash
# 1. WebGPU Compute Shader Optimization for Audio Models
# Firefox shows ~20% better performance than Chrome for audio models
# Test with various audio models
python test/test_web_platform_optimizations.py --compute-shaders --model whisper
python test/test_web_platform_optimizations.py --compute-shaders --model wav2vec2
python test/test_web_platform_optimizations.py --compute-shaders --model clap

# Enable via environment variable
export WEBGPU_COMPUTE_SHADERS_ENABLED=1
python test/web_platform_benchmark.py --model whisper

# Firefox-specific optimizations (uses 256x1x1 workgroup vs Chrome's 128x2x1)
./run_web_platform_tests.sh --firefox --enable-compute-shaders --model whisper

# Compare Firefox vs Chrome with various audio durations
python test/test_firefox_webgpu_compute_shaders.py --model whisper --audio-durations 5,15,30,60

# Direct API access to Firefox optimized compute shaders
from fixed_web_platform.webgpu_audio_compute_shaders import optimize_for_firefox

# 2. Parallel Model Loading for Multimodal Models
# Test with various multimodal models
python test/test_web_platform_optimizations.py --parallel-loading --model clip
python test/test_web_platform_optimizations.py --parallel-loading --model llava
python test/test_webgpu_parallel_model_loading.py --model-type multimodal

# Enable via environment variable
export WEB_PARALLEL_LOADING_ENABLED=1
python test/web_platform_benchmark.py --model clip

# 3. Shader Precompilation for Faster Startup
# Test with any WebGPU model
python test/test_web_platform_optimizations.py --shader-precompile --model bert
python test/test_web_platform_optimizations.py --shader-precompile --model vit

# Enable via environment variable
export WEBGPU_SHADER_PRECOMPILE_ENABLED=1
python test/web_platform_benchmark.py --model bert

# Testing all optimizations together
python test/test_web_platform_optimizations.py --all-optimizations
./run_web_platform_integration_tests.sh --all-optimizations --model clap

# Model-specific optimization recommendations
# For Text Models (BERT, T5, etc.)
./run_web_platform_integration_tests.sh --model bert --enable-shader-precompile

# For Vision Models (ViT, ResNet, etc.)
./run_web_platform_integration_tests.sh --model vit --enable-shader-precompile

# For Audio Models (Whisper, Wav2Vec2, CLAP)
# Firefox performs ~20% better than Chrome for audio models
./run_web_platform_integration_tests.sh --firefox --model whisper --enable-compute-shaders --enable-shader-precompile

# For Multimodal Models (CLIP, LLaVA, XCLIP)
./run_web_platform_integration_tests.sh --model clip --enable-parallel-loading --enable-shader-precompile

# For Audio-Multimodal Models (CLAP)
# Firefox shows ~21% better performance than Chrome for CLAP
./run_web_platform_integration_tests.sh --firefox --model clap --all-optimizations

# Compare Firefox vs Chrome browser performance
./run_web_platform_tests.sh --compare-browsers --model whisper
```

### QNN (Qualcomm Neural Networks) Support and Advanced Quantization (March 2025)
```bash
# Generate tests for QNN hardware
python test/qualified_test_generator.py -g bert-base-uncased -p qnn -o test_bert_qnn.py

# Run tests on QNN hardware
python test_bert_qnn.py

# Run comprehensive QNN integration test suite (stores results in DuckDB)
python test/test_qnn_integration.py --db-path ./benchmark_db.duckdb

# Run test suite with specific models
python test/test_qnn_integration.py --models BAAI/bge-small-en-v1.5,prajjwal1/bert-tiny

# Run test suite with comprehensive model set
python test/test_qnn_integration.py --models all

# Generate QNN performance visualizations from test data
python test/visualize_qnn_performance.py --db-path ./benchmark_db.duckdb --output ./reports

# Automated hardware selection including QNN
python test/automated_hardware_selection.py --model bert-base-uncased --include-qnn

# Benchmark with QNN hardware
python test/benchmark_all_key_models.py --hardware qnn

# Test power efficiency metrics for mobile/edge devices (QNN)
python test/test_hardware_backend.py --backend qnn --model bert-tiny --power-metrics

# Compare QNN vs other hardware platforms using DuckDB data
python test/scripts/benchmark_db_query.py --report qnn_comparison --format html --output qnn_report.html

# Extract device and SDK information for QNN
python test/test_qnn_integration.py --device-info-only

# Basic Quantization Usage
# ========================

# Quantize a model for QNN hardware
python test/qnn_quantization_support.py quantize \
  --model-path models/bert-base-uncased.onnx \
  --output-path models/bert-base-uncased.qnn \
  --method int8 \
  --model-type text

# Compare different quantization methods
python test/qnn_quantization_support.py compare \
  --model-path models/bert-base-uncased.onnx \
  --output-dir ./quantized_models \
  --model-type text \
  --report-path ./reports/quantization_comparison.md

# List available quantization methods for QNN
python test/qnn_quantization_support.py list

# Run a complete quantization example
python test/test_examples/qnn_quantization_example.py \
  --model-path models/bert-base-uncased.onnx \
  --model-type text \
  --mock

# Advanced Quantization Methods (March 2025)
# =========================================

# Weight Clustering Quantization
python test/qnn_advanced_quantization.py cluster \
  --model-path models/bert-base-uncased.onnx \
  --output-path models/bert-base-uncased-clustered.qnn \
  --clusters 16 \
  --model-type text \
  --optimize-for hexagon

# Hybrid/Mixed Precision Quantization
python test/qnn_advanced_quantization.py hybrid \
  --model-path models/llama-7b.onnx \
  --output-path models/llama-7b-hybrid.qnn \
  --attention-precision int8 \
  --feedforward-precision int4 \
  --model-type text_generation \
  --optimize-for mobile

# Per-Channel Quantization
python test/qnn_advanced_quantization.py per-channel \
  --model-path models/clip-vit.onnx \
  --output-path models/clip-vit-perchannel.qnn \
  --model-type vision

# Learned Quantization Parameters (QAT)
python test/qnn_advanced_quantization.py qat \
  --model-path models/bert-base-uncased.onnx \
  --output-path models/bert-base-uncased-qat.qnn \
  --train-dataset glue/mrpc \
  --epochs 3 \
  --learning-rate 5e-5 \
  --model-type text

# Sparse Quantization with Pruning
python test/qnn_advanced_quantization.py sparse \
  --model-path models/whisper-small.onnx \
  --output-path models/whisper-small-sparse.qnn \
  --sparsity 0.5 \
  --pruning-method magnitude \
  --model-type audio

# Method Comparison Framework
python test/quantization_comparison_tools.py compare-all \
  --model-path models/bert-base-uncased.onnx \
  --output-dir ./comparison_results \
  --methods int8,int4,cluster,hybrid,sparse \
  --metrics accuracy,latency,power,size \
  --model-type text

# Generate Quantization Impact Visualization
python test/quantization_comparison_tools.py visualize \
  --results-path ./comparison_results/bert-base-uncased-comparison.json \
  --output-path ./visualization/bert-quantization-impact.html \
  --plot-type radar

# Hardware-Specific Optimizations for Quantized Models
python test/qnn_hardware_optimizations.py optimize \
  --model-path models/bert-base-uncased-int8.qnn \
  --output-path models/bert-base-uncased-int8-optimized.qnn \
  --device sm8550 \
  --optimize memory,power,latency

# Memory Bandwidth Optimization
python test/qnn_hardware_optimizations.py memory-optimize \
  --model-path models/llama-7b-int4.qnn \
  --output-path models/llama-7b-int4-memopt.qnn \
  --cache-config aggressive \
  --tiling-strategy optimal

# Power State Management Integration
python test/qnn_hardware_optimizations.py power-optimize \
  --model-path models/whisper-small-int8.qnn \
  --output-path models/whisper-small-int8-poweropt.qnn \
  --battery-mode efficient \
  --dynamic-scaling enabled
```

### Distributed Training Configuration
```bash
# Generate distributed training configuration
python test/hardware_selector.py --model-family text_generation --model-name t5-small --mode training --distributed --gpu-count 4

# Generate training benchmark configuration for a model
python test/run_training_benchmark.py --model bert-base-uncased --distributed --max-gpus 4 --output bert_benchmark.json

# List available sample models for benchmarking
python test/run_training_benchmark.py --list-models

# Generate a memory-optimized training configuration
python test/hardware_selector.py --model-family text_generation --model-name llama-7b --mode training --distributed --gpu-count 8 --max-memory-gb 24
```

### Model Benchmarking with Template-Based Generation
```bash
# Run comprehensive benchmarks for all 300+ models using database templates
python test/benchmark_all_key_models.py --all-models --use-db-templates

# Run benchmarks for a specific model using database templates
python test/benchmark_all_key_models.py --model bert --use-db-templates

# Run benchmarks for all models in a family using database templates
python test/benchmark_all_key_models.py --family text-embedding --use-db-templates

# Create a new benchmark template and store in database
python test/template_database.py --create-benchmark-template --model-type llama --store-in-db

# Run standard model benchmarks with database integration and templates
python test/run_model_benchmarks.py --models bert,t5,vit --use-db-templates --db-path ./benchmark_db.duckdb

# Generate benchmarks for all 300+ models (results stored directly in database)
python test/run_model_benchmarks.py --generate-all --use-db-templates --db-path ./benchmark_db.duckdb
```

### Traditional Model Benchmarking and Validation
```bash
# Run comprehensive benchmarks for all 13 high-priority models across all hardware platforms
python test/benchmark_all_key_models.py --output-dir ./benchmark_results

# Use smaller model variants for faster testing
python test/benchmark_all_key_models.py --small-models --output-dir ./benchmark_results

# Test specific hardware platforms
python test/benchmark_all_key_models.py --hardware cpu cuda openvino --output-dir ./benchmark_results

# Automatically fix implementation issues
python test/benchmark_all_key_models.py --debug --output-dir ./benchmark_results

# Run standard model benchmarks with database integration
python test/run_model_benchmarks.py --output-dir ./benchmark_results --db-path ./benchmark_db.duckdb

# Test on specific hardware platforms with small model set
python test/run_model_benchmarks.py --hardware cpu cuda --models-set small --db-path ./benchmark_db.duckdb

# Run benchmarks without storing in database
python test/run_model_benchmarks.py --hardware cpu --models-set small --no-db-store

# Generate database visualizations from benchmark results
python test/run_model_benchmarks.py --hardware cuda --visualize-from-db

# Manual model functionality verification
python test/verify_model_functionality.py --models bert t5 vit --hardware cpu cuda

# Run detailed hardware benchmarks
python test/hardware_benchmark_runner.py --model-families embedding text_generation --hardware cpu cuda
```

### Benchmark Database and Result Management
```bash
# Set the database path environment variable (recommended)
export BENCHMARK_DB_PATH=./benchmark_db.duckdb

# JSON output is deprecated and now disabled by default
# All results are stored directly in the database

# Update database schema to add simulation flags
python update_db_schema_for_simulation.py

# Check QNN simulation status
python qnn_simulation_helper.py --check

# Enable QNN simulation (for testing only)
python qnn_simulation_helper.py --enable

# Disable QNN simulation
python qnn_simulation_helper.py --disable

# Migrate existing JSON files to the database 
python test/migrate_all_json_files.py --db-path ./benchmark_db.duckdb --archive

# Migrate and archive all JSON files (keeps archives)
python test/migrate_all_json_files.py --db-path ./benchmark_db.duckdb --archive --archive-dir ./archived_json_files

# Migrate all JSON files and delete them after successful migration and archiving
python test/migrate_all_json_files.py --db-path ./benchmark_db.duckdb --delete

# Convert existing benchmark JSON files to DuckDB format
python test/benchmark_db_converter.py --input-dir ./archived_test_results

# Consolidate test results across directories
python test/benchmark_db_converter.py --consolidate --categories performance hardware compatibility

# Comprehensive data migration with validation and deduplication
python test/benchmark_db_converter.py --consolidate --deduplicate --directories archived_test_results benchmark_results critical_model_results hardware_fix_results api_check_results

# Archive JSON files after migration to DuckDB
tar -czf archived_json_files/archived_test_results_$(date +%Y%m%d).tar.gz archived_test_results/*.json

# Create initial database schema with sample data
python test/scripts/create_benchmark_schema.py --sample-data

# Database maintenance and optimization
python test/scripts/benchmark_db_maintenance.py --optimize-db --vacuum

# Create database backup with compression
python test/scripts/benchmark_db_maintenance.py --backup --backup-dir ./db_backups --backup-compress

# Check database integrity
python test/scripts/benchmark_db_maintenance.py --check-integrity

# Generate migration statistics report
python test/scripts/benchmark_db_maintenance.py --migration-stats --output migration_report.json

# Purge old database backups based on retention policy
python test/scripts/benchmark_db_maintenance.py --purge-backups --backup-retention 30 --backup-dir ./db_backups

# Query benchmark database with SQL
python test/scripts/benchmark_db_query.py --sql "SELECT model_name, hardware_type, AVG(throughput_items_per_second) FROM performance_results JOIN models USING(model_id) JOIN hardware_platforms USING(hardware_id) GROUP BY model_name, hardware_type"

# Generate reports from DuckDB benchmark database
python test/scripts/benchmark_db_query.py --report performance --format html --output benchmark_report.html
python test/scripts/benchmark_db_query.py --report hardware --format html --output hardware_report.html
python test/scripts/benchmark_db_query.py --report compatibility --format html --output compatibility_matrix.html

# Compare hardware platforms for a specific model
python test/scripts/benchmark_db_query.py --model bert-base-uncased --metric throughput --compare-hardware --output bert_hardware_comparison.png

# Compare models on a specific hardware platform
python test/scripts/benchmark_db_query.py --hardware cuda --metric throughput --compare-models --output cuda_model_comparison.png

# Plot performance trends over time
python test/scripts/benchmark_db_query.py --trend performance --model bert-base-uncased --hardware cuda --metric throughput --format chart

# Export data from the database
python test/scripts/benchmark_db_query.py --sql "SELECT * FROM performance_results" --format csv --output performance_data.csv

# Run benchmarks (results stored directly in database)
python test/run_benchmark_with_db.py --model bert-base-uncased --hardware cuda --batch-sizes 1,2,4,8,16

# Run standard model benchmarks (results stored directly in database)
python test/run_model_benchmarks.py --models bert-base-uncased,t5-small --hardware cuda

# Run CI/CD benchmark workflow manually via GitHub CLI
gh workflow run benchmark_db_ci.yml --ref main -f test_model=bert-base-uncased -f hardware=cpu -f batch_size=1,2,4,8

# Run test_ipfs_accelerate.py with database integration
python test/test_ipfs_accelerate.py --db-path ./benchmark_db.duckdb

# Generate a test report from the DuckDB database
python test/test_ipfs_accelerate.py --report --format markdown --output test_report.md
```

#### DuckDB Test Results Schema

Our DuckDB database schema has been enhanced to store detailed test results and hardware metrics:

```sql
-- Main test results table
CREATE TABLE IF NOT EXISTS test_results (
    id INTEGER PRIMARY KEY,
    timestamp TIMESTAMP,
    test_date VARCHAR,
    status VARCHAR,
    test_type VARCHAR,
    model_name VARCHAR,
    endpoint_type VARCHAR,
    hardware_type VARCHAR,
    success BOOLEAN,
    error_message VARCHAR,
    execution_time FLOAT,
    memory_usage FLOAT,
    power_consumption FLOAT,       -- Added for mobile/edge devices
    temperature FLOAT,             -- Added for thermal monitoring
    qnn_version VARCHAR,           -- Qualcomm Neural Network SDK version
    sdk_type VARCHAR,              -- QNN or QTI SDK type
    details JSON
);

-- Hardware capability tracking
CREATE TABLE IF NOT EXISTS hardware_capabilities (
    id INTEGER PRIMARY KEY,
    hardware_type VARCHAR,
    device_name VARCHAR,
    compute_units INTEGER,
    memory_capacity FLOAT,
    driver_version VARCHAR,
    supported_precisions JSON,     -- FP32, FP16, INT8, INT4 support
    max_batch_size INTEGER,
    throughput_benchmark FLOAT,
    latency_benchmark FLOAT,
    power_efficiency FLOAT,        -- Important for mobile/edge
    detected_at TIMESTAMP
);

-- Model conversion metrics
CREATE TABLE IF NOT EXISTS model_conversion_metrics (
    id INTEGER PRIMARY KEY,
    model_name VARCHAR,
    source_format VARCHAR,
    target_format VARCHAR,
    hardware_target VARCHAR,
    conversion_success BOOLEAN,
    conversion_time FLOAT,
    file_size_before FLOAT,
    file_size_after FLOAT,
    precision VARCHAR,
    optimization_level INTEGER,
    error_message VARCHAR,
    timestamp TIMESTAMP
);

-- Performance comparison 
CREATE TABLE IF NOT EXISTS performance_comparison (
    id INTEGER PRIMARY KEY,
    model_name VARCHAR,
    test_id INTEGER,
    test_date TIMESTAMP,
    hardware_type VARCHAR,
    batch_size INTEGER,
    sequence_length INTEGER,
    latency_ms FLOAT,
    throughput_items_per_sec FLOAT,
    memory_mb FLOAT,
    power_watts FLOAT,            -- Added for mobile/edge
    energy_efficiency_items_per_joule FLOAT,
    performance_score FLOAT        -- Composite metric
);

-- Cross-platform compatibility matrix
CREATE TABLE IF NOT EXISTS cross_platform_compatibility (
    id INTEGER PRIMARY KEY,
    model_name VARCHAR,
    model_type VARCHAR,
    model_size VARCHAR,
    cpu_support BOOLEAN,
    cuda_support BOOLEAN,
    rocm_support BOOLEAN,
    mps_support BOOLEAN,
    openvino_support BOOLEAN,
    qnn_support BOOLEAN,          -- Qualcomm Neural Networks support
    webnn_support BOOLEAN,
    webgpu_support BOOLEAN,
    recommended_platform VARCHAR, 
    last_updated TIMESTAMP
);
```

For working with the schema:

```bash
# Query hardware capabilities
python test/scripts/benchmark_db_query.py --sql "SELECT * FROM hardware_capabilities" --format html --output capabilities.html

# Check cross-platform compatibility by model type
python test/scripts/benchmark_db_query.py --sql "SELECT model_type, COUNT(*) as total, SUM(CASE WHEN qnn_support THEN 1 ELSE 0 END) as qnn_compatible, ROUND(SUM(CASE WHEN qnn_support THEN 1 ELSE 0 END) * 100.0 / COUNT(*), 1) as compatibility_rate FROM cross_platform_compatibility GROUP BY model_type ORDER BY compatibility_rate DESC" --format markdown

# Compare power efficiency across hardware platforms
python test/scripts/benchmark_db_query.py --sql "SELECT hardware_type, AVG(energy_efficiency_items_per_joule) as avg_efficiency FROM performance_comparison GROUP BY hardware_type ORDER BY avg_efficiency DESC" --format chart --output power_efficiency.png
```

## Performance Benchmarks

### Latest Performance Metrics

For detailed performance benchmarks, please refer to the following resources:
- Database dashboard: `http://localhost:8000/dashboard` (when running benchmark_db_api.py)
- API documentation: `http://localhost:8000/docs` (complete REST API for all benchmark data)
- Generated reports: 
  - `python test/scripts/benchmark_db_query.py --report summary --format html --output summary_report.html`
  - `python test/scripts/benchmark_db_query.py --compatibility-matrix --format html --output matrix.html`

Legacy documentation (being migrated to database):
- Hardware-specific benchmarks: `test/HARDWARE_BENCHMARKING_GUIDE.md`
- Model compression results: `test/MODEL_COMPRESSION_GUIDE.md`
- Training benchmarks: `test/TRAINING_BENCHMARKING_GUIDE.md`
- Web platform audio tests: `test/WEB_PLATFORM_AUDIO_TESTING_GUIDE.md`
- Hardware selection system: `test/HARDWARE_SELECTION_GUIDE.md`
- Web platform support: `test/README_WEB_PLATFORM_SUPPORT.md`
- QNN implementation: `test/QNN_IMPLEMENTATION_SUMMARY.md`

### QNN (Qualcomm Neural Networks) Performance

The QNN integration (March 2025) provides specialized support for Snapdragon SoCs and mobile/edge devices:

| Model Type | Model Size | QNN vs CPU | Power Efficiency | Key Metric |
|------------|------------|------------|------------------|------------|
| Embedding | Small | 2.5-3.8x faster | 4.0-5.5x better | 78% lower power consumption |
| Text Generation | Tiny (<1B) | 1.8-2.2x faster | 3.0-4.0x better | Optimal for battery life |
| Vision | Small-Medium | 3.0-5.0x faster | 3.5-4.5x better | Great for mobile vision |
| Audio | Tiny | 2.0-3.0x faster | 3.0-4.0x better | Suitable for voice assistants |
| Multimodal | Tiny-Small | 1.5-2.0x faster | 2.5-3.5x better | Limited by memory |

Performance varies by hardware generation and specific Snapdragon model. Benchmarks were conducted on Snapdragon 8 Gen 3 hardware with the latest QNN SDK (version 2.10).

**QNN Implementation Features:**
- Model conversion pipeline (PyTorch ‚Üí ONNX ‚Üí QNN format)
- Support for both QNN and QTI SDKs
- Power and thermal measurement capabilities
- Mobile-optimized inference settings
- Edge-aware batching and memory management
- Fallback mechanisms for unsupported operations
- Mock implementations for testing without physical hardware

For detailed QNN performance testing and reports, run:
```bash
# Run comprehensive QNN test suite and generate reports
python test/test_qnn_integration.py --models all
python test/visualize_qnn_performance.py --output ./reports
```

### Web Platform Performance Results

The March 2025 enhancements have significantly improved web platform performance:

| Model Type | WebNN vs. CPU | WebGPU vs. CPU | WebGPU Standard | WebGPU March 2025 | Recommended Size |
|------------|--------------|----------------|-----------------|-------------------|------------------|
| BERT Embeddings | 2.0-3.0x faster | 2.2-3.4x faster | 2.2-3.4x faster | 2.4-3.6x faster | Small-Medium |
| Vision Models | 3.0-4.0x faster | 4.0-6.0x faster | 4.0-6.0x faster | 4.5-6.5x faster | Any size |
| Small T5 | 1.5-2.0x faster | 1.3-1.8x faster | 1.3-1.8x faster | 1.6-2.2x faster | Small |
| Tiny LLAMA | 1.0-1.2x faster | 1.2-1.5x faster | 1.2-1.5x faster | 1.4-1.9x faster | Tiny (<1B) |
| Audio Models | 0.8-1.2x CPU | 1.0-1.2x CPU | 1.0-1.2x CPU | 1.2-1.5x faster | Tiny-Small |

**March 2025 Optimization Details:**

1. **WebGPU Compute Shader Optimization for Audio Models**:
   - 20-35% performance improvement (43% in tests for Whisper)
   - Firefox-specific optimizations using 256x1x1 workgroup size vs Chrome's 128x2x1
   - Targeted at audio models (Whisper, Wav2Vec2, CLAP)
   - Implementation in `fixed_web_platform/webgpu_audio_compute_shaders.py`

2. **Parallel Loading for Multimodal Models**: 
   - 30-45% loading time reduction
   - Multiple model components loaded simultaneously
   - Especially effective for models with separate encoders (vision, text)
   - Implementation in `fixed_web_platform/progressive_model_loader.py`

3. **Shader Precompilation**:
   - 30-45% faster first inference
   - Precompiles shaders during model initialization
   - Most effective for vision models with complex shader pipelines
   - Implementation in `fixed_web_platform/webgpu_shader_precompilation.py`

**Current Implementation Status:**

| Feature | Status | Implementation | Browser Support |
|---------|--------|----------------|----------------|
| WebNN Core | ‚úÖ Complete | Simulation + transformers.js | Chrome, Edge, Safari |
| WebGPU Core | ‚úÖ Complete | Simulation + transformers.js | Chrome, Edge, Firefox, Safari (partial) |
| Compute Shader Optimization | ‚úÖ Complete | Custom implementation | Chrome, Edge, Firefox (best) |
| Shader Precompilation | ‚úÖ Complete | Custom implementation | Chrome, Edge, Safari (limited) |
| Parallel Model Loading | ‚úÖ Complete | Custom implementation | All browsers |
| 4-bit Quantization | üîÑ In Progress | Custom implementation | Chrome, Edge, Firefox |
| KV-Cache Optimization | üîÑ In Progress | Planned | Chrome, Edge |
| Browser API Detection | ‚úÖ Complete | Robust checks | All browsers |
| Graceful Fallbacks | ‚úÖ Complete | Feature detection | All browsers |

**Optimization Impact by Model Type:**

| Model Type | Example Models | Primary Optimizations | Performance Impact |
|------------|----------------|----------------------|-------------------|
| Text Models | BERT, T5, RoBERTa | Shader Precompilation | 30-45% faster first inference |
| Vision Models | ViT, ResNet, CLIP | Shader Precompilation | 30-45% faster first inference |
| Audio Models | Whisper, Wav2Vec2 | Compute Shaders (Firefox ~20% faster than Chrome), Shader Precompilation | 20-35% faster audio processing, 30-45% faster first inference |
| Multimodal Models | CLIP, LLaVA, XCLIP | Parallel Loading, Shader Precompilation | 30-45% faster initialization, 30-45% faster first inference |
| Audio-Multimodal | CLAP | All Optimizations | 45-60% overall improvement |

**Browser Compatibility:**

| Browser | WebGPU Support | Compute Shaders | Parallel Loading | Shader Precompilation | 4-bit Quantization | Flash Attention |
|---------|---------------|-----------------|------------------|----------------------|-------------------|-----------------|
| Chrome | ‚úÖ Full | ‚úÖ Full | ‚úÖ Full | ‚úÖ Full | ‚úÖ Full | ‚úÖ Full |
| Edge | ‚úÖ Full | ‚úÖ Full | ‚úÖ Full | ‚úÖ Full | ‚úÖ Full | ‚úÖ Full |
| Firefox | ‚úÖ Full | ‚úÖ Full | ‚úÖ Full | ‚ö†Ô∏è Limited | ‚úÖ Full | ‚úÖ Full |
| Safari | ‚ö†Ô∏è Limited | ‚ö†Ô∏è Limited | ‚úÖ Full | ‚ö†Ô∏è Limited | ‚ö†Ô∏è Limited | ‚ö†Ô∏è Limited |

For detailed web platform performance testing and reports, run:
```bash
# Run comprehensive tests for all optimizations
./run_web_platform_integration_tests.sh --all-models --all-optimizations

# Generate detailed performance report
python test/scripts/benchmark_db_query.py --report web_platform --format html --output web_platform_report.html

# Generate optimization comparison chart
python test/scripts/benchmark_db_query.py --report web_optimizations --format chart --output web_optimization_chart.png
```

See the [Web Platform Optimization Guide](WEB_PLATFORM_OPTIMIZATION_GUIDE.md) for implementation details and usage recommendations.

### August 2025 Web Platform Implementation Additions

The August 2025 update completes the web platform implementation with:

- **Unified Framework Integration**: Standardized API across all platform components
- **Comprehensive Error Handling**: Graceful degradation with browser-specific recovery strategies
- **Configuration Validation System**: Auto-correction for invalid settings with browser compatibility checks
- **Model Sharding System**: Run large models by distributing across multiple browser tabs
- **Mobile Device Support**: Optimized configurations for mobile browsers

To use the unified framework:

```python
from fixed_web_platform.unified_framework import UnifiedWebPlatform

# Create platform with automatic browser detection
platform = UnifiedWebPlatform(
    model_name="llama-7b",
    model_type="text",
    platform="webgpu"
)

# Run inference with unified API (handles all browser compatibility)
result = platform.run_inference({"input_text": "Sample text"})
```

For model sharding across multiple browser tabs:

```python
from fixed_web_platform.unified_framework.model_sharding import ModelShardingManager

# Create model sharding manager
sharding_manager = ModelShardingManager(
    model_name="llama-7b",
    num_shards=4,
    shard_type="layer"
)

# Initialize sharding (opens browser tabs)
sharding_manager.initialize_sharding()

# Run inference across shards
result = sharding_manager.run_inference_sharded({"input_text": "Sample text"})
```

### April 2025 Memory Optimization Tools

To analyze memory usage and test cross-platform 4-bit inference:

```bash
# Visualize memory usage for models across platforms
python test/visualize_memory_usage.py --model llama --platform webgpu --output html

# Test cross-platform 4-bit inference compatibility and performance
python test/test_cross_platform_4bit.py --model llama --hardware cuda webgpu --output-report report.html

# Test WebGPU 4-bit inference with specialized matrix multiplication kernels
python test/test_webgpu_4bit_inference.py --model llama --all-tests
```

*Note: Performance varies significantly based on hardware, browser version, and model size.*

### Test and Template Database Architecture

The DuckDB/Parquet-based database system is now the primary storage for all benchmark results and templates (JSON output is deprecated). This system provides:

#### Template Database Schema
The database stores templates for tests, skills, benchmarks, and helper functions for 300+ HuggingFace models:
- **Template Tables**:
  - `templates`: Stores core templates indexed by model type and template type
  - `template_helpers`: Common helper functions shared across templates
  - `template_dependencies`: Maps dependencies between templates
  - `template_versions`: Tracks template versions and updates
  - `template_variables`: Defines substitution variables for templates

- **Template Categories**:
  - Test templates (for generating test files)
  - Skill templates (for generating skill implementation files)
  - Benchmark templates (for generating benchmark scripts)
  - Helper templates (shared utility functions)
  - Hardware-specific templates (platform-specific code)

- **Template Management Tools**:
  - `template_database.py`: Core template CRUD operations
  - `template_validator.py`: Validates template syntax and dependencies
  - `template_migration.py`: Migrates templates between versions
  - `template_inheritance.py`: Handles inheritance between templates
  - `template_instantiator.py`: Instantiates templates with model-specific values

#### Benchmark Results Database
The database also stores all benchmark results and test outputs:
- **Performance Improvements**:
  - 50-80% size reduction compared to JSON files
  - 5-20x faster queries for complex analysis
  - 70% less disk I/O for test result management
  - Parallel processing for batch data migration

- **Advanced Features**:
  - SQL-based querying with full JOIN support
  - Foreign key constraints for data integrity
  - Comprehensive schema for all test types
  - Time-series analysis of performance trends
  - Visualization tools for performance comparisons
  - REST API for programmatic access
  - Interactive dashboard for result exploration

- **Core Components**:
  - `create_benchmark_schema.py`: Schema definition and initialization
  - `benchmark_db_converter.py`: JSON to database migration
  - `benchmark_db_updater.py`: Direct database writing interface
  - `benchmark_db_query.py`: Comprehensive query tool
  - `benchmark_db_maintenance.py`: Database optimization
  - `benchmark_db_api.py`: REST API and dashboard
  - `benchmark_db_performance.py`: Performance testing
  - `run_benchmark_with_db.py`: Example integration
  - `cleanup_test_results.py`: Automated migration utility
  - `generate_compatibility_matrix.py`: Creates comprehensive model compatibility matrix
  - `update_db_schema_for_simulation.py`: Updates schema with simulation flags

#### Model Compatibility Matrix
The database enables automatic generation of a comprehensive compatibility matrix for all 300+ HuggingFace model classes:

- **Matrix Generation**:
  ```bash
  # Generate the complete compatibility matrix
  python test/generate_compatibility_matrix.py
  
  # Generate matrix with specific filters
  python test/generate_compatibility_matrix.py --filter vision --hardware cuda,qualcomm,webgpu
  
  # Custom output formats
  python test/generate_compatibility_matrix.py --format markdown --output custom_matrix.md
  ```

- **Matrix Features**:
  - Cross-platform compatibility status for all models
  - Visual indicators for compatibility levels
  - Hardware-specific performance metrics
  - Advanced quantization support indicators
  - Automatic updates via CI/CD pipeline
  - Filtering by model type and hardware platform
  - Custom output formats (markdown, HTML)

Documentation and guides:
- [Benchmark Database Guide](BENCHMARK_DATABASE_GUIDE.md)
- [Database Migration Guide](DATABASE_MIGRATION_GUIDE.md)
- [Phase 16 Database Implementation](PHASE16_DATABASE_IMPLEMENTATION.md)
- [Web Platform Support](README_WEB_PLATFORM_SUPPORT.md)
- [Web Platform Integration Guide](web_platform_integration_guide.md)
- [Template Database Guide](TEMPLATE_INHERITANCE_GUIDE.md)
- [Comprehensive Model Compatibility Matrix](COMPREHENSIVE_MODEL_COMPATIBILITY_MATRIX.md)
- [Simulation Detection Improvements](SIMULATION_DETECTION_IMPROVEMENTS.md)

### Hardware Selection and Performance Prediction System

The framework now includes a comprehensive hardware selection and performance prediction system that leverages machine learning and historical benchmark data to provide optimal hardware recommendations:

- **Hardware Selection**: Automatically determines the best hardware platform for a given model and task
- **Performance Prediction**: Predicts throughput, latency, and memory usage for any model-hardware combination
- **Cross-Platform Support**: Covers all supported hardware platforms including CPU, CUDA, ROCm, MPS, OpenVINO, QNN, WebNN, and WebGPU
- **Precision-Aware**: Considers different precision formats (fp32, fp16, int8) in recommendations
- **Visualization Tools**: Generates comparative visualizations for hardware performance analysis

For detailed information, see the [Hardware Selection Guide](HARDWARE_SELECTION_GUIDE.md).