#!/usr/bin/env python3
"""
Benchmark Database Updater

This module provides functionality to update the benchmark database with new test results.
It handles inserts, updates, and upserts for all types of benchmark data.

Usage:
    python benchmark_db_updater.py --input performance_results.json --type performance
    python benchmark_db_updater.py --input hardware_detection.json --type hardware
    python benchmark_db_updater.py --input compatibility_matrix.json --type compatibility
"""

import os
import sys
import json
import logging
import argparse
import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional, Union, Tuple

try:
    import duckdb
    import pandas as pd
except ImportError:
    print("Error: Required packages not installed. Please install with:")
    print("pip install duckdb pandas")
    sys.exit(1)

# Configure logging
logging.basicConfig(level=logging.INFO,
                   format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class BenchmarkDBUpdater:
    """
    Class for updating the benchmark database with new test results.
    """
    
    def __init__(self, db_path: str = "./benchmark_db.duckdb", debug: bool = False):
        """
        Initialize the benchmark database updater.
        
        Args:
            db_path: Path to the DuckDB database
            debug: Enable debug logging
        """
        self.db_path = db_path
        
        # Set up logging
        if debug:
            logger.setLevel(logging.DEBUG)
        
        # Verify database exists
        if not os.path.exists(db_path):
            logger.warning(f"Database file not found: {db_path}")
            logger.info("Creating a new database file")
        
        # Connect to database
        try:
            self.conn = duckdb.connect(db_path, read_only=False)
            logger.info(f"Connected to database: {db_path}")
        except Exception as e:
            logger.error(f"Error connecting to database: {e}")
            raise
        
        # Set up table mapping
        self.table_mapping = {
            "performance": "performance_results",
            "hardware": "hardware_platforms",
            "compatibility": "hardware_compatibility",
            "integration": "integration_test_results"
        }
        
        # Verify schema
        self._verify_schema()
    
    def _verify_schema(self):
        """Verify that the database schema is valid."""
        try:
            tables = {row[0] for row in self.conn.execute("SHOW TABLES").fetchall()}
            required_tables = {
                "models", "hardware_platforms", "test_runs", 
                "performance_results", "hardware_compatibility", 
                "integration_test_results"
            }
            
            missing_tables = required_tables - tables
            if missing_tables:
                logger.error(f"Missing required tables: {missing_tables}")
                logger.info("Run create_benchmark_schema.py to initialize the database schema.")
                raise ValueError(f"Missing required tables: {missing_tables}")
            
            logger.debug("Database schema verification successful")
        except Exception as e:
            logger.error(f"Error verifying schema: {e}")
            raise
    
    def _get_or_create_model(self, model_data: Dict[str, Any]) -> int:
        """
        Get or create a model record.
        
        Args:
            model_data: Dictionary containing model data
            
        Returns:
            int: Model ID
        """
        model_name = model_data.get("model_name")
        if not model_name:
            raise ValueError("Model name is required")
        
        # Check if model exists
        result = self.conn.execute(f"SELECT model_id FROM models WHERE model_name = '{model_name}'").fetchone()
        if result:
            model_id = result[0]
            logger.debug(f"Found existing model ID: {model_id} for model: {model_name}")
            return model_id
        
        # Create new model record
        model_family = model_data.get("model_family", "")
        modality = model_data.get("modality", "")
        source = model_data.get("source", "")
        version = model_data.get("version", "")
        parameters_million = model_data.get("parameters_million", 0)
        metadata = json.dumps(model_data.get("metadata", {}))
        
        # Get next model ID
        next_id = self.conn.execute("SELECT COALESCE(MAX(model_id) + 1, 1) FROM models").fetchone()[0]
        
        # Insert new model
        self.conn.execute(f"""
            INSERT INTO models (
                model_id, model_name, model_family, modality, 
                source, version, parameters_million, metadata
            ) VALUES (
                {next_id}, '{model_name}', '{model_family}', '{modality}',
                '{source}', '{version}', {parameters_million}, '{metadata}'
            )
        """)
        
        logger.info(f"Created new model record with ID: {next_id} for model: {model_name}")
        return next_id
    
    def _get_or_create_hardware(self, hardware_data: Dict[str, Any]) -> int:
        """
        Get or create a hardware platform record.
        
        Args:
            hardware_data: Dictionary containing hardware data
            
        Returns:
            int: Hardware ID
        """
        hardware_type = hardware_data.get("hardware_type")
        device_name = hardware_data.get("device_name", "")
        
        if not hardware_type:
            raise ValueError("Hardware type is required")
        
        # Check if hardware exists
        query = f"SELECT hardware_id FROM hardware_platforms WHERE hardware_type = '{hardware_type}'"
        if device_name:
            query += f" AND device_name = '{device_name}'"
        
        result = self.conn.execute(query).fetchone()
        if result:
            hardware_id = result[0]
            logger.debug(f"Found existing hardware ID: {hardware_id} for type: {hardware_type}, device: {device_name}")
            return hardware_id
        
        # Create new hardware record
        platform = hardware_data.get("platform", "")
        platform_version = hardware_data.get("platform_version", "")
        driver_version = hardware_data.get("driver_version", "")
        memory_gb = hardware_data.get("memory_gb", 0)
        compute_units = hardware_data.get("compute_units", 0)
        metadata = json.dumps(hardware_data.get("metadata", {}))
        
        # Get next hardware ID
        next_id = self.conn.execute("SELECT COALESCE(MAX(hardware_id) + 1, 1) FROM hardware_platforms").fetchone()[0]
        
        # Insert new hardware
        self.conn.execute(f"""
            INSERT INTO hardware_platforms (
                hardware_id, hardware_type, device_name, platform, 
                platform_version, driver_version, memory_gb, compute_units, metadata
            ) VALUES (
                {next_id}, '{hardware_type}', '{device_name}', '{platform}',
                '{platform_version}', '{driver_version}', {memory_gb}, {compute_units}, '{metadata}'
            )
        """)
        
        logger.info(f"Created new hardware record with ID: {next_id} for type: {hardware_type}, device: {device_name}")
        return next_id
    
    def _get_or_create_test_run(self, test_data: Dict[str, Any]) -> int:
        """
        Get or create a test run record.
        
        Args:
            test_data: Dictionary containing test run data
            
        Returns:
            int: Test run ID
        """
        test_name = test_data.get("test_name")
        test_type = test_data.get("test_type", "performance")
        
        if not test_name:
            # Generate a default test name if not provided
            timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
            test_name = f"{test_type}_test_{timestamp}"
            logger.debug(f"Generated test name: {test_name}")
        
        # Get next test run ID
        next_id = self.conn.execute("SELECT COALESCE(MAX(run_id) + 1, 1) FROM test_runs").fetchone()[0]
        
        # Extract other test run data
        started_at = test_data.get("started_at", datetime.datetime.now().isoformat())
        completed_at = test_data.get("completed_at", datetime.datetime.now().isoformat())
        execution_time_seconds = test_data.get("execution_time_seconds", 0)
        success = test_data.get("success", True)
        git_commit = test_data.get("git_commit", "")
        git_branch = test_data.get("git_branch", "")
        command_line = test_data.get("command_line", "")
        metadata = json.dumps(test_data.get("metadata", {}))
        
        # Insert new test run
        self.conn.execute(f"""
            INSERT INTO test_runs (
                run_id, test_name, test_type, started_at, completed_at,
                execution_time_seconds, success, git_commit, git_branch,
                command_line, metadata
            ) VALUES (
                {next_id}, '{test_name}', '{test_type}', '{started_at}', '{completed_at}',
                {execution_time_seconds}, {success}, '{git_commit}', '{git_branch}',
                '{command_line}', '{metadata}'
            )
        """)
        
        logger.info(f"Created new test run record with ID: {next_id} for test: {test_name}")
        return next_id
    
    def update_performance_results(self, 
                                  performance_data: List[Dict[str, Any]], 
                                  test_run_id: Optional[int] = None) -> List[int]:
        """
        Update the database with performance benchmark results.
        
        Args:
            performance_data: List of dictionaries containing performance data
            test_run_id: Optional test run ID (will create a new one if not provided)
            
        Returns:
            List[int]: List of result IDs
        """
        if not performance_data:
            logger.warning("No performance data provided")
            return []
        
        # Create a test run if needed
        run_id = test_run_id
        if not run_id:
            test_run_data = {"test_type": "performance"}
            # Extract test name and other metadata if available in the first performance data
            if isinstance(performance_data[0], dict):
                test_name = performance_data[0].get("test_name", "")
                if test_name:
                    test_run_data["test_name"] = test_name
                
                # Extract git info if available
                git_commit = performance_data[0].get("git_commit", "")
                if git_commit:
                    test_run_data["git_commit"] = git_commit
                
                git_branch = performance_data[0].get("git_branch", "")
                if git_branch:
                    test_run_data["git_branch"] = git_branch
            
            run_id = self._get_or_create_test_run(test_run_data)
        
        result_ids = []
        for perf_data in performance_data:
            # Get or create model record
            model_info = {
                "model_name": perf_data.get("model_name", "unknown"),
                "model_family": perf_data.get("model_family", ""),
                "modality": perf_data.get("modality", ""),
                "source": perf_data.get("source", ""),
                "version": perf_data.get("version", ""),
                "parameters_million": perf_data.get("parameters_million", 0),
                "metadata": perf_data.get("model_metadata", {})
            }
            model_id = self._get_or_create_model(model_info)
            
            # Get or create hardware record
            hardware_info = {
                "hardware_type": perf_data.get("hardware_type", "cpu"),
                "device_name": perf_data.get("device_name", ""),
                "platform": perf_data.get("platform", ""),
                "platform_version": perf_data.get("platform_version", ""),
                "driver_version": perf_data.get("driver_version", ""),
                "memory_gb": perf_data.get("memory_gb", 0),
                "compute_units": perf_data.get("compute_units", 0),
                "metadata": perf_data.get("hardware_metadata", {})
            }
            hardware_id = self._get_or_create_hardware(hardware_info)
            
            # Get next result ID
            next_id = self.conn.execute("SELECT COALESCE(MAX(result_id) + 1, 1) FROM performance_results").fetchone()[0]
            
            # Extract performance metrics
            test_case = perf_data.get("test_case", "default")
            batch_size = perf_data.get("batch_size", 1)
            precision = perf_data.get("precision", "fp32")
            total_time_seconds = perf_data.get("total_time_seconds", 0)
            average_latency_ms = perf_data.get("average_latency_ms", perf_data.get("latency_avg", 0))
            throughput = perf_data.get("throughput_items_per_second", perf_data.get("throughput", 0))
            memory_peak_mb = perf_data.get("memory_peak_mb", perf_data.get("memory_peak", 0))
            iterations = perf_data.get("iterations", 0)
            warmup_iterations = perf_data.get("warmup_iterations", 0)
            metrics = json.dumps(perf_data.get("metrics", {}))
            
            # Insert performance result
            self.conn.execute(f"""
                INSERT INTO performance_results (
                    result_id, run_id, model_id, hardware_id, test_case, batch_size,
                    precision, total_time_seconds, average_latency_ms, throughput_items_per_second,
                    memory_peak_mb, iterations, warmup_iterations, metrics
                ) VALUES (
                    {next_id}, {run_id}, {model_id}, {hardware_id}, '{test_case}', {batch_size},
                    '{precision}', {total_time_seconds}, {average_latency_ms}, {throughput},
                    {memory_peak_mb}, {iterations}, {warmup_iterations}, '{metrics}'
                )
            """)
            
            result_ids.append(next_id)
            logger.debug(f"Inserted performance result with ID: {next_id}")
            
            # Insert batch results if available
            batch_results = perf_data.get("batch_results", [])
            if batch_results:
                for i, batch in enumerate(batch_results):
                    batch_index = batch.get("batch_index", i)
                    batch_size = batch.get("batch_size", perf_data.get("batch_size", 1))
                    latency_ms = batch.get("latency_ms", 0)
                    memory_usage_mb = batch.get("memory_usage_mb", 0)
                    
                    # Get next batch ID
                    batch_id = self.conn.execute("SELECT COALESCE(MAX(batch_id) + 1, 1) FROM performance_batch_results").fetchone()[0]
                    
                    self.conn.execute(f"""
                        INSERT INTO performance_batch_results (
                            batch_id, result_id, batch_index, batch_size, latency_ms, memory_usage_mb
                        ) VALUES (
                            {batch_id}, {next_id}, {batch_index}, {batch_size}, {latency_ms}, {memory_usage_mb}
                        )
                    """)
                
                logger.debug(f"Inserted {len(batch_results)} batch results for performance result ID: {next_id}")
        
        logger.info(f"Inserted {len(result_ids)} performance results")
        return result_ids
    
    def update_hardware_compatibility(self, 
                                     compatibility_data: List[Dict[str, Any]], 
                                     test_run_id: Optional[int] = None) -> List[int]:
        """
        Update the database with hardware compatibility results.
        
        Args:
            compatibility_data: List of dictionaries containing compatibility data
            test_run_id: Optional test run ID (will create a new one if not provided)
            
        Returns:
            List[int]: List of compatibility IDs
        """
        if not compatibility_data:
            logger.warning("No compatibility data provided")
            return []
        
        # Create a test run if needed
        run_id = test_run_id
        if not run_id:
            test_run_data = {"test_type": "hardware"}
            # Extract test name and other metadata if available
            if isinstance(compatibility_data[0], dict):
                test_name = compatibility_data[0].get("test_name", "")
                if test_name:
                    test_run_data["test_name"] = test_name
            
            run_id = self._get_or_create_test_run(test_run_data)
        
        compatibility_ids = []
        for compat_data in compatibility_data:
            # Get or create model record
            model_info = {
                "model_name": compat_data.get("model_name", "unknown"),
                "model_family": compat_data.get("model_family", ""),
                "modality": compat_data.get("modality", ""),
                "source": compat_data.get("source", ""),
                "version": compat_data.get("version", ""),
                "parameters_million": compat_data.get("parameters_million", 0),
                "metadata": compat_data.get("model_metadata", {})
            }
            model_id = self._get_or_create_model(model_info)
            
            # Get or create hardware record
            hardware_info = {
                "hardware_type": compat_data.get("hardware_type", "cpu"),
                "device_name": compat_data.get("device_name", ""),
                "platform": compat_data.get("platform", ""),
                "platform_version": compat_data.get("platform_version", ""),
                "driver_version": compat_data.get("driver_version", ""),
                "memory_gb": compat_data.get("memory_gb", 0),
                "compute_units": compat_data.get("compute_units", 0),
                "metadata": compat_data.get("hardware_metadata", {})
            }
            hardware_id = self._get_or_create_hardware(hardware_info)
            
            # Get next compatibility ID
            next_id = self.conn.execute("SELECT COALESCE(MAX(compatibility_id) + 1, 1) FROM hardware_compatibility").fetchone()[0]
            
            # Extract compatibility data
            is_compatible = compat_data.get("is_compatible", False)
            detection_success = compat_data.get("detection_success", True)
            initialization_success = compat_data.get("initialization_success", False)
            error_message = compat_data.get("error_message", "")
            error_type = compat_data.get("error_type", "")
            suggested_fix = compat_data.get("suggested_fix", "")
            workaround_available = compat_data.get("workaround_available", False)
            compatibility_score = compat_data.get("compatibility_score", 0 if not is_compatible else 1)
            metadata = json.dumps(compat_data.get("metadata", {}))
            
            # Insert compatibility result
            self.conn.execute(f"""
                INSERT INTO hardware_compatibility (
                    compatibility_id, run_id, model_id, hardware_id, is_compatible,
                    detection_success, initialization_success, error_message, error_type,
                    suggested_fix, workaround_available, compatibility_score, metadata
                ) VALUES (
                    {next_id}, {run_id}, {model_id}, {hardware_id}, {is_compatible},
                    {detection_success}, {initialization_success}, '{error_message}', '{error_type}',
                    '{suggested_fix}', {workaround_available}, {compatibility_score}, '{metadata}'
                )
            """)
            
            compatibility_ids.append(next_id)
            logger.debug(f"Inserted compatibility result with ID: {next_id}")
        
        logger.info(f"Inserted {len(compatibility_ids)} compatibility results")
        return compatibility_ids
    
    def update_integration_test_results(self, 
                                       test_results: List[Dict[str, Any]], 
                                       test_run_id: Optional[int] = None) -> List[int]:
        """
        Update the database with integration test results.
        
        Args:
            test_results: List of dictionaries containing integration test results
            test_run_id: Optional test run ID (will create a new one if not provided)
            
        Returns:
            List[int]: List of test result IDs
        """
        if not test_results:
            logger.warning("No integration test results provided")
            return []
        
        # Create a test run if needed
        run_id = test_run_id
        if not run_id:
            test_run_data = {"test_type": "integration"}
            # Extract test name and other metadata if available
            if isinstance(test_results[0], dict):
                test_name = test_results[0].get("test_name", "")
                if test_name:
                    test_run_data["test_name"] = test_name
            
            run_id = self._get_or_create_test_run(test_run_data)
        
        result_ids = []
        for test_result in test_results:
            # Get next test result ID
            next_id = self.conn.execute("SELECT COALESCE(MAX(test_result_id) + 1, 1) FROM integration_test_results").fetchone()[0]
            
            # Extract test result data
            test_module = test_result.get("test_module", "")
            test_class = test_result.get("test_class", "")
            test_name = test_result.get("test_name", "")
            status = test_result.get("status", "")
            execution_time_seconds = test_result.get("execution_time_seconds", 0)
            error_message = test_result.get("error_message", "")
            error_traceback = test_result.get("error_traceback", "")
            metadata = json.dumps(test_result.get("metadata", {}))
            
            # Get model and hardware IDs if provided
            model_id = "NULL"
            if "model_name" in test_result:
                model_info = {
                    "model_name": test_result.get("model_name", ""),
                    "model_family": test_result.get("model_family", ""),
                    "modality": test_result.get("modality", ""),
                    "source": test_result.get("source", ""),
                    "version": test_result.get("version", ""),
                    "parameters_million": test_result.get("parameters_million", 0),
                    "metadata": test_result.get("model_metadata", {})
                }
                model_id = self._get_or_create_model(model_info)
            
            hardware_id = "NULL"
            if "hardware_type" in test_result:
                hardware_info = {
                    "hardware_type": test_result.get("hardware_type", ""),
                    "device_name": test_result.get("device_name", ""),
                    "platform": test_result.get("platform", ""),
                    "platform_version": test_result.get("platform_version", ""),
                    "driver_version": test_result.get("driver_version", ""),
                    "memory_gb": test_result.get("memory_gb", 0),
                    "compute_units": test_result.get("compute_units", 0),
                    "metadata": test_result.get("hardware_metadata", {})
                }
                hardware_id = self._get_or_create_hardware(hardware_info)
            
            # Insert test result
            self.conn.execute(f"""
                INSERT INTO integration_test_results (
                    test_result_id, run_id, test_module, test_class, test_name,
                    status, execution_time_seconds, hardware_id, model_id,
                    error_message, error_traceback, metadata
                ) VALUES (
                    {next_id}, {run_id}, '{test_module}', '{test_class}', '{test_name}',
                    '{status}', {execution_time_seconds}, {hardware_id}, {model_id},
                    '{error_message}', '{error_traceback}', '{metadata}'
                )
            """)
            
            result_ids.append(next_id)
            logger.debug(f"Inserted integration test result with ID: {next_id}")
            
            # Insert assertions if available
            assertions = test_result.get("assertions", [])
            if assertions:
                for i, assertion in enumerate(assertions):
                    assertion_name = assertion.get("assertion_name", "")
                    passed = assertion.get("passed", False)
                    expected_value = assertion.get("expected_value", "")
                    actual_value = assertion.get("actual_value", "")
                    message = assertion.get("message", "")
                    
                    # Get next assertion ID
                    assertion_id = self.conn.execute("SELECT COALESCE(MAX(assertion_id) + 1, 1) FROM integration_test_assertions").fetchone()[0]
                    
                    self.conn.execute(f"""
                        INSERT INTO integration_test_assertions (
                            assertion_id, test_result_id, assertion_name, passed,
                            expected_value, actual_value, message
                        ) VALUES (
                            {assertion_id}, {next_id}, '{assertion_name}', {passed},
                            '{expected_value}', '{actual_value}', '{message}'
                        )
                    """)
                
                logger.debug(f"Inserted {len(assertions)} assertions for test result ID: {next_id}")
        
        logger.info(f"Inserted {len(result_ids)} integration test results")
        return result_ids
    
    def load_from_file(self, file_path: str, data_type: str) -> List[int]:
        """
        Load data from a JSON file and update the database.
        
        Args:
            file_path: Path to the JSON file
            data_type: Type of data (performance, hardware, compatibility, integration)
            
        Returns:
            List[int]: List of record IDs
        """
        try:
            with open(file_path, 'r') as f:
                data = json.load(f)
            
            logger.info(f"Loaded {len(data) if isinstance(data, list) else 1} records from {file_path}")
            
            # Convert to list if it's a single record
            if not isinstance(data, list):
                data = [data]
            
            # Update database based on data type
            if data_type == "performance":
                return self.update_performance_results(data)
            elif data_type == "compatibility":
                return self.update_hardware_compatibility(data)
            elif data_type == "integration":
                return self.update_integration_test_results(data)
            else:
                logger.error(f"Unsupported data type: {data_type}")
                return []
        
        except Exception as e:
            logger.error(f"Error loading data from {file_path}: {e}")
            return []

def main():
    """Command-line interface for the benchmark database updater."""
    parser = argparse.ArgumentParser(description="Benchmark Database Updater")
    parser.add_argument("--db", default="./benchmark_db.duckdb",
                        help="Path to the DuckDB database")
    parser.add_argument("--input", required=True,
                        help="Path to the input JSON file")
    parser.add_argument("--type", choices=["performance", "compatibility", "integration"],
                        required=True, help="Type of data to update")
    parser.add_argument("--debug", action="store_true",
                        help="Enable debug logging")
    args = parser.parse_args()
    
    # Create updater
    try:
        updater = BenchmarkDBUpdater(db_path=args.db, debug=args.debug)
    except Exception as e:
        logger.error(f"Error initializing updater: {e}")
        return
    
    # Load data and update database
    record_ids = updater.load_from_file(args.input, args.type)
    
    if record_ids:
        logger.info(f"Updated {len(record_ids)} records in the database")
    else:
        logger.error("No records were updated")

if __name__ == "__main__":
    main()