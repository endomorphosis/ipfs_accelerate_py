{
  "model_families": {
    "text_embeddings": {
      "template_model": "hf_bert",
      "description": "Models designed for text encoding and embeddings",
      "task_types": ["text-classification", "fill-mask", "feature-extraction"],
      "models": [
        "bert", "roberta", "distilbert", "electra", "albert", "camembert", "xlm-roberta", 
        "deberta", "deberta-v2", "flaubert", "mpnet", "ernie", "megatron-bert", "roformer",
        "xlnet", "bart", "ibert", "mobilebert", "squeezebert", "nezha", "layoutlm", "canine", 
        "luke", "xlm", "reformer", "funnel", "bloom", "ernie-m"
      ]
    },
    "text_generation": {
      "template_model": "hf_llama",
      "description": "Models designed for autoregressive text generation",
      "task_types": ["text-generation", "causal-lm"],
      "models": [
        "llama", "gpt2", "gpt-neo", "gpt-neox", "gptj", "opt", "bloom", "mistral", "falcon",
        "gemma", "phi", "mpt", "xglm", "llama2", "llama3", "stableLM", "starcoder", "rwkv",
        "codegen", "codellama", "santacoder", "gpt-bigcode", "qwen2", "qwen3", "gemma2",
        "phi3", "deepseek", "mixtral", "persimmon", "command-r", "mamba", "mamba2"
      ]
    },
    "seq_to_seq": {
      "template_model": "hf_t5",
      "description": "Models designed for sequence-to-sequence tasks like translation and summarization",
      "task_types": ["text2text-generation", "summarization", "translation"],
      "models": [
        "t5", "mt5", "byT5", "bart", "pegasus", "marian", "m2m-100", "longt5", "led", "blenderbot",
        "umt5", "mt5", "nat", "nllb", "flan-t5", "fsmt", "switch-transformers", "mbart", "bigbird-pegasus",
        "prophetnet", "bigbird", "longt5", "xlm-prophetnet", "pegasus-x", "plbart"
      ]
    },
    "vision": {
      "template_model": "hf_vit",
      "description": "Models designed for computer vision tasks",
      "task_types": ["image-classification", "object-detection", "image-segmentation", "image-to-text", "image-to-image"],
      "models": [
        "vit", "deit", "beit", "segformer", "detr", "convnext", "bit", "regnet", "mobilenetv1", 
        "mobilenetv2", "swin", "convnext", "mobilevit", "dino", "vitdet", "dinov2", "deformable-detr",
        "maskformer", "upernet", "yolos", "glpn", "dpt", "conditional-detr", "table-transformer",
        "seggpt", "vitmatte", "seggpt", "resnet", "pvt", "poolformer", "van"
      ]
    },
    "audio": {
      "template_model": "hf_whisper",
      "alternative_templates": ["hf_wav2vec2", "hf_clap"],
      "description": "Models designed for audio processing tasks",
      "task_types": ["automatic-speech-recognition", "audio-classification", "audio-to-audio"],
      "models": [
        "whisper", "wav2vec2", "clap", "hubert", "wavlm", "mms", "encodec", "sew", "sew-d",
        "unispeech", "unispeech-sat", "speecht5", "musicgen", "bark", "fastspeech2-conformer",
        "mctct", "audio-spectrogram-transformer", "wavlm", "univnet", "vits"
      ]
    },
    "multimodal": {
      "template_model": "hf_llava",
      "alternative_templates": ["hf_clip", "hf_xclip"],
      "description": "Models designed for multiple modalities (text+image or text+audio)",
      "task_types": ["image-to-text", "text-to-image", "visual-question-answering"],
      "models": [
        "llava", "llava-next", "llava-next-video", "blip", "blip-2", "instructblip", "vilt",
        "git", "paligemma", "clip", "xclip", "chinese-clip", "siglip", "clipseg", "owlvit",
        "owlv2", "fuyu", "flava", "bridgetower", "vision-text-dual-encoder", "vision-encoder-decoder",
        "videomae", "tvlt", "instructblipvideo", "video-llava"
      ]
    }
  },
  
  "task_handler_map": {
    "text-classification": "text_classification",
    "token-classification": "token_classification",
    "fill-mask": "fill_mask",
    "feature-extraction": "text_embedding",
    "text-generation": "text_generation",
    "causal-lm": "text_generation",
    "text2text-generation": "text_generation",
    "summarization": "summarization",
    "translation": "translation",
    "image-classification": "image_classification",
    "object-detection": "object_detection",
    "image-segmentation": "image_segmentation",
    "image-to-text": "image_to_text",
    "image-to-image": "image_to_image",
    "automatic-speech-recognition": "speech_recognition",
    "audio-classification": "audio_classification",
    "audio-to-audio": "audio_to_audio",
    "visual-question-answering": "visual_qa"
  },
  
  "hardware_support_templates": {
    "cpu": {
      "include_for_all": true,
      "template_name": "init_cpu",
      "handler_template": "create_cpu_{task}_endpoint_handler"
    },
    "cuda": {
      "include_for_all": true,
      "template_name": "init_cuda",
      "handler_template": "create_cuda_{task}_endpoint_handler"
    },
    "openvino": {
      "include_for_all": true,
      "template_name": "init_openvino",
      "handler_template": "create_openvino_{task}_endpoint_handler"
    },
    "apple": {
      "include_for_all": true,
      "template_name": "init_apple",
      "handler_template": "create_apple_{task}_endpoint_handler"
    },
    "qualcomm": {
      "include_for_all": false,
      "excluded_model_families": ["multimodal"],
      "template_name": "init_qualcomm",
      "handler_template": "create_qualcomm_{task}_endpoint_handler"
    },
    "webnn": {
      "include_for_all": false,
      "included_model_families": ["text_embeddings", "text_generation", "vision"],
      "template_name": "init_webnn",
      "handler_template": "create_webnn_{task}_endpoint_handler"
    },
    "webgpu": {
      "include_for_all": false,
      "included_model_families": ["text_embeddings", "text_generation", "vision"],
      "template_name": "init_webgpu",
      "handler_template": "create_webgpu_{task}_endpoint_handler"
    }
  }
}