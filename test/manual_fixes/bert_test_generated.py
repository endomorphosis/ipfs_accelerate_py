#!/usr/bin/env python3
"""
Test file for bert-base-uncased model.

This file is auto-generated using the template-based test generator.
Generated: 2025-03-10 01:26:53
"""

import os
import sys
import logging
import torch
import numpy as np
from pathlib import Path

# Set up logging
logging.basicConfig(level=logging.INFO, 
                   format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class TestBertBaseUncased:
    """Test class for bert-base-uncased model."""
    
    def __init__(self):
        """Initialize the test with model details and hardware detection."""
        self.model_name = "bert-base-uncased"
        self.model_type = "text_embedding"
        self.setup_hardware()
    
    def setup_hardware(self):
        """Set up hardware detection for the template."""
        # CUDA support
        self.has_cuda = torch.cuda.is_available()
        # MPS support (Apple Silicon)
        self.has_mps = hasattr(torch.backends, 'mps') and torch.backends.mps.is_available()
        # ROCm support (AMD)
        self.has_rocm = hasattr(torch, 'version') and hasattr(torch.version, 'hip') and torch.version.hip is not None
        # OpenVINO support
        self.has_openvino = 'openvino' in sys.modules
        # Qualcomm AI Engine support
        self.has_qualcomm = 'qti' in sys.modules or 'qnn_wrapper' in sys.modules
        # WebNN/WebGPU support
        self.has_webnn = False  # Will be set by WebNN bridge if available
        self.has_webgpu = False  # Will be set by WebGPU bridge if available
        
        # Set default device
        if self.has_cuda:
            self.device = 'cuda'
        elif self.has_mps:
            self.device = 'mps'
        elif self.has_rocm:
            self.device = 'cuda'  # ROCm uses CUDA compatibility layer
        else:
            self.device = 'cpu'
            
        logger.info(f"Using device: {self.device}")
        
    def get_model(self):
        """Load model from HuggingFace."""
        try:
            from transformers import AutoModel, AutoTokenizer
            
            # Get tokenizer
            tokenizer = AutoTokenizer.from_pretrained(self.model_name)
            
            # Get model
            model = AutoModel.from_pretrained(self.model_name)
            model = model.to(self.device)
            
            return model, tokenizer
        except Exception as e:
            logger.error(f"Error loading model: {e}")
            return None, None
    
    def get_model_specific(self):
        """Load model with specialized configuration."""
        try:
            from transformers import AutoModel, AutoTokenizer
            
            # Get tokenizer with specific settings
            tokenizer = AutoTokenizer.from_pretrained(
                self.model_name,
                truncation_side="right",
                use_fast=True
            )
            
            # Get model with specific settings
            model = AutoModel.from_pretrained(
                self.model_name,
                torchscript=True if self.device == 'cpu' else False
            )
            model = model.to(self.device)
            
            # Put model in evaluation mode
            model.eval()
            
            return model, tokenizer
        except Exception as e:
            logger.error(f"Error loading model with specific settings: {e}")
            return None, None
    
    def test_basic_inference(self):
        """Run a basic inference test with the model."""
        model, tokenizer = self.get_model()
        
        if model is None or tokenizer is None:
            logger.error("Failed to load model or tokenizer")
            return False
        
        try:
            # Prepare input
                        # Prepare text input
            text = "This is a sample text for testing the {{model_name}} model."
            inputs = tokenizer(text, return_tensors="pt")
            inputs = {k: v.to(self.device) for k, v in inputs.items()}
            
            # Run inference
            with torch.no_grad():
                outputs = model(**inputs)
                
            # Check outputs
                        # Check output shape and values
            assert hasattr(outputs, "last_hidden_state"), "Missing last_hidden_state in outputs"
            assert outputs.last_hidden_state.shape[0] == 1, "Batch size should be 1"
            assert outputs.last_hidden_state.shape[1] > 0, "Sequence length should be positive"
            logger.info(f"Output shape: {outputs.last_hidden_state.shape}")
            
            logger.info("Basic inference test passed")
            return True
        except Exception as e:
            logger.error(f"Error during inference: {e}")
            return False
    
    def test_hardware_compatibility(self):
        """Test model compatibility with different hardware platforms."""
        devices_to_test = []
        
        if self.has_cuda:
            devices_to_test.append('cuda')
        if self.has_mps:
            devices_to_test.append('mps')
        if self.has_rocm:
            devices_to_test.append('cuda')  # ROCm uses CUDA compatibility layer
        if self.has_openvino:
            devices_to_test.append('openvino')
        if self.has_qualcomm:
            devices_to_test.append('qualcomm')
        
        # Always test CPU
        if 'cpu' not in devices_to_test:
            devices_to_test.append('cpu')
        
        results = {}
        
        for device in devices_to_test:
            try:
                logger.info(f"Testing on {device}...")
                original_device = self.device
                self.device = device
                
                # Run a simple test
                success = self.test_basic_inference()
                results[device] = success
                
                # Restore original device
                self.device = original_device
            except Exception as e:
                logger.error(f"Error testing on {device}: {e}")
                results[device] = False
        
        return results
    
    def run(self):
        """Run all tests."""
        logger.info(f"Testing {self.model_name} on {self.device}")
        
        # Run basic inference test
        basic_result = self.test_basic_inference()
        
        # Run hardware compatibility test
        hw_results = self.test_hardware_compatibility()
        
        # Summarize results
        logger.info("Test Results:")
        logger.info(f"- Basic inference: {'PASS' if basic_result else 'FAIL'}")
        logger.info("- Hardware compatibility:")
        for device, result in hw_results.items():
            logger.info(f"  - {device}: {'PASS' if result else 'FAIL'}")
        
        return basic_result and all(hw_results.values())


# Additional methods for text embedding models
def test_embedding_similarity(self):
    """Test embedding similarity functionality."""
    model, tokenizer = self.get_model()
    
    if model is None or tokenizer is None:
        logger.error("Failed to load model or tokenizer")
        return False
    
    try:
        # Prepare input texts
        texts = [
            "This is a sample text for testing embeddings.",
            "Another example text that is somewhat similar.",
            "This text is completely different from the others."
        ]
        
        # Get embeddings
        embeddings = []
        for text in texts:
            inputs = tokenizer(text, return_tensors="pt")
            inputs = {k: v.to(self.device) for k, v in inputs.items()}
            
            with torch.no_grad():
                outputs = model(**inputs)
                
            # Use mean pooling to get sentence embedding
            embedding = outputs.last_hidden_state.mean(dim=1)
            embeddings.append(embedding)
        
        # Calculate similarities
        import torch.nn.functional as F
        
        sim_0_1 = F.cosine_similarity(embeddings[0], embeddings[1])
        sim_0_2 = F.cosine_similarity(embeddings[0], embeddings[2])
        
        logger.info(f"Similarity between text 0 and 1: {sim_0_1.item():.4f}")
        logger.info(f"Similarity between text 0 and 2: {sim_0_2.item():.4f}")
        
        # First two should be more similar than first and third
        assert sim_0_1 > sim_0_2, "Expected similarity between similar texts to be higher"
        
        return True
    except Exception as e:
        logger.error(f"Error during embedding similarity test: {e}")
        return False


if __name__ == "__main__":
    # Create and run the test
    test = TestBertBaseUncased()
    test.run()
