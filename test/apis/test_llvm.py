#!/usr/bin/env python
"""
Test suite for LLVM API implementation.

This module tests the LLVM API backend functionality, including:
- Connection to LLVM server
- Request handling
- Response processing
- Error handling
- Queue and backoff systems
"""

import os
import sys
import unittest
import json
import time
import random
import threading
from concurrent.futures import ThreadPoolExecutor
from unittest import mock

# Add parent directory to path for imports
script_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(script_dir)
sys.path.insert(0, parent_dir)
grand_parent_dir = os.path.dirname(parent_dir)
sys.path.insert(0, grand_parent_dir)

# Import LLVM client - adjust import path as needed
try:
    from ipfs_accelerate_py.ipfs_accelerate_py.api_backends.llvm import LlvmClient
except ImportError:
    try:
        from ipfs_accelerate_py.api_backends.llvm import LlvmClient
    except ImportError:
        # Mock implementation for testing
        class LlvmClient:
            def __init__(self, **kwargs):
                self.api_key = kwargs.get("api_key", "test_key")
                self.base_url = kwargs.get("base_url", "http://localhost:8000")
                self.request_count = 0
                self.max_retries = 3
                self.retry_delay = 1
                
            def set_api_key(self, api_key):
                self.api_key = api_key
                
            def get_model_info(self, model_id):
                self.request_count += 1
                return {"model_id": model_id, "status": "loaded"}
                
            def run_inference(self, model_id, inputs, **kwargs):
                self.request_count += 1
                return {"model_id": model_id, "outputs": f"Output for {inputs}"}
                
            def list_models(self):
                self.request_count += 1
                return {"models": ["model1", "model2", "model3"]}


class TestLlvmApiBackend(unittest.TestCase):
    """Test cases for LLVM API backend implementation."""
    
    def setUp(self):
        """Set up test environment."""
        # Use mock server by default
        self.client = LlvmClient(
            api_key="test_key",
            base_url="http://mock-llvm-server"
        )
        
        # Optional: Configure with real credentials from environment variables
        api_key = os.environ.get("LLVM_API_KEY")
        base_url = os.environ.get("LLVM_BASE_URL")
        
        if api_key and base_url:
            self.client = LlvmClient(
                api_key=api_key,
                base_url=base_url
            )
            self.using_real_client = True
        else:
            self.using_real_client = False
    
    def test_initialization(self):
        """Test client initialization with API key."""
        client = LlvmClient(api_key="test_api_key")
        self.assertEqual(client.api_key, "test_api_key")
        
        # Test initialization without API key
        with mock.patch.dict(os.environ, {"LLVM_API_KEY": "env_api_key"}):
            client = LlvmClient()
            self.assertEqual(client.api_key, "env_api_key")
    
    def test_list_models(self):
        """Test listing available models."""
        response = self.client.list_models()
        self.assertIsInstance(response, dict)
        self.assertIn("models", response)
        self.assertIsInstance(response["models"], list)
    
    def test_get_model_info(self):
        """Test retrieving model information."""
        model_id = "test-model"
        response = self.client.get_model_info(model_id)
        self.assertIsInstance(response, dict)
        self.assertIn("model_id", response)
        self.assertEqual(response["model_id"], model_id)
    
    def test_run_inference(self):
        """Test running inference with a model."""
        model_id = "test-model"
        inputs = "Test input data"
        response = self.client.run_inference(model_id, inputs)
        self.assertIsInstance(response, dict)
        self.assertIn("model_id", response)
        self.assertIn("outputs", response)
    
    def test_concurrent_requests(self):
        """Test handling concurrent requests."""
        num_requests = 5
        
        def make_request(i):
            return self.client.run_inference("test-model", f"Input {i}")
        
        with ThreadPoolExecutor(max_workers=num_requests) as executor:
            results = list(executor.map(make_request, range(num_requests)))
        
        self.assertEqual(len(results), num_requests)
        for i, result in enumerate(results):
            self.assertIn("outputs", result)
    
    def test_retry_mechanism(self):
        """Test retry mechanism for failed requests."""
        # Mock a server error
        original_run_inference = self.client.run_inference
        fail_count = [0]
        
        def mock_run_inference(model_id, inputs, **kwargs):
            fail_count[0] += 1
            if fail_count[0] <= 2:  # Fail twice then succeed
                raise Exception("Simulated server error")
            return original_run_inference(model_id, inputs, **kwargs)
        
        with mock.patch.object(self.client, 'run_inference', side_effect=mock_run_inference):
            try:
                # This should succeed after retries
                result = self.client._with_backoff(
                    lambda: self.client.run_inference("test-model", "test input")
                )
                self.assertIsInstance(result, dict)
                self.assertIn("outputs", result)
                self.assertEqual(fail_count[0], 3)  # 2 failures + 1 success
            except Exception as e:
                if not self.using_real_client:
                    self.fail(f"Retry mechanism failed: {e}")
                else:
                    # Skip for real client as we can't mock its methods reliably
                    logger.warning("Skipping retry test with real client")

    def test_error_handling(self):
        """Test error handling for invalid requests."""
        # Test with invalid model ID
        with self.assertRaises(Exception):
            with mock.patch.object(self.client, 'get_model_info', side_effect=Exception("Model not found")):
                self.client.get_model_info("invalid-model")
                
    def test_api_key_handling(self):
        """Test API key handling in requests."""
        # Test setting a new API key
        new_key = "new_test_key"
        self.client.set_api_key(new_key)
        self.assertEqual(self.client.api_key, new_key)
        
        # Verify it's used in requests
        with mock.patch.object(self.client, '_make_request') as mock_make_request:
            try:
                self.client.list_models()
                # Check if API key was used in headers (mocked client only)
                if not self.using_real_client:
                    mock_make_request.assert_called()
                    args, kwargs = mock_make_request.call_args
                    self.assertIn("headers", kwargs)
                    self.assertIn("Authorization", kwargs["headers"])
                    self.assertIn(new_key, kwargs["headers"]["Authorization"])
            except Exception:
                if not self.using_real_client:
                    raise
    
    def test_queue_system(self):
        """Test the request queue system."""
        if not hasattr(self.client, 'request_queue'):
            logger.warning("Skipping queue test - client doesn't have queue attribute")
            return
            
        # Test queue size configuration
        self.client.max_concurrent_requests = 2
        
        # Simulate concurrent requests that take time
        def slow_request(i):
            return self.client._with_queue(
                lambda: (time.sleep(0.5), self.client.run_inference("test-model", f"Input {i}"))[1]
            )
        
        start_time = time.time()
        with ThreadPoolExecutor(max_workers=4) as executor:
            results = list(executor.map(slow_request, range(4)))
        end_time = time.time()
        
        # Verify results
        self.assertEqual(len(results), 4)
        
        # Check if it took enough time for queue processing
        # (4 requests with 2 concurrency and 0.5s sleep should take ~1.0s)
        if not self.using_real_client:
            self.assertGreaterEqual(end_time - start_time, 1.0)
    
    def test_queue_and_retry_integration(self):
        """Test integration of queue system with retry mechanism."""
        if not hasattr(self.client, 'request_queue') or not hasattr(self.client, 'max_retries'):
            logger.warning("Skipping queue+retry test - missing required attributes")
            return
            
        # Mock a sometimes-failing function
        fail_rate = 0.5
        
        def flaky_function(i):
            if random.random() < fail_rate:
                raise Exception(f"Simulated random failure {i}")
            return f"Success {i}"
        
        # Wrap with queue and backoff
        def process_with_queue_and_backoff(i):
            return self.client._with_queue(
                lambda: self.client._with_backoff(
                    lambda: flaky_function(i)
                )
            )
            
        # Run concurrent requests
        num_requests = 5
        with ThreadPoolExecutor(max_workers=num_requests) as executor:
            results = list(executor.map(process_with_queue_and_backoff, range(num_requests)))
            
        # All should eventually succeed
        self.assertEqual(len(results), num_requests)
        for i, result in enumerate(results):
            self.assertEqual(result, f"Success {i}")


if __name__ == "__main__":
    unittest.main()
