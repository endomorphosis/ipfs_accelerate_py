import os
import sys
import json
import time
import argparse
import datetime
import unittest
from unittest.mock import MagicMock, patch

sys.path.append(os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), 'ipfs_accelerate_py'))

# Import the test class
from apis.test_ollama import test_ollama

# Import for performance testing
import numpy as np
import requests

class TestOllama(unittest.TestCase):
    """Unified test suite for Ollama API"""
    
    def setUp(self):
        """Set up test environment"""
        self.metadata = {
            "ollama_api_url": os.environ.get("OLLAMA_API_URL", "http://localhost:11434/api"),
            "ollama_model": os.environ.get("OLLAMA_MODEL", "llama2"),
            "timeout": int(os.environ.get("OLLAMA_TIMEOUT", "30"))
        }
        self.resources = {}
        
        # Standard test prompts for consistency
        self.test_prompts = [
            "The quick brown fox jumps over the lazy dog.",
            "Explain the concept of machine learning in simple terms.",
            "Write a short poem about nature.",
            "Translate 'hello world' to French.",
            "What is the capital of France?"
        ]
    
    def test_standard_api(self):
        """Test the standard Ollama API functionality"""
        tester = test_ollama(self.resources, self.metadata)
        results = tester.test()
        
        # Verify critical tests passed
        self.assertIn("endpoint_handler", results)
        self.assertIn("test_endpoint", results)
        self.assertIn("post_request", results)
        
        # Save test results
        self._save_test_results(results, "ollama_test_results.json")
    
    def test_performance(self):
        """Test the performance of the Ollama API"""
        # Skip if performance tests disabled
        if os.environ.get("SKIP_PERFORMANCE_TESTS", "").lower() in ("true", "1", "yes"):
            self.skipTest("Performance tests disabled by environment variable")
        
        performance_results = {}
        
        # Create mock data for generation and embedding
        with patch.object(requests, 'post') as mock_post:
            # For generation
            gen_mock_response = MagicMock()
            gen_mock_response.status_code = 200
            gen_mock_response.json.return_value = {
                "model": self.metadata.get("ollama_model", "llama2"),
                "response": "This is a test response from Ollama API",
                "done": True
            }
            
            # For embeddings
            embed_mock_response = MagicMock()
            embed_mock_response.status_code = 200
            embed_mock_response.json.return_value = {
                "embedding": [0.1, 0.2, 0.3] * 100  # 300-dim vector
            }
            
            # Initialize test object
            tester = test_ollama(self.resources, self.metadata)
            
            # Set up endpoint URL
            endpoint_url = f"{self.metadata.get('ollama_api_url', 'http://localhost:11434/api')}/generate"
            embeddings_url = f"{self.metadata.get('ollama_api_url', 'http://localhost:11434/api')}/embeddings"
            
            # Test single generation performance
            mock_post.return_value = gen_mock_response
            
            start_time = time.time()
            for _ in range(5):  # Run 5 iterations for more stable measurement
                data = {
                    "model": self.metadata.get("ollama_model", "llama2"),
                    "prompt": self.test_prompts[0],
                    "stream": False
                }
                tester.ollama.make_post_request_ollama(endpoint_url, data)
            single_gen_time = (time.time() - start_time) / 5  # Average time
            performance_results["single_generation_time"] = f"{single_gen_time:.4f}s"
            
            # Test chat completion performance
            start_time = time.time()
            for _ in range(5):  # Run 5 iterations
                messages = [{"role": "user", "content": self.test_prompts[0]}]
                if hasattr(tester.ollama, 'chat'):
                    tester.ollama.chat(
                        self.metadata.get("ollama_model", "llama2"),
                        messages
                    )
            chat_time = (time.time() - start_time) / 5  # Average time
            performance_results["chat_completion_time"] = f"{chat_time:.4f}s"
            
            # Test embedding performance
            mock_post.return_value = embed_mock_response
            
            if hasattr(tester.ollama, 'generate_embeddings'):
                start_time = time.time()
                for _ in range(5):  # Run 5 iterations
                    tester.ollama.generate_embeddings(
                        self.metadata.get("ollama_model", "llama2"),
                        self.test_prompts[0]
                    )
                embed_time = (time.time() - start_time) / 5  # Average time
                performance_results["embedding_time"] = f"{embed_time:.4f}s"
                
                # Test batch embedding if supported
                if hasattr(tester.ollama, 'batch_embeddings'):
                    start_time = time.time()
                    for _ in range(3):  # Run 3 iterations with batches
                        tester.ollama.batch_embeddings(
                            self.metadata.get("ollama_model", "llama2"),
                            self.test_prompts
                        )
                    batch_time = (time.time() - start_time) / 3  # Average time
                    performance_results["batch_embedding_time"] = f"{batch_time:.4f}s"
                    
                    # Calculate throughput
                    sentences_per_second = len(self.test_prompts) / batch_time
                    performance_results["sentences_per_second"] = f"{sentences_per_second:.2f}"
                    
                    # Speedup factor
                    speedup = (embed_time * len(self.test_prompts)) / batch_time
                    performance_results["batch_speedup_factor"] = f"{speedup:.2f}x"
            
        # Save performance results
        performance_file = os.path.join(
            os.path.dirname(os.path.abspath(__file__)), 
            'collected_results', 
            f'ollama_performance_{datetime.datetime.now().strftime("%Y%m%d_%H%M%S")}.json'
        )
        os.makedirs(os.path.dirname(performance_file), exist_ok=True)
        with open(performance_file, 'w') as f:
            json.dump(performance_results, f, indent=2)
        
        return performance_results
    
    def test_real_connection(self):
        """Test actual connection to Ollama server if available"""
        # Skip if real connection tests disabled
        if os.environ.get("SKIP_REAL_TESTS", "").lower() in ("true", "1", "yes"):
            self.skipTest("Real connection tests disabled by environment variable")
        
        connection_results = {}
        
        # Try to connect to the Ollama server
        try:
            # Check if Ollama server is running
            api_url = self.metadata.get("ollama_api_url", "http://localhost:11434/api")
            response = requests.get(f"{api_url}/tags")
            
            if response.status_code == 200:
                connection_results["server_connection"] = "Success"
                models_data = response.json()
                
                # Check if we have the requested model
                model_found = False
                model_name = self.metadata.get("ollama_model", "llama2")
                
                if "models" in models_data:
                    for model in models_data["models"]:
                        if model.get("name") == model_name or model.get("name").startswith(f"{model_name}:"):
                            model_found = True
                            connection_results["model_available"] = "Success"
                            connection_results["model_info"] = model
                            break
                
                if not model_found:
                    connection_results["model_available"] = f"Failed - Model {model_name} not found"
                    
                # Try a simple generation request
                if model_found:
                    try:
                        gen_response = requests.post(
                            f"{api_url}/generate", 
                            json={
                                "model": model_name,
                                "prompt": "Hello, world!",
                                "stream": False
                            },
                            timeout=self.metadata.get("timeout", 30)
                        )
                        
                        if gen_response.status_code == 200:
                            connection_results["generation_test"] = "Success"
                            generation_data = gen_response.json()
                            connection_results["generation_response"] = generation_data.get("response", "")[:100] + "..." if len(generation_data.get("response", "")) > 100 else generation_data.get("response", "")
                        else:
                            connection_results["generation_test"] = f"Failed with status {gen_response.status_code}"
                    except Exception as e:
                        connection_results["generation_test"] = f"Error: {str(e)}"
            else:
                connection_results["server_connection"] = f"Failed with status {response.status_code}"
        
        except requests.ConnectionError:
            connection_results["server_connection"] = "Failed - Could not connect to Ollama server"
        except Exception as e:
            connection_results["server_connection"] = f"Error: {str(e)}"
        
        # Save connection results
        connection_file = os.path.join(
            os.path.dirname(os.path.abspath(__file__)), 
            'collected_results', 
            f'ollama_connection_{datetime.datetime.now().strftime("%Y%m%d_%H%M%S")}.json'
        )
        os.makedirs(os.path.dirname(connection_file), exist_ok=True)
        with open(connection_file, 'w') as f:
            json.dump(connection_results, f, indent=2)
        
        return connection_results
    
    def _save_test_results(self, results, filename):
        """Save test results to file"""
        # Create directories if they don't exist
        base_dir = os.path.dirname(os.path.abspath(__file__))
        collected_dir = os.path.join(base_dir, 'collected_results')
        os.makedirs(collected_dir, exist_ok=True)
        
        # Save results
        results_file = os.path.join(collected_dir, filename)
        with open(results_file, 'w') as f:
            json.dump(results, f, indent=2)

def run_tests():
    """Run all tests or selected tests based on command line arguments"""
    parser = argparse.ArgumentParser(description='Test Ollama API')
    parser.add_argument('--standard', action='store_true', help='Run standard API tests only')
    parser.add_argument('--performance', action='store_true', help='Run performance tests')
    parser.add_argument('--real', action='store_true', help='Run real connection tests')
    parser.add_argument('--all', action='store_true', help='Run all tests (standard, performance, real)')
    parser.add_argument('--model', type=str, default='llama2', help='Model to use for testing')
    parser.add_argument('--api-url', type=str, help='URL for Ollama API')
    parser.add_argument('--timeout', type=int, default=30, help='Timeout in seconds for API requests')
    
    args = parser.parse_args()
    
    # Set environment variables from arguments
    if args.model:
        os.environ["OLLAMA_MODEL"] = args.model
    if args.api_url:
        os.environ["OLLAMA_API_URL"] = args.api_url
    if args.timeout:
        os.environ["OLLAMA_TIMEOUT"] = str(args.timeout)
    
    # Create test suite
    suite = unittest.TestSuite()
    
    # Add standard API tests
    if args.standard or args.all or (not args.standard and not args.performance and not args.real):
        suite.addTest(TestOllama('test_standard_api'))
    
    # Add performance tests
    if args.performance or args.all:
        suite.addTest(TestOllama('test_performance'))
    
    # Add real connection tests
    if args.real or args.all:
        suite.addTest(TestOllama('test_real_connection'))
    
    # Run tests
    runner = unittest.TextTestRunner(verbosity=2)
    result = runner.run(suite)
    
    # Save summary report
    summary = {
        "timestamp": datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "model": os.environ.get("OLLAMA_MODEL", "llama2"),
        "tests_run": len(result.failures) + len(result.errors) + result.testsRun - len(result.skipped),
        "success": result.wasSuccessful(),
        "failures": len(result.failures),
        "errors": len(result.errors),
        "skipped": len(result.skipped)
    }
    
    # Create summary file with timestamp
    summary_file = os.path.join(
        os.path.dirname(os.path.abspath(__file__)),
        'collected_results',
        f'ollama_summary_{datetime.datetime.now().strftime("%Y%m%d_%H%M%S")}.json'
    )
    os.makedirs(os.path.dirname(summary_file), exist_ok=True)
    with open(summary_file, 'w') as f:
        json.dump(summary, f, indent=2)
    
    return 0 if result.wasSuccessful() else 1

if __name__ == "__main__":
    sys.exit(run_tests())