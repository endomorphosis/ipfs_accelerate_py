#!/usr/bin/env python3
"""
Unified Test Runner for IPFS Accelerate Python

This script runs test files generated by the unified test generator:
1. Model tests - For testing Hugging Face and other model implementations
2. API backend tests - For testing API clients like OpenAI, Claude, Groq, etc.

Features:
- Parallel test execution for faster results
- Comprehensive reporting of test results
- Support for different test categories
- Filtering by implementation type
- Performance metrics collection
"""

import os
import sys
import json
import glob
import time
import argparse
import logging
import datetime
import importlib.util
import traceback
import concurrent.futures
from pathlib import Path
from typing import Dict, List, Any, Tuple, Optional, Set, Union

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler("test_run.log")
    ]
)
logger = logging.getLogger("unified_test_runner")

# Constants for paths
CURRENT_DIR = Path(os.path.dirname(os.path.abspath(__file__)))
PARENT_DIR = CURRENT_DIR.parent
SKILLS_DIR = CURRENT_DIR / "skills"
APIS_DIR = CURRENT_DIR / "apis"
RESULTS_DIR = CURRENT_DIR / "test_results"

# Ensure results directory exists
RESULTS_DIR.mkdir(exist_ok=True, parents=True)

def get_test_files(directory: Path, prefix: str = "test_", suffix: str = ".py") -> List[Path]:
    """
    Get all test files in a directory
    
    Args:
        directory: Directory to search
        prefix: Prefix for test files
        suffix: Suffix for test files
        
    Returns:
        List of test file paths
    """
    if not directory.exists():
        return []
    
    return sorted(directory.glob(f"{prefix}*{suffix}"))

def normalize_name(file_path: Path, prefix: str = "test_", suffix: str = ".py") -> str:
    """
    Extract normalized name from test file path
    
    Args:
        file_path: Path to test file
        prefix: Prefix to remove
        suffix: Suffix to remove
        
    Returns:
        Normalized name
    """
    name = file_path.name
    if name.startswith(prefix):
        name = name[len(prefix):]
    if name.endswith(suffix):
        name = name[:-len(suffix)]
    return name

def load_module_from_file(file_path: Path) -> Any:
    """
    Dynamically load a Python module from file path
    
    Args:
        file_path: Path to Python file
        
    Returns:
        Loaded module or None if failed
    """
    try:
        spec = importlib.util.spec_from_file_location(file_path.stem, file_path)
        if spec is None or spec.loader is None:
            logger.error(f"Failed to create module spec for {file_path}")
            return None
            
        module = importlib.util.module_from_spec(spec)
        sys.modules[file_path.stem] = module
        spec.loader.exec_module(module)
        return module
    except Exception as e:
        logger.error(f"Error loading module from {file_path}: {e}")
        return None

def run_model_test(file_path: Path) -> Dict[str, Any]:
    """
    Run a model test from file path
    
    Args:
        file_path: Path to test file
        
    Returns:
        Test results
    """
    model_name = normalize_name(file_path, prefix="test_hf_")
    logger.info(f"Running test for model: {model_name}")
    
    start_time = time.time()
    
    try:
        # Load module
        module = load_module_from_file(file_path)
        if not module:
            return {
                "model": model_name,
                "status": "failed",
                "error": "Failed to load module",
                "elapsed_time": time.time() - start_time
            }
        
        # Find test class
        test_class_name = f"test_hf_{model_name}"
        test_class = getattr(module, test_class_name, None)
        
        if not test_class:
            return {
                "model": model_name,
                "status": "failed",
                "error": f"Test class {test_class_name} not found",
                "elapsed_time": time.time() - start_time
            }
        
        # Initialize and run test
        test_instance = test_class()
        test_results = test_instance.__test__()
        
        # Extract implementation status
        status = "unknown"
        implementation_type = "unknown"
        
        if "status" in test_results:
            if "cpu_handler" in test_results["status"]:
                if "REAL" in test_results["status"]["cpu_handler"]:
                    implementation_type = "REAL"
                elif "MOCK" in test_results["status"]["cpu_handler"]:
                    implementation_type = "MOCK"
            
            if "test_error" in test_results["status"]:
                status = "error"
            else:
                status = "success"
        
        return {
            "model": model_name,
            "status": status,
            "implementation_type": implementation_type,
            "elapsed_time": time.time() - start_time,
            "results": test_results
        }
    except Exception as e:
        error_str = str(e)
        logger.error(f"Error running test for model {model_name}: {error_str}")
        return {
            "model": model_name,
            "status": "error",
            "error": error_str,
            "traceback": traceback.format_exc(),
            "elapsed_time": time.time() - start_time
        }

def run_api_test(file_path: Path) -> Dict[str, Any]:
    """
    Run an API test from file path
    
    Args:
        file_path: Path to test file
        
    Returns:
        Test results
    """
    api_name = normalize_name(file_path, prefix="test_")
    logger.info(f"Running test for API: {api_name}")
    
    start_time = time.time()
    
    try:
        import unittest
        from io import StringIO
        import contextlib
        
        # Load module
        module = load_module_from_file(file_path)
        if not module:
            return {
                "api": api_name,
                "status": "failed",
                "error": "Failed to load module",
                "elapsed_time": time.time() - start_time
            }
        
        # Capture test output
        output = StringIO()
        with contextlib.redirect_stdout(output):
            test_suite = unittest.TestLoader().loadTestsFromModule(module)
            test_result = unittest.TextTestRunner(verbosity=2).run(test_suite)
        
        test_output = output.getvalue()
        
        # Determine test status
        status = "success"
        if test_result.failures or test_result.errors:
            status = "failed"
        
        # Count tests
        tests_run = test_result.testsRun
        failures = len(test_result.failures)
        errors = len(test_result.errors)
        skipped = len(test_result.skipped)
        
        return {
            "api": api_name,
            "status": status,
            "tests_run": tests_run,
            "failures": failures,
            "errors": errors,
            "skipped": skipped,
            "elapsed_time": time.time() - start_time,
            "output": test_output
        }
    except Exception as e:
        error_str = str(e)
        logger.error(f"Error running test for API {api_name}: {error_str}")
        return {
            "api": api_name,
            "status": "error",
            "error": error_str,
            "traceback": traceback.format_exc(),
            "elapsed_time": time.time() - start_time
        }

def run_model_tests(test_files: List[Path], max_workers: int = 4) -> Dict[str, Any]:
    """
    Run model tests in parallel
    
    Args:
        test_files: List of test file paths
        max_workers: Maximum number of parallel workers
        
    Returns:
        Combined test results
    """
    start_time = time.time()
    logger.info(f"Running {len(test_files)} model tests with {max_workers} workers")
    
    results = []
    
    with concurrent.futures.ProcessPoolExecutor(max_workers=max_workers) as executor:
        future_to_file = {executor.submit(run_model_test, file_path): file_path for file_path in test_files}
        
        for future in concurrent.futures.as_completed(future_to_file):
            file_path = future_to_file[future]
            try:
                result = future.result()
                results.append(result)
                
                # Log progress
                completed = len(results)
                total = len(test_files)
                progress = completed / total * 100
                logger.info(f"Progress: {completed}/{total} ({progress:.1f}%)")
            except Exception as e:
                model_name = normalize_name(file_path, prefix="test_hf_")
                logger.error(f"Error processing result for {model_name}: {e}")
                results.append({
                    "model": model_name,
                    "status": "error",
                    "error": str(e),
                    "traceback": traceback.format_exc(),
                    "elapsed_time": 0
                })
    
    # Sort results by model name
    results.sort(key=lambda x: x.get("model", ""))
    
    # Generate summary
    total_time = time.time() - start_time
    success_count = sum(1 for r in results if r["status"] == "success")
    error_count = sum(1 for r in results if r["status"] in ["error", "failed"])
    real_count = sum(1 for r in results if r.get("implementation_type") == "REAL")
    mock_count = sum(1 for r in results if r.get("implementation_type") == "MOCK")
    
    summary = {
        "total_tests": len(results),
        "success_count": success_count,
        "error_count": error_count,
        "real_implementations": real_count,
        "mock_implementations": mock_count,
        "total_time": total_time
    }
    
    return {
        "summary": summary,
        "results": results,
        "timestamp": datetime.datetime.now().isoformat()
    }

def run_api_tests(test_files: List[Path], max_workers: int = 4) -> Dict[str, Any]:
    """
    Run API tests in parallel
    
    Args:
        test_files: List of test file paths
        max_workers: Maximum number of parallel workers
        
    Returns:
        Combined test results
    """
    start_time = time.time()
    logger.info(f"Running {len(test_files)} API tests with {max_workers} workers")
    
    results = []
    
    with concurrent.futures.ProcessPoolExecutor(max_workers=max_workers) as executor:
        future_to_file = {executor.submit(run_api_test, file_path): file_path for file_path in test_files}
        
        for future in concurrent.futures.as_completed(future_to_file):
            file_path = future_to_file[future]
            try:
                result = future.result()
                results.append(result)
                
                # Log progress
                completed = len(results)
                total = len(test_files)
                progress = completed / total * 100
                logger.info(f"Progress: {completed}/{total} ({progress:.1f}%)")
            except Exception as e:
                api_name = normalize_name(file_path, prefix="test_")
                logger.error(f"Error processing result for {api_name}: {e}")
                results.append({
                    "api": api_name,
                    "status": "error",
                    "error": str(e),
                    "traceback": traceback.format_exc(),
                    "elapsed_time": 0
                })
    
    # Sort results by API name
    results.sort(key=lambda x: x.get("api", ""))
    
    # Generate summary
    total_time = time.time() - start_time
    success_count = sum(1 for r in results if r["status"] == "success")
    error_count = sum(1 for r in results if r["status"] in ["error", "failed"])
    total_tests = sum(r.get("tests_run", 0) for r in results)
    total_failures = sum(r.get("failures", 0) for r in results)
    total_errors = sum(r.get("errors", 0) for r in results)
    total_skipped = sum(r.get("skipped", 0) for r in results)
    
    summary = {
        "total_apis": len(results),
        "success_count": success_count,
        "error_count": error_count,
        "total_tests": total_tests,
        "total_failures": total_failures,
        "total_errors": total_errors,
        "total_skipped": total_skipped,
        "total_time": total_time
    }
    
    return {
        "summary": summary,
        "results": results,
        "timestamp": datetime.datetime.now().isoformat()
    }

def save_results(results: Dict[str, Any], file_path: Path):
    """
    Save test results to file
    
    Args:
        results: Test results to save
        file_path: Path to save results
    """
    try:
        with open(file_path, 'w') as f:
            json.dump(results, f, indent=2)
        logger.info(f"Results saved to {file_path}")
    except Exception as e:
        logger.error(f"Error saving results to {file_path}: {e}")

def generate_report(model_results: Optional[Dict[str, Any]] = None, 
                   api_results: Optional[Dict[str, Any]] = None) -> str:
    """
    Generate a human-readable test report
    
    Args:
        model_results: Model test results
        api_results: API test results
        
    Returns:
        Report as string
    """
    report = []
    timestamp = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    
    report.append(f"# IPFS Accelerate Python Test Report")
    report.append(f"Generated: {timestamp}\n")
    
    # Model test report
    if model_results:
        summary = model_results["summary"]
        report.append(f"## Model Tests Summary")
        report.append(f"- Total models tested: {summary['total_tests']}")
        report.append(f"- Successful tests: {summary['success_count']} ({summary['success_count'] / summary['total_tests'] * 100:.1f}%)")
        report.append(f"- Failed tests: {summary['error_count']} ({summary['error_count'] / summary['total_tests'] * 100:.1f}%)")
        report.append(f"- Real implementations: {summary['real_implementations']} ({summary['real_implementations'] / summary['total_tests'] * 100:.1f}%)")
        report.append(f"- Mock implementations: {summary['mock_implementations']} ({summary['mock_implementations'] / summary['total_tests'] * 100:.1f}%)")
        report.append(f"- Total time: {summary['total_time']:.2f} seconds\n")
        
        # Implementation status by model
        report.append(f"### Model Implementation Status")
        report.append(f"| Model | Status | Implementation | Time (s) |")
        report.append(f"|-------|--------|----------------|----------|")
        
        for result in model_results["results"]:
            model = result.get("model", "Unknown")
            status = result.get("status", "Unknown")
            impl_type = result.get("implementation_type", "Unknown")
            time_taken = result.get("elapsed_time", 0)
            
            # Format status with emoji
            if status == "success":
                status_str = "✅ Success"
            elif status == "error" or status == "failed":
                status_str = "❌ Failed"
            else:
                status_str = "⚠️ Unknown"
            
            # Format implementation type
            if impl_type == "REAL":
                impl_str = "🟢 REAL"
            elif impl_type == "MOCK":
                impl_str = "🟡 MOCK"
            else:
                impl_str = "⚪ Unknown"
            
            report.append(f"| {model} | {status_str} | {impl_str} | {time_taken:.2f} |")
        
        report.append("")
    
    # API test report
    if api_results:
        summary = api_results["summary"]
        report.append(f"## API Tests Summary")
        report.append(f"- Total APIs tested: {summary['total_apis']}")
        report.append(f"- Successful APIs: {summary['success_count']} ({summary['success_count'] / summary['total_apis'] * 100:.1f}%)")
        report.append(f"- Failed APIs: {summary['error_count']} ({summary['error_count'] / summary['total_apis'] * 100:.1f}%)")
        report.append(f"- Total individual tests: {summary['total_tests']}")
        report.append(f"- Test failures: {summary['total_failures']}")
        report.append(f"- Test errors: {summary['total_errors']}")
        report.append(f"- Skipped tests: {summary['total_skipped']}")
        report.append(f"- Total time: {summary['total_time']:.2f} seconds\n")
        
        # API test results
        report.append(f"### API Test Results")
        report.append(f"| API | Status | Tests | Failures | Errors | Skipped | Time (s) |")
        report.append(f"|-----|--------|-------|----------|--------|---------|----------|")
        
        for result in api_results["results"]:
            api = result.get("api", "Unknown")
            status = result.get("status", "Unknown")
            tests = result.get("tests_run", 0)
            failures = result.get("failures", 0)
            errors = result.get("errors", 0)
            skipped = result.get("skipped", 0)
            time_taken = result.get("elapsed_time", 0)
            
            # Format status with emoji
            if status == "success":
                status_str = "✅ Success"
            elif status == "error" or status == "failed":
                status_str = "❌ Failed"
            else:
                status_str = "⚠️ Unknown"
            
            report.append(f"| {api} | {status_str} | {tests} | {failures} | {errors} | {skipped} | {time_taken:.2f} |")
        
        report.append("")
    
    return "\n".join(report)

def parse_args():
    """Parse command line arguments"""
    parser = argparse.ArgumentParser(description="Unified test runner for IPFS Accelerate Python")
    
    # General options
    parser.add_argument(
        "--type", type=str, 
        choices=["model", "api", "all"],
        default="all",
        help="Type of tests to run"
    )
    parser.add_argument(
        "--workers", type=int, default=4,
        help="Maximum number of parallel workers"
    )
    parser.add_argument(
        "--verbose", action="store_true",
        help="Enable verbose output"
    )
    
    # Model test options
    model_group = parser.add_argument_group("Model test options")
    model_group.add_argument(
        "--models", type=str, nargs="+",
        help="List of specific models to test"
    )
    model_group.add_argument(
        "--category", type=str, 
        choices=["language", "vision", "audio", "multimodal", "specialized"],
        help="Category of models to test"
    )
    model_group.add_argument(
        "--impl-type", type=str,
        choices=["real", "mock", "all"],
        default="all",
        help="Implementation type to test"
    )
    model_group.add_argument(
        "--limit", type=int,
        help="Maximum number of model tests to run"
    )
    
    # API test options
    api_group = parser.add_argument_group("API test options")
    api_group.add_argument(
        "--apis", type=str, nargs="+",
        help="List of specific APIs to test"
    )
    
    # Output options
    parser.add_argument(
        "--output-dir", type=str,
        help="Custom output directory for test results"
    )
    parser.add_argument(
        "--report", type=str,
        help="Path to save test report"
    )
    
    return parser.parse_args()

def main():
    """Main function"""
    # Parse command line arguments
    args = parse_args()
    
    # Set logging level
    if args.verbose:
        logger.setLevel(logging.DEBUG)
    else:
        logger.setLevel(logging.INFO)
    
    # Set output directory
    output_dir = Path(args.output_dir) if args.output_dir else RESULTS_DIR
    output_dir.mkdir(exist_ok=True, parents=True)
    
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # Initialize results
    model_results = None
    api_results = None
    
    # Run model tests if requested
    if args.type in ["model", "all"]:
        model_test_files = get_test_files(SKILLS_DIR, prefix="test_hf_")
        
        if not model_test_files:
            logger.warning("No model test files found")
        else:
            # Filter model tests if needed
            if args.models:
                model_names = [m.lower() for m in args.models]
                model_test_files = [f for f in model_test_files 
                                 if normalize_name(f, prefix="test_hf_").lower() in model_names]
            
            # Apply limit if specified
            if args.limit and args.limit > 0:
                model_test_files = model_test_files[:args.limit]
            
            # Run the tests
            if model_test_files:
                logger.info(f"Running {len(model_test_files)} model tests")
                model_results = run_model_tests(model_test_files, max_workers=args.workers)
                
                # Save results
                model_results_path = output_dir / f"model_test_results_{timestamp}.json"
                save_results(model_results, model_results_path)
    
    # Run API tests if requested
    if args.type in ["api", "all"]:
        api_test_files = get_test_files(APIS_DIR, prefix="test_")
        
        if not api_test_files:
            logger.warning("No API test files found")
        else:
            # Filter API tests if needed
            if args.apis:
                api_names = [a.lower() for a in args.apis]
                api_test_files = [f for f in api_test_files 
                               if normalize_name(f, prefix="test_").lower() in api_names]
            
            # Run the tests
            if api_test_files:
                logger.info(f"Running {len(api_test_files)} API tests")
                api_results = run_api_tests(api_test_files, max_workers=args.workers)
                
                # Save results
                api_results_path = output_dir / f"api_test_results_{timestamp}.json"
                save_results(api_results, api_results_path)
    
    # Generate and save report
    if model_results or api_results:
        report = generate_report(model_results, api_results)
        
        # Save report
        report_path = args.report if args.report else output_dir / f"test_report_{timestamp}.md"
        try:
            with open(report_path, 'w') as f:
                f.write(report)
            logger.info(f"Report saved to {report_path}")
            
            # Print report summary
            print("\n" + report)
        except Exception as e:
            logger.error(f"Error saving report to {report_path}: {e}")
            # Print report summary anyway
            print("\n" + report)

if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        logger.info("Operation canceled by user")
        sys.exit(1)
    except Exception as e:
        logger.error(f"Unexpected error: {e}")
        traceback.print_exc()
        sys.exit(1)