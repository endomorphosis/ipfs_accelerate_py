#!/usr/bin/env python3
"""
Model export capability module for the enhanced model registry.
Provides functionality to export models to ONNX, WebNN, and other formats
with hardware-specific optimizations.
"""

import os
import sys
import json
import logging
import torch
import numpy as np
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Union, Any, Set
from dataclasses import dataclass, field
import importlib.util

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler(sys.stdout)]
)
logger = logging.getLogger("model_export")

# Check for optional dependencies
HAS_ONNX = importlib.util.find_spec("onnx") is not None
HAS_ONNXRUNTIME = importlib.util.find_spec("onnxruntime") is not None
HAS_WEBNN = importlib.util.find_spec("webnn") is not None or importlib.util.find_spec("webnn_js") is not None

# Import when available
if HAS_ONNX:
    import onnx
if HAS_ONNXRUNTIME:
    import onnxruntime as ort

# Add path to local modules
sys.path.append(os.path.dirname(os.path.abspath(__file__)))
try:
    from auto_hardware_detection import detect_all_hardware, determine_precision_for_all_hardware
except ImportError:
    logger.warning("Could not import auto_hardware_detection module. Hardware optimization will be limited.")


@dataclass
class InputOutputSpec:
    """Specification for model inputs and outputs"""
    name: str
    shape: List[Union[int, str]]  # Can include dynamic dimensions as strings like "batch_size"
    dtype: str
    is_required: bool = True
    is_dynamic: bool = False
    min_shape: Optional[List[int]] = None
    max_shape: Optional[List[int]] = None
    typical_shape: Optional[List[int]] = None
    description: Optional[str] = None


@dataclass
class ExportConfig:
    """Configuration for model export"""
    format: str  # "onnx", "webnn", etc.
    opset_version: int = 14  # For ONNX models
    dynamic_axes: Optional[Dict[str, Dict[int, str]]] = None
    optimization_level: int = 99  # Higher means more aggressive optimization
    target_hardware: Optional[str] = None
    precision: Optional[str] = None
    quantize: bool = False
    simplify: bool = True
    constant_folding: bool = True
    preserve_metadata: bool = True
    export_params: bool = True
    verbose: bool = False
    input_names: Optional[List[str]] = None
    output_names: Optional[List[str]] = None
    additional_options: Dict[str, Any] = field(default_factory=dict)


@dataclass
class WebNNBackendInfo:
    """Information specific to WebNN backend implementation"""
    supported: bool = False
    preferred_backend: str = "gpu"  # 'gpu', 'cpu', or 'wasm'
    fallback_backends: List[str] = field(default_factory=lambda: ["cpu"])
    operation_support: Dict[str, bool] = field(default_factory=dict)
    requires_polyfill: bool = False
    browser_compatibility: Dict[str, bool] = field(default_factory=dict)
    supports_async: bool = True
    estimated_memory_usage_mb: Optional[float] = None
    js_dependencies: List[str] = field(default_factory=list)
    js_code_template: Optional[str] = None


@dataclass
class ModelExportCapability:
    """Describes model export capabilities"""
    model_id: str
    supported_formats: Set[str] = field(default_factory=lambda: {"onnx"})
    inputs: List[InputOutputSpec] = field(default_factory=list)
    outputs: List[InputOutputSpec] = field(default_factory=list)
    supported_opset_versions: List[int] = field(default_factory=lambda: [9, 10, 11, 12, 13, 14, 15])
    recommended_opset_version: int = 14
    hardware_compatibility: Dict[str, List[str]] = field(default_factory=dict)
    precision_compatibility: Dict[str, List[str]] = field(default_factory=dict)
    operation_limitations: List[str] = field(default_factory=list)
    export_warnings: List[str] = field(default_factory=list)
    quantization_support: Dict[str, bool] = field(default_factory=dict)
    
    # Model architecture details
    model_type: str = ""  # e.g., "bert", "t5", "vit", etc.
    model_family: str = ""  # e.g., "transformer", "cnn", etc.
    architecture_params: Dict[str, Any] = field(default_factory=dict)  # Key architecture parameters
    custom_ops: List[str] = field(default_factory=list)  # Any custom operations
    
    # Pre/post-processing information
    preprocessing_info: Dict[str, Any] = field(default_factory=dict)  # Input preprocessing requirements
    postprocessing_info: Dict[str, Any] = field(default_factory=dict)  # Output postprocessing requirements
    input_normalization: Dict[str, List[float]] = field(default_factory=dict)  # e.g., {"mean": [0.485, 0.456, 0.406], "std": [0.229, 0.224, 0.225]}
    
    # WebNN specific information
    webnn_info: WebNNBackendInfo = field(default_factory=WebNNBackendInfo)
    
    # JavaScript inference code templates
    js_inference_snippets: Dict[str, str] = field(default_factory=dict)
    
    # ONNX conversion specifics
    onnx_custom_ops_mapping: Dict[str, str] = field(default_factory=dict)
    onnx_additional_conversion_args: Dict[str, Any] = field(default_factory=dict)
    
    def is_supported_format(self, format_name: str) -> bool:
        """Check if a specific export format is supported"""
        return format_name.lower() in self.supported_formats
    
    def get_recommended_hardware(self, format_name: str) -> List[str]:
        """Get recommended hardware for a specific export format"""
        return self.hardware_compatibility.get(format_name.lower(), [])
    
    def get_supported_precisions(self, format_name: str, hardware: str) -> List[str]:
        """Get supported precision types for a format and hardware combination"""
        key = f"{format_name.lower()}_{hardware}"
        return self.precision_compatibility.get(key, [])
    
    def generate_js_inference_code(self, format_name: str = "webnn") -> str:
        """Generate JavaScript inference code for the model"""
        if format_name.lower() not in self.supported_formats:
            return f"// {format_name} is not supported for this model"
            
        if format_name.lower() == "webnn" and self.webnn_info.js_code_template:
            # Create template for substitution
            template = self.webnn_info.js_code_template
            
            # Substitute template variables
            code = template
            
            # Replace input shapes
            input_shapes = {}
            for inp in self.inputs:
                shape_str = str(inp.typical_shape if inp.typical_shape else inp.shape)
                input_shapes[inp.name] = shape_str
            
            code = code.replace("{{INPUT_SHAPES}}", json.dumps(input_shapes))
            
            # Replace preprocessing info
            code = code.replace("{{PREPROCESSING}}", json.dumps(self.preprocessing_info))
            
            # Replace model type
            code = code.replace("{{MODEL_TYPE}}", self.model_type)
            
            return code
        else:
            return self.js_inference_snippets.get(format_name.lower(), "// No template available for this format")
    
    def to_json(self) -> str:
        """Convert to JSON for storage in model registry"""
        # Create a dictionary with all the relevant fields
        export_dict = {
            "model_id": self.model_id,
            "supported_formats": list(self.supported_formats),
            "inputs": [vars(inp) for inp in self.inputs],
            "outputs": [vars(out) for out in self.outputs],
            "supported_opset_versions": self.supported_opset_versions,
            "recommended_opset_version": self.recommended_opset_version,
            "hardware_compatibility": self.hardware_compatibility,
            "precision_compatibility": self.precision_compatibility,
            "operation_limitations": self.operation_limitations,
            "model_type": self.model_type,
            "model_family": self.model_family,
            "architecture_params": self.architecture_params,
            "custom_ops": self.custom_ops,
            "preprocessing_info": self.preprocessing_info,
            "postprocessing_info": self.postprocessing_info,
            "input_normalization": self.input_normalization,
            "webnn_info": vars(self.webnn_info),
            "onnx_custom_ops_mapping": self.onnx_custom_ops_mapping,
        }
        
        return json.dumps(export_dict, indent=2)


def check_onnx_compatibility(model: torch.nn.Module, inputs: Dict[str, torch.Tensor]) -> Tuple[bool, List[str]]:
    """
    Check if a PyTorch model can be exported to ONNX
    
    Args:
        model: PyTorch model to check
        inputs: Example inputs for the model
        
    Returns:
        compatibility: Boolean indicating if model is compatible with ONNX
        issues: List of identified compatibility issues
    """
    if not HAS_ONNX:
        return False, ["ONNX package not installed"]
    
    issues = []
    
    # Check model parameters
    for name, param in model.named_parameters():
        if param.dtype not in [torch.float32, torch.float16, torch.int32, torch.int64, torch.int8]:
            issues.append(f"Parameter {name} has unsupported dtype {param.dtype}")
    
    # Try to trace the model
    try:
        with torch.no_grad():
            traced_model = torch.jit.trace(model, tuple(inputs.values()) if isinstance(inputs, dict) else inputs)
    except Exception as e:
        issues.append(f"Failed to trace model: {str(e)}")
        return False, issues
    
    # Check for unsupported operations
    graph = traced_model.graph
    for node in graph.nodes():
        if node.kind() == 'aten::upsample_bilinear2d':
            issues.append("Warning: aten::upsample_bilinear2d operation may have compatibility issues in some ONNX versions")
        elif node.kind() == 'aten::index':
            issues.append("Warning: aten::index operation has limited ONNX support")
        elif node.kind() == 'prim::PythonOp':
            issues.append("Warning: Custom Python operations are not supported in ONNX")
    
    # Basic compatibility check passed
    compatibility = len(issues) == 0 or all(issue.startswith("Warning") for issue in issues)
    
    return compatibility, issues


def check_webnn_compatibility(model: torch.nn.Module, inputs: Dict[str, torch.Tensor]) -> Tuple[bool, List[str]]:
    """
    Check if a PyTorch model can be exported for WebNN
    
    Args:
        model: PyTorch model to check
        inputs: Example inputs for the model
        
    Returns:
        compatibility: Boolean indicating if model is compatible with WebNN
        issues: List of identified compatibility issues
    """
    # WebNN compatibility is more restrictive than ONNX
    # First check ONNX compatibility as a baseline
    onnx_compatible, onnx_issues = check_onnx_compatibility(model, inputs)
    
    if not onnx_compatible:
        return False, ["WebNN requires ONNX compatibility: "] + onnx_issues
    
    issues = []
    
    # WebNN supports a smaller subset of operations than ONNX
    # Check for specific supported operations
    try:
        with torch.no_grad():
            traced_model = torch.jit.trace(model, tuple(inputs.values()) if isinstance(inputs, dict) else inputs)
            
            # Check for unsupported operations in WebNN
            graph = traced_model.graph
            for node in graph.nodes():
                if node.kind() in {
                    'aten::lstm', 'aten::gru', 'aten::rnn', 
                    'aten::custom', 'aten::scatter', 'aten::index_put',
                }:
                    issues.append(f"Operation {node.kind()} is not supported in WebNN")
                elif 'quantize' in node.kind() or 'dequantize' in node.kind():
                    issues.append(f"Quantization operation {node.kind()} may have limited WebNN support")
    
    except Exception as e:
        issues.append(f"Failed to trace model for WebNN compatibility check: {str(e)}")
        return False, issues
    
    # Check model size limitations
    model_size_mb = sum(p.numel() * p.element_size() for p in model.parameters()) / (1024 * 1024)
    if model_size_mb > 100:
        issues.append(f"Model size ({model_size_mb:.2f} MB) may be too large for efficient WebNN deployment")
    
    # Check precision compatibility
    for name, param in model.named_parameters():
        if param.dtype not in [torch.float32, torch.float16]:
            issues.append(f"Parameter {name} has dtype {param.dtype} which may not be supported in WebNN")
    
    # Basic compatibility check passed with potential warnings
    compatibility = len(issues) == 0 or all(issue.startswith("Warning") for issue in issues)
    
    return compatibility, issues


def export_to_onnx(
    model: torch.nn.Module, 
    inputs: Union[Dict[str, torch.Tensor], torch.Tensor, Tuple[torch.Tensor, ...]], 
    output_path: str,
    config: Optional[ExportConfig] = None
) -> Tuple[bool, str]:
    """
    Export PyTorch model to ONNX format
    
    Args:
        model: PyTorch model to export
        inputs: Example inputs for the model
        output_path: Path to save the ONNX model
        config: Export configuration options
        
    Returns:
        success: Boolean indicating success
        message: Message describing the result or error
    """
    if not HAS_ONNX:
        return False, "ONNX package not installed"
    
    # Create default config if not provided
    if config is None:
        config = ExportConfig(format="onnx")
    
    # Prepare model for export
    model.eval()
    
    # Prepare input names and output names
    input_names = config.input_names
    if input_names is None:
        if isinstance(inputs, dict):
            input_names = list(inputs.keys())
        else:
            input_names = [f"input_{i}" for i in range(len(inputs) if isinstance(inputs, tuple) else 1)]
    
    output_names = config.output_names
    if output_names is None:
        output_names = ["output"]
    
    # Prepare dynamic axes
    dynamic_axes = config.dynamic_axes
    
    try:
        # Convert inputs to appropriate format
        if isinstance(inputs, dict):
            input_values = tuple(inputs.values())
        else:
            input_values = inputs
        
        # Export the model
        with torch.no_grad():
            torch.onnx.export(
                model,
                input_values,
                output_path,
                export_params=config.export_params,
                opset_version=config.opset_version,
                do_constant_folding=config.constant_folding,
                input_names=input_names,
                output_names=output_names,
                dynamic_axes=dynamic_axes,
                verbose=config.verbose
            )
        
        # Verify the exported model
        if HAS_ONNX:
            try:
                onnx_model = onnx.load(output_path)
                onnx.checker.check_model(onnx_model)
            except Exception as e:
                return False, f"ONNX model verification failed: {str(e)}"
        
        # Apply optimizations if requested
        if config.optimization_level > 0 and HAS_ONNXRUNTIME:
            try:
                from onnxruntime.transformers import optimizer
                optimized_model = optimizer.optimize_model(
                    output_path,
                    model_type='bert',  # This could be dynamic based on the model
                    num_heads=12,  # This should come from model metadata
                    hidden_size=768  # This should come from model metadata
                )
                optimized_model.save_model_to_file(output_path)
            except Exception as e:
                logger.warning(f"ONNX optimization failed: {str(e)}")
        
        # Quantize if requested
        if config.quantize and HAS_ONNXRUNTIME:
            try:
                from onnxruntime.quantization import quantize_dynamic, QuantType
                quantized_output_path = output_path.replace('.onnx', '_quantized.onnx')
                quantize_dynamic(
                    output_path,
                    quantized_output_path,
                    weight_type=QuantType.QInt8
                )
                # Replace original with quantized model
                import shutil
                shutil.move(quantized_output_path, output_path)
            except Exception as e:
                logger.warning(f"ONNX quantization failed: {str(e)}")
        
        return True, f"Model successfully exported to {output_path}"
    
    except Exception as e:
        import traceback
        return False, f"Export failed: {str(e)}\n{traceback.format_exc()}"


def export_to_webnn(
    model: torch.nn.Module, 
    inputs: Union[Dict[str, torch.Tensor], torch.Tensor, Tuple[torch.Tensor, ...]], 
    output_dir: str,
    config: Optional[ExportConfig] = None
) -> Tuple[bool, str]:
    """
    Export PyTorch model to WebNN compatible format (via ONNX)
    
    Args:
        model: PyTorch model to export
        inputs: Example inputs for the model
        output_dir: Directory to save WebNN and intermediate files
        config: Export configuration options
        
    Returns:
        success: Boolean indicating success
        message: Message describing the result or error
    """
    # WebNN export uses ONNX as an intermediate format
    if not HAS_ONNX:
        return False, "ONNX package not installed for WebNN export"
    
    # Create default config if not provided
    if config is None:
        config = ExportConfig(format="webnn")
    
    # Create output directory
    os.makedirs(output_dir, exist_ok=True)
    
    # First export to ONNX as intermediate format
    onnx_path = os.path.join(output_dir, "model_intermediate.onnx")
    
    # Clone the config and modify for ONNX export
    onnx_config = ExportConfig(
        format="onnx",
        opset_version=config.opset_version,
        dynamic_axes=config.dynamic_axes,
        optimization_level=config.optimization_level,
        target_hardware=config.target_hardware,
        precision=config.precision,
        quantize=config.quantize,
        simplify=config.simplify,
        constant_folding=config.constant_folding,
        input_names=config.input_names,
        output_names=config.output_names
    )
    
    success, message = export_to_onnx(model, inputs, onnx_path, onnx_config)
    if not success:
        return False, f"WebNN export failed at ONNX intermediate stage: {message}"
    
    # Optional: Convert ONNX to specific WebNN format
    # This would depend on the specific WebNN implementation available
    if HAS_WEBNN:
        try:
            # This is a placeholder for actual WebNN conversion
            # Different WebNN implementations would have different APIs
            webnn_path = os.path.join(output_dir, "model.webnn")
            
            # Placeholder for WebNN conversion
            # import webnn
            # webnn.convert_from_onnx(onnx_path, webnn_path)
            
            # For now, we just generate a metadata file
            webnn_metadata = {
                "original_model": model.__class__.__name__,
                "intermediate_format": "ONNX",
                "intermediate_path": onnx_path,
                "opset_version": onnx_config.opset_version,
                "input_names": onnx_config.input_names,
                "output_names": onnx_config.output_names,
                "target_hardware": config.target_hardware,
                "precision": config.precision,
            }
            
            with open(os.path.join(output_dir, "webnn_metadata.json"), "w") as f:
                json.dump(webnn_metadata, f, indent=2)
            
            return True, f"Model exported for WebNN compatibility in {output_dir}"
        
        except Exception as e:
            return False, f"WebNN conversion failed: {str(e)}"
    else:
        # If WebNN package is not available, we provide instructions
        with open(os.path.join(output_dir, "webnn_instructions.txt"), "w") as f:
            f.write("WebNN Conversion Instructions:\n\n")
            f.write("1. Install WebNN tooling (see https://webmachinelearning.github.io/webnn/)\n")
            f.write("2. Use the intermediate ONNX model generated at: " + onnx_path + "\n")
            f.write("3. Convert using appropriate WebNN tooling for your target environment\n")
        
        return True, f"Intermediate ONNX model for WebNN generated at {onnx_path}. See instructions in {output_dir}/webnn_instructions.txt"


def get_model_export_capability(model_id: str, model: Optional[torch.nn.Module] = None) -> ModelExportCapability:
    """
    Get export capability information for a specific model
    
    Args:
        model_id: Identifier for the model (e.g., "bert-base-uncased")
        model: Optional PyTorch model instance
        
    Returns:
        capability: ModelExportCapability object with export information
    """
    # Initialize with defaults
    capability = ModelExportCapability(model_id=model_id)
    
    # Set supported formats
    capability.supported_formats = {"onnx"}
    if HAS_WEBNN:
        capability.supported_formats.add("webnn")
    
    # Detect hardware to determine hardware compatibility
    hardware = None
    try:
        from auto_hardware_detection import detect_all_hardware
        hardware = detect_all_hardware()
    except ImportError:
        logger.warning("Could not import auto_hardware_detection for hardware compatibility check")
    
    # Default hardware compatibility
    capability.hardware_compatibility = {
        "onnx": ["cpu", "cuda", "amd", "openvino"],
        "webnn": ["cpu", "wasm"]
    }
    
    # Determine precision compatibility
    capability.precision_compatibility = {
        "onnx_cpu": ["fp32", "int8"],
        "onnx_cuda": ["fp32", "fp16", "int8", "int4"],
        "onnx_amd": ["fp32", "fp16"],
        "onnx_openvino": ["fp32", "fp16", "int8"],
        "webnn_cpu": ["fp32", "fp16"],
        "webnn_wasm": ["fp32", "fp16"]
    }
    
    # Set quantization support
    capability.quantization_support = {
        "onnx": True,
        "webnn": False  # WebNN quantization support is limited
    }
    
    # Model-specific adjustments based on model ID
    if model_id.startswith("bert-"):
        # BERT models generally have good export support
        capability.model_type = "bert"
        capability.model_family = "transformer"
        
        # Architecture parameters
        capability.architecture_params = {
            "hidden_size": 768,
            "num_attention_heads": 12,
            "num_hidden_layers": 12,
            "intermediate_size": 3072,
            "max_position_embeddings": 512
        }
        
        # Input/output specs
        capability.inputs = [
            InputOutputSpec(
                name="input_ids",
                shape=["batch_size", "sequence_length"],
                dtype="int64",
                is_dynamic=True,
                typical_shape=[1, 128]
            ),
            InputOutputSpec(
                name="attention_mask",
                shape=["batch_size", "sequence_length"],
                dtype="int64",
                is_dynamic=True,
                typical_shape=[1, 128]
            ),
            InputOutputSpec(
                name="token_type_ids",
                shape=["batch_size", "sequence_length"],
                dtype="int64",
                is_dynamic=True,
                typical_shape=[1, 128],
                is_required=False
            )
        ]
        capability.outputs = [
            InputOutputSpec(
                name="last_hidden_state",
                shape=["batch_size", "sequence_length", "hidden_size"],
                dtype="float32",
                is_dynamic=True,
                typical_shape=[1, 128, 768]
            )
        ]
        
        # Preprocessing information
        capability.preprocessing_info = {
            "tokenizer": "BertTokenizer",
            "padding": "max_length",
            "truncation": True,
            "add_special_tokens": True,
            "return_tensors": "pt",
            "max_length": 128
        }
        
        # Postprocessing information
        capability.postprocessing_info = {
            "output_hidden_states": True,
            "output_attentions": False
        }
        
        # WebNN specific information
        capability.webnn_info = WebNNBackendInfo(
            supported=True,
            preferred_backend="gpu",
            fallback_backends=["cpu"],
            operation_support={
                "matmul": True,
                "attention": True,
                "layernorm": True,
                "gelu": True
            },
            requires_polyfill=False,
            browser_compatibility={
                "chrome": True,
                "firefox": True,
                "safari": True,
                "edge": True
            },
            estimated_memory_usage_mb=350,
            js_dependencies=[
                "onnxruntime-web@1.14.0",
                "webnn-polyfill@0.1.0"
            ],
            js_code_template="""
// WebNN inference code for {{MODEL_TYPE}} model
import * as ort from 'onnxruntime-web';
// May need WebNN polyfill based on browser support
// import * as webnn from 'webnn-polyfill';

// Model input shapes
const inputShapes = {{INPUT_SHAPES}};

// Preprocessing configuration
const preprocessingConfig = {{PREPROCESSING}};

// Initialize tokenizer - use a BertTokenizer implementation
async function initTokenizer() {
  // Here you would load your tokenizer vocab or use a JS implementation
  return new BertTokenizer({ 
    vocabFile: 'vocab.txt',
    doLowerCase: true
  });
}

// Preprocess inputs - tokenize text
async function preprocessInput(text, tokenizer) {
  const tokenized = await tokenizer.tokenize(text, {
    maxLength: preprocessingConfig.max_length,
    padding: preprocessingConfig.padding,
    truncation: preprocessingConfig.truncation,
    addSpecialTokens: preprocessingConfig.add_special_tokens
  });
  
  return {
    input_ids: tokenized.inputIds,
    attention_mask: tokenized.attentionMask,
    token_type_ids: tokenized.tokenTypeIds
  };
}

// Load model
async function loadModel(modelPath) {
  try {
    // Create WebNN execution provider if available
    let webnnEp = null;
    try {
      if ('ml' in navigator) {
        // Use WebNN API directly if available in browser
        const context = await navigator.ml.createContext({ type: 'gpu' });
        if (context) {
          webnnEp = {
            name: 'webnn',
            context: context
          };
        }
      }
    } catch (e) {
      console.warn('WebNN not available:', e);
    }
    
    // Create session with WebNN or fallback to WASM
    const session = await ort.InferenceSession.create(modelPath, {
      executionProviders: webnnEp ? ['webnn', 'wasm'] : ['wasm'],
      graphOptimizationLevel: 'all'
    });
    
    return session;
  } catch (e) {
    console.error('Failed to load model:', e);
    throw e;
  }
}

// Run inference
async function runInference(session, inputData) {
  try {
    // Prepare input tensors
    const feeds = {};
    for (const [name, data] of Object.entries(inputData)) {
      feeds[name] = new ort.Tensor(
        name === 'input_ids' || name === 'attention_mask' || name === 'token_type_ids' ? 'int64' : 'float32',
        data,
        Array.isArray(data) ? [1, data.length] : data.shape
      );
    }
    
    // Run inference
    const results = await session.run(feeds);
    return results;
  } catch (e) {
    console.error('Inference failed:', e);
    throw e;
  }
}

// Full pipeline
async function bertPipeline(text, modelPath) {
  // Initialize
  const tokenizer = await initTokenizer();
  const model = await loadModel(modelPath);
  
  // Preprocess
  const inputs = await preprocessInput(text, tokenizer);
  
  // Run inference
  const results = await runInference(model, inputs);
  
  // Return results
  return results;
}

// Export the pipeline
export { bertPipeline };
"""
        )
        
        # JavaScript inference snippets
        capability.js_inference_snippets = {
            "onnx": """
import * as ort from 'onnxruntime-web';

async function runBertOnnx(text, modelPath) {
  // Load ONNX model
  const session = await ort.InferenceSession.create(modelPath);
  
  // Tokenize input (implementation depends on your tokenizer)
  const tokenizer = new BertTokenizer();
  const encoded = await tokenizer.encode(text);
  
  // Create input tensors
  const feeds = {};
  feeds['input_ids'] = new ort.Tensor('int64', encoded.inputIds, [1, encoded.inputIds.length]);
  feeds['attention_mask'] = new ort.Tensor('int64', encoded.attentionMask, [1, encoded.attentionMask.length]);
  feeds['token_type_ids'] = new ort.Tensor('int64', encoded.tokenTypeIds, [1, encoded.tokenTypeIds.length]);
  
  // Run inference
  const results = await session.run(feeds);
  return results;
}
"""
        }
        
        # ONNX conversion specifics
        capability.onnx_custom_ops_mapping = {}
        capability.onnx_additional_conversion_args = {
            "atol": 1e-4,
            "input_names": ["input_ids", "attention_mask", "token_type_ids"],
            "output_names": ["last_hidden_state"]
        }
        
    elif model_id.startswith("t5-"):
        # T5 models have some export limitations
        capability.model_type = "t5"
        capability.model_family = "transformer"
        
        # Architecture parameters
        capability.architecture_params = {
            "hidden_size": 512,
            "num_attention_heads": 8,
            "num_hidden_layers": 6,
            "d_ff": 2048,
            "d_kv": 64
        }
        
        # Input/output specs
        capability.inputs = [
            InputOutputSpec(
                name="input_ids",
                shape=["batch_size", "sequence_length"],
                dtype="int64",
                is_dynamic=True,
                typical_shape=[1, 128]
            ),
            InputOutputSpec(
                name="attention_mask",
                shape=["batch_size", "sequence_length"],
                dtype="int64",
                is_dynamic=True,
                typical_shape=[1, 128],
                is_required=False
            )
        ]
        capability.outputs = [
            InputOutputSpec(
                name="last_hidden_state",
                shape=["batch_size", "sequence_length", "hidden_size"],
                dtype="float32",
                is_dynamic=True,
                typical_shape=[1, 128, 512]
            )
        ]
        
        # Preprocessing information
        capability.preprocessing_info = {
            "tokenizer": "T5Tokenizer",
            "padding": "max_length",
            "truncation": True,
            "return_tensors": "pt",
            "max_length": 128
        }
        
        # WebNN setup for T5
        capability.webnn_info = WebNNBackendInfo(
            supported=True,
            preferred_backend="gpu",
            fallback_backends=["cpu"],
            requires_polyfill=True,
            browser_compatibility={
                "chrome": True,
                "firefox": False,
                "safari": True,
                "edge": True
            },
            estimated_memory_usage_mb=250,
            js_dependencies=[
                "onnxruntime-web@1.14.0",
                "webnn-polyfill@0.1.0"
            ]
        )
        
        capability.operation_limitations.append("T5 attention mechanism may require opset >= 12")
        capability.export_warnings.append("T5 decoder may not export correctly with dynamic generation")
        
    elif model_id.startswith("vit-"):
        # Vision Transformer models
        capability.model_type = "vit"
        capability.model_family = "transformer"
        
        # Architecture parameters
        capability.architecture_params = {
            "hidden_size": 768,
            "num_attention_heads": 12,
            "num_hidden_layers": 12,
            "intermediate_size": 3072,
            "patch_size": 16,
            "image_size": 224
        }
        
        # Input normalization - ImageNet defaults
        capability.input_normalization = {
            "mean": [0.485, 0.456, 0.406],
            "std": [0.229, 0.224, 0.225]
        }
        
        # Input/output specs
        capability.inputs = [
            InputOutputSpec(
                name="pixel_values",
                shape=["batch_size", "num_channels", "height", "width"],
                dtype="float32",
                is_dynamic=True,
                typical_shape=[1, 3, 224, 224]
            )
        ]
        capability.outputs = [
            InputOutputSpec(
                name="last_hidden_state",
                shape=["batch_size", "sequence_length", "hidden_size"],
                dtype="float32",
                is_dynamic=True,
                typical_shape=[1, 197, 768]  # 196 patches + 1 cls token
            )
        ]
        
        # Preprocessing information
        capability.preprocessing_info = {
            "resize": [224, 224],
            "normalize": True,
            "center_crop": True,
            "return_tensors": "pt"
        }
        
        # WebNN information for vision models
        capability.webnn_info = WebNNBackendInfo(
            supported=True,
            preferred_backend="gpu",
            fallback_backends=["cpu"],
            requires_polyfill=False,
            browser_compatibility={
                "chrome": True,
                "firefox": True,
                "safari": True,
                "edge": True
            },
            js_code_template="""
// WebNN inference code for {{MODEL_TYPE}} model
import * as ort from 'onnxruntime-web';

// Model input shapes
const inputShapes = {{INPUT_SHAPES}};

// Preprocessing configuration
const preprocessingConfig = {{PREPROCESSING}};

// Image preprocessing
async function preprocessImage(imageData) {
  const canvas = document.createElement('canvas');
  canvas.width = preprocessingConfig.resize[0];
  canvas.height = preprocessingConfig.resize[1];
  
  const ctx = canvas.getContext('2d');
  ctx.drawImage(imageData, 0, 0, canvas.width, canvas.height);
  
  // Get image data
  const imageDataResized = ctx.getImageData(0, 0, canvas.width, canvas.height);
  const data = imageDataResized.data;
  
  // Convert to tensor format [1, 3, height, width] and normalize
  const mean = [0.485, 0.456, 0.406];
  const std = [0.229, 0.224, 0.225];
  
  const tensor = new Float32Array(3 * canvas.height * canvas.width);
  
  for (let y = 0; y < canvas.height; y++) {
    for (let x = 0; x < canvas.width; x++) {
      const pixelIndex = (y * canvas.width + x) * 4;
      
      // RGB channels
      for (let c = 0; c < 3; c++) {
        const normalizedValue = (data[pixelIndex + c] / 255.0 - mean[c]) / std[c];
        // Store in CHW format
        tensor[c * canvas.height * canvas.width + y * canvas.width + x] = normalizedValue;
      }
    }
  }
  
  return tensor;
}

// Load model
async function loadModel(modelPath) {
  try {
    // Create WebNN execution provider if available
    let webnnEp = null;
    try {
      if ('ml' in navigator) {
        // Use WebNN API directly if available in browser
        const context = await navigator.ml.createContext({ type: 'gpu' });
        if (context) {
          webnnEp = {
            name: 'webnn',
            context: context
          };
        }
      }
    } catch (e) {
      console.warn('WebNN not available:', e);
    }
    
    // Create session with WebNN or fallback to WASM
    const session = await ort.InferenceSession.create(modelPath, {
      executionProviders: webnnEp ? ['webnn', 'wasm'] : ['wasm'],
      graphOptimizationLevel: 'all'
    });
    
    return session;
  } catch (e) {
    console.error('Failed to load model:', e);
    throw e;
  }
}

// Run inference
async function runInference(session, imageData) {
  try {
    // Preprocess image
    const inputTensor = await preprocessImage(imageData);
    
    // Create input tensor
    const feeds = {};
    feeds['pixel_values'] = new ort.Tensor(
      'float32',
      inputTensor,
      [1, 3, preprocessingConfig.resize[0], preprocessingConfig.resize[1]]
    );
    
    // Run inference
    const results = await session.run(feeds);
    return results;
  } catch (e) {
    console.error('Inference failed:', e);
    throw e;
  }
}

// Full pipeline
async function vitPipeline(imageData, modelPath) {
  // Initialize
  const model = await loadModel(modelPath);
  
  // Run inference
  const results = await runInference(model, imageData);
  
  // Return results
  return results;
}

// Export the pipeline
export { vitPipeline };
"""
        )
        
    elif model_id.startswith("whisper-"):
        # Whisper models
        capability.model_type = "whisper"
        capability.model_family = "transformer"
        
        # Architecture parameters
        capability.architecture_params = {
            "hidden_size": 512,
            "encoder_layers": 6,
            "encoder_attention_heads": 8,
            "decoder_layers": 6,
            "decoder_attention_heads": 8,
            "max_source_positions": 1500
        }
        
        # Input/output specs
        capability.inputs = [
            InputOutputSpec(
                name="input_features",
                shape=["batch_size", "feature_size", "sequence_length"],
                dtype="float32",
                is_dynamic=True,
                typical_shape=[1, 80, 3000]
            )
        ]
        capability.outputs = [
            InputOutputSpec(
                name="last_hidden_state",
                shape=["batch_size", "sequence_length", "hidden_size"],
                dtype="float32",
                is_dynamic=True,
                typical_shape=[1, 1500, 512]
            )
        ]
        
        # Preprocessing information
        capability.preprocessing_info = {
            "feature_extraction": "whisper_log_mel_spectrogram",
            "sampling_rate": 16000,
            "n_fft": 400,
            "hop_length": 160,
            "n_mels": 80,
            "padding": "longest"
        }
        
        # WebNN not recommended for complex audio models
        capability.webnn_info = WebNNBackendInfo(
            supported=False,
            requires_polyfill=True,
            browser_compatibility={
                "chrome": False,
                "firefox": False,
                "safari": False,
                "edge": False
            },
            js_dependencies=[
                "onnxruntime-web@1.14.0",
                "web-audio-api@0.2.2"
            ]
        )
        
        capability.export_warnings.append("Whisper generation functionality may not export correctly")
        capability.operation_limitations.append("Whisper may require custom processing for audio features")
        capability.custom_ops.append("mel_filter_bank")
    
    # If we have an actual model instance, get more specific information
    if model is not None:
        # Check parameter count and model size
        param_count = sum(p.numel() for p in model.parameters())
        model_size_mb = sum(p.numel() * p.element_size() for p in model.parameters()) / (1024 * 1024)
        
        # Add to architecture params
        capability.architecture_params["param_count"] = param_count
        capability.architecture_params["model_size_mb"] = model_size_mb
        
        if model_size_mb > 2000:
            capability.export_warnings.append(f"Model is very large ({model_size_mb:.2f} MB), export may be slow and result in large files")
            # Adjust WebNN compatibility
            if "webnn" in capability.supported_formats:
                capability.supported_formats.remove("webnn")
                capability.webnn_info.supported = False
                capability.export_warnings.append("Model too large for WebNN, removed from supported formats")
        
        # Update WebNN estimated memory usage
        capability.webnn_info.estimated_memory_usage_mb = model_size_mb * 1.2  # 20% overhead
        
        # Create dummy inputs based on input specs
        dummy_inputs = {}
        try:
            for input_spec in capability.inputs:
                shape = input_spec.typical_shape if input_spec.typical_shape else [1 if isinstance(dim, str) else dim for dim in input_spec.shape]
                dtype = torch.float32 if input_spec.dtype == "float32" else torch.int64
                if input_spec.is_required:
                    dummy_inputs[input_spec.name] = torch.ones(shape, dtype=dtype)
            
            # Check ONNX compatibility
            onnx_compatible, onnx_issues = check_onnx_compatibility(model, dummy_inputs)
            if not onnx_compatible:
                capability.export_warnings.extend(onnx_issues)
            
            # Check WebNN compatibility
            if "webnn" in capability.supported_formats:
                webnn_compatible, webnn_issues = check_webnn_compatibility(model, dummy_inputs)
                if not webnn_compatible:
                    capability.export_warnings.extend(webnn_issues)
                    if not all(issue.startswith("Warning") for issue in webnn_issues):
                        capability.supported_formats.remove("webnn")
                        capability.webnn_info.supported = False
        
        except Exception as e:
            capability.export_warnings.append(f"Failed to perform detailed compatibility check: {str(e)}")
    
    return capability


def get_optimized_export_config(
    model_id: str,
    export_format: str,
    hardware_target: Optional[str] = None,
    precision: Optional[str] = None
) -> ExportConfig:
    """
    Get optimized export configuration for a specific model, format, and hardware target
    
    Args:
        model_id: Identifier for the model (e.g., "bert-base-uncased")
        export_format: Target export format (e.g., "onnx", "webnn")
        hardware_target: Target hardware (e.g., "cpu", "cuda", "amd")
        precision: Target precision (e.g., "fp32", "fp16", "int8")
        
    Returns:
        config: Optimized ExportConfig object
    """
    # Initialize with defaults
    config = ExportConfig(format=export_format)
    
    # Detect hardware if not specified
    if hardware_target is None:
        try:
            from auto_hardware_detection import detect_all_hardware
            hardware = detect_all_hardware()
            detected_hw = [hw for hw, info in hardware.items() if info.detected]
            
            # Get hardware with priority
            hw_priority = ["cuda", "amd", "openvino", "mps", "cpu"]
            hardware_target = next((hw for hw in hw_priority if hw in detected_hw), "cpu")
        except ImportError:
            hardware_target = "cpu"
            logger.warning("Could not import auto_hardware_detection, defaulting to CPU target")
    
    # Determine precision if not specified
    if precision is None:
        try:
            from auto_hardware_detection import detect_all_hardware, determine_precision_for_all_hardware
            hardware = detect_all_hardware()
            precision_info = determine_precision_for_all_hardware(hardware)
            
            if hardware_target in precision_info:
                precision = precision_info[hardware_target].optimal
            else:
                # Default precisions based on hardware
                if hardware_target in ["cuda", "amd", "mps"]:
                    precision = "fp16"
                elif hardware_target == "openvino":
                    precision = "int8"
                else:
                    precision = "fp32"
        except ImportError:
            # Default to fp32 if we can't detect
            precision = "fp32"
    
    # Set target hardware
    config.target_hardware = hardware_target
    
    # Set precision
    config.precision = precision
    
    # Model-specific optimizations
    if model_id.startswith("bert-"):
        # BERT models work well with opset 12+
        config.opset_version = 12
        
        # Set up dynamic axes for batch size and sequence length
        config.dynamic_axes = {
            "input_ids": {0: "batch_size", 1: "sequence_length"},
            "attention_mask": {0: "batch_size", 1: "sequence_length"},
            "token_type_ids": {0: "batch_size", 1: "sequence_length"},
            "output": {0: "batch_size", 1: "sequence_length"}
        }
        
        # Enable optimizations based on precision
        if precision == "int8":
            config.quantize = True
    
    elif model_id.startswith("t5-"):
        # T5 models need newer opset versions
        config.opset_version = 13
        
        # Set dynamic axes
        config.dynamic_axes = {
            "input_ids": {0: "batch_size", 1: "sequence_length"},
            "attention_mask": {0: "batch_size", 1: "sequence_length"},
            "output": {0: "batch_size", 1: "sequence_length"}
        }
    
    elif model_id.startswith("vit-"):
        # Vision transformers
        config.opset_version = 12
        
        # Set dynamic axes - vision models often can use fixed image sizes
        config.dynamic_axes = {
            "pixel_values": {0: "batch_size"}
        }
    
    elif model_id.startswith("whisper-"):
        # Whisper models
        config.opset_version = 14
        
        # Set dynamic axes
        config.dynamic_axes = {
            "input_features": {0: "batch_size", 2: "sequence_length"},
            "output": {0: "batch_size", 1: "sequence_length"}
        }
    
    # Hardware-specific optimizations
    if hardware_target == "cpu":
        # CPU optimizations
        config.constant_folding = True
        config.optimization_level = 99
        
        # For smaller deployments, quantization can be helpful
        if precision == "int8":
            config.quantize = True
    
    elif hardware_target in ["cuda", "amd"]:
        # GPU optimizations
        config.optimization_level = 1  # Less aggressive to maintain GPU-specific optimizations
        
        # Set precision-specific options
        if precision == "fp16":
            config.additional_options["fp16_mode"] = True
    
    elif hardware_target == "openvino":
        # OpenVINO specific
        config.optimization_level = 99
        config.additional_options["optimize_for_openvino"] = True
        
        # INT8 works well with OpenVINO
        if precision == "int8":
            config.quantize = True
    
    # Format-specific optimizations
    if export_format == "webnn":
        # WebNN generally works best with opset 12 for broader compatibility
        config.opset_version = 12
        
        # WebNN has more limited quantization options
        config.quantize = False
        
        # Add WebNN-specific options
        config.additional_options["optimize_for_web"] = True
        config.additional_options["minimize_model_size"] = True
    
    return config


# Main export function that ties everything together
def export_model(
    model: torch.nn.Module,
    model_id: str,
    output_path: str,
    export_format: str = "onnx",
    example_inputs: Optional[Union[Dict[str, torch.Tensor], torch.Tensor, Tuple[torch.Tensor, ...]]] = None,
    hardware_target: Optional[str] = None,
    precision: Optional[str] = None,
    custom_config: Optional[ExportConfig] = None
) -> Tuple[bool, str]:
    """
    Export a model to the specified format with optimized settings
    
    Args:
        model: PyTorch model to export
        model_id: Identifier for the model (e.g., "bert-base-uncased")
        output_path: Path to save the exported model
        export_format: Target export format (e.g., "onnx", "webnn")
        example_inputs: Example inputs for the model
        hardware_target: Target hardware (e.g., "cpu", "cuda", "amd")
        precision: Target precision (e.g., "fp32", "fp16", "int8")
        custom_config: Optional custom export configuration
        
    Returns:
        success: Boolean indicating success
        message: Message describing the result or error
    """
    # Ensure model is in eval mode
    model.eval()
    
    # Get model export capability information
    capability = get_model_export_capability(model_id, model)
    
    # Check if the requested format is supported
    if not capability.is_supported_format(export_format):
        return False, f"Export format '{export_format}' is not supported for model '{model_id}'"
    
    # Get optimized export configuration
    if custom_config is None:
        config = get_optimized_export_config(model_id, export_format, hardware_target, precision)
    else:
        config = custom_config
    
    # Generate example inputs if not provided
    if example_inputs is None:
        example_inputs = {}
        for input_spec in capability.inputs:
            if input_spec.is_required:
                shape = input_spec.typical_shape if input_spec.typical_shape else [1 if isinstance(dim, str) else dim for dim in input_spec.shape]
                dtype = torch.float32 if input_spec.dtype == "float32" else torch.int64
                example_inputs[input_spec.name] = torch.ones(shape, dtype=dtype)
    
    # Export the model based on the format
    if export_format.lower() == "onnx":
        return export_to_onnx(model, example_inputs, output_path, config)
    elif export_format.lower() == "webnn":
        return export_to_webnn(model, example_inputs, output_path, config)
    else:
        return False, f"Unsupported export format: {export_format}"


# Utility function to check export capability and provide recommendations
def analyze_model_export_compatibility(
    model: torch.nn.Module,
    model_id: str,
    formats: Optional[List[str]] = None
) -> Dict[str, Any]:
    """
    Analyze a model's compatibility with different export formats
    
    Args:
        model: PyTorch model to analyze
        model_id: Identifier for the model
        formats: List of export formats to check (defaults to ["onnx", "webnn"])
        
    Returns:
        report: Dictionary with compatibility information and recommendations
    """
    if formats is None:
        formats = ["onnx", "webnn"]
    
    # Get capability information
    capability = get_model_export_capability(model_id, model)
    
    # Create dummy inputs
    dummy_inputs = {}
    for input_spec in capability.inputs:
        if input_spec.is_required:
            shape = input_spec.typical_shape if input_spec.typical_shape else [1 if isinstance(dim, str) else dim for dim in input_spec.shape]
            dtype = torch.float32 if input_spec.dtype == "float32" else torch.int64
            dummy_inputs[input_spec.name] = torch.ones(shape, dtype=dtype)
    
    # Check each format
    format_reports = {}
    for fmt in formats:
        compatible = capability.is_supported_format(fmt)
        issues = []
        
        if fmt.lower() == "onnx":
            is_compat, fmt_issues = check_onnx_compatibility(model, dummy_inputs)
            compatible = compatible and is_compat
            issues.extend(fmt_issues)
        elif fmt.lower() == "webnn":
            is_compat, fmt_issues = check_webnn_compatibility(model, dummy_inputs)
            compatible = compatible and is_compat
            issues.extend(fmt_issues)
        
        # Get recommended hardware
        recommended_hardware = capability.get_recommended_hardware(fmt)
        
        # Get recommended configuration
        config = get_optimized_export_config(model_id, fmt)
        
        format_reports[fmt] = {
            "compatible": compatible,
            "issues": issues,
            "recommended_hardware": recommended_hardware,
            "recommended_config": {
                "opset_version": config.opset_version,
                "precision": config.precision,
                "quantize": config.quantize,
                "dynamic_axes": config.dynamic_axes is not None
            }
        }
    
    # Overall report
    report = {
        "model_id": model_id,
        "formats": format_reports,
        "supported_formats": list(capability.supported_formats),
        "inputs": [{"name": inp.name, "shape": inp.shape, "dtype": inp.dtype} for inp in capability.inputs],
        "outputs": [{"name": out.name, "shape": out.shape, "dtype": out.dtype} for out in capability.outputs],
        "warnings": capability.export_warnings,
        "limitations": capability.operation_limitations,
        "recommendations": []
    }
    
    # Add overall recommendations
    if any(not details["compatible"] for fmt, details in format_reports.items()):
        report["recommendations"].append("Model has compatibility issues with some export formats")
    
    if "onnx" in format_reports and format_reports["onnx"]["compatible"]:
        report["recommendations"].append("ONNX export is recommended for best compatibility")
    
    return report


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="Model Export Capability Tool")
    parser.add_argument("--model", required=True, help="Model ID or path (e.g., bert-base-uncased)")
    parser.add_argument("--format", default="onnx", choices=["onnx", "webnn"], help="Export format")
    parser.add_argument("--output", default="exported_model", help="Output path for exported model")
    parser.add_argument("--hardware", help="Target hardware (cpu, cuda, amd, openvino)")
    parser.add_argument("--precision", help="Target precision (fp32, fp16, int8)")
    parser.add_argument("--analyze", action="store_true", help="Only analyze compatibility without exporting")
    
    args = parser.parse_args()
    
    try:
        # Load model
        from transformers import AutoModel
        model = AutoModel.from_pretrained(args.model)
        
        if args.analyze:
            # Just analyze compatibility
            report = analyze_model_export_compatibility(model, args.model, [args.format])
            print(json.dumps(report, indent=2))
        else:
            # Export the model
            success, message = export_model(
                model=model,
                model_id=args.model,
                output_path=args.output,
                export_format=args.format,
                hardware_target=args.hardware,
                precision=args.precision
            )
            
            if success:
                print(f"✅ {message}")
            else:
                print(f"❌ {message}")
    
    except Exception as e:
        print(f"Error: {str(e)}")
        import traceback
        traceback.print_exc()