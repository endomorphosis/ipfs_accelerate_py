#!/usr/bin/env python
"""
Master script to fix all API backends with:
1. Environment variable handling for API keys
2. Request queueing with concurrency control
3. Exponential backoff retry with proper error handling
4. Test updates for the new functionality
5. REAL implementations for Ollama, HF TGI, HF TEI, and Gemini
"""

import os
import sys
import subprocess
import argparse
import time
from pathlib import Path

def run_script(script_path, args=None):
    """Run a Python script with optional arguments"""
    # Convert Path object to string if needed
    if isinstance(script_path, Path):
        script_path = str(script_path)
        
    cmd = [sys.executable, script_path]
    if args:
        cmd.extend(args)
        
    print(f"Running: {' '.join(cmd)}")
    
    try:
        result = subprocess.run(cmd, check=True, capture_output=True, text=True)
        print(result.stdout)
        return True
    except subprocess.CalledProcessError as e:
        print(f"Error running {script_path}:")
        print(e.stdout)
        print(e.stderr)
        return False

def create_env_file():
    """Create a .env.example file if it doesn't exist"""
    env_example_path = Path(__file__).parent / ".env.example"
    
    if not env_example_path.exists():
        print("Creating .env.example file...")
        env_content = """# Example environment variables for API access
# Copy this file to .env and fill in your API keys
# WARNING: Never commit API keys to version control!

# OpenAI API
OPENAI_API_KEY=your_openai_api_key_here

# Groq API
GROQ_API_KEY=your_groq_api_key_here

# Claude API
ANTHROPIC_API_KEY=your_claude_api_key_here

# Gemini API
GOOGLE_API_KEY=your_gemini_api_key_here

# Hugging Face API
HF_API_TOKEN=your_huggingface_api_token_here

# Ollama API
OLLAMA_API_URL=http://localhost:11434/api
OLLAMA_MODEL=llama3

# LLVM API
LLVM_API_ENDPOINT=http://localhost:8080/v1

# OPEA API
OPEA_API_ENDPOINT=http://localhost:8000/v1

# OVMS API
OVMS_API_URL=http://localhost:9000
OVMS_MODEL=model

# S3 Kit
S3_ACCESS_KEY=your_s3_access_key
S3_SECRET_KEY=your_s3_secret_key
S3_ENDPOINT=http://localhost:9000
"""
        with open(env_example_path, 'w') as f:
            f.write(env_content)
        print(f"Created {env_example_path}")
    else:
        print(f"{env_example_path} already exists")

def create_api_implementation(api_name, output_dir):
    """Create a REAL implementation for the specified API"""
    # Using triple single quotes for templates to avoid syntax errors with triple double quotes
    all_templates = {
        "ollama": '''import os
import json
import time
import threading
import requests
import uuid
from concurrent.futures import Future
from queue import Queue
from dotenv import load_dotenv

class ollama:
    def __init__(self, resources=None, metadata=None):
        self.resources = resources if resources else {}
        self.metadata = metadata if metadata else {}
        
        # Get Ollama API endpoint from metadata or environment
        self.ollama_api_url = self._get_ollama_api_url()
        
        # Initialize queue and backoff systems
        self.max_concurrent_requests = 5
        self.queue_size = 100
        self.request_queue = Queue(maxsize=self.queue_size)
        self.active_requests = 0
        self.queue_lock = threading.RLock()
        
        # Start queue processor
        self.queue_processor = threading.Thread(target=self._process_queue)
        self.queue_processor.daemon = True
        self.queue_processor.start()
        
        # Initialize backoff configuration
        self.max_retries = 5
        self.initial_retry_delay = 1
        self.backoff_factor = 2
        self.max_retry_delay = 16
        
        # Default model
        self.default_model = os.environ.get("OLLAMA_MODEL", "llama3")
        
        return None

    def _get_ollama_api_url(self):
        """Get Ollama API URL from metadata or environment"""
        # Try to get from metadata
        api_url = self.metadata.get("ollama_api_url")
        if api_url:
            return api_url
        
        # Try to get from environment
        env_url = os.environ.get("OLLAMA_API_URL")
        if env_url:
            return env_url
        
        # Try to load from dotenv
        try:
            load_dotenv()
            env_url = os.environ.get("OLLAMA_API_URL")
            if env_url:
                return env_url
        except ImportError:
            pass
        
        # Return default if no URL found
        return "http://localhost:11434/api"
        
    def _process_queue(self):
        """Process queued requests with proper concurrency management"""
        while True:
            try:
                future, endpoint_url, data, stream, request_id = self.request_queue.get()
                
                with self.queue_lock:
                    self.active_requests += 1
                
                # Process with retry logic
                retry_count = 0
                while retry_count <= self.max_retries:
                    try:
                        # Construct headers
                        headers = {"Content-Type": "application/json"}
                        
                        # Make request with proper error handling
                        if stream:
                            response = requests.post(
                                endpoint_url,
                                json=data,
                                headers=headers,
                                stream=True,
                                timeout=self.metadata.get("timeout", 30)
                            )
                        else:
                            response = requests.post(
                                endpoint_url,
                                json=data,
                                headers=headers,
                                timeout=self.metadata.get("timeout", 30)
                            )
                        
                        # Check for HTTP errors
                        response.raise_for_status()
                        
                        # Return response based on stream mode
                        if stream:
                            # Create a streaming generator
                            def response_generator():
                                for line in response.iter_lines():
                                    if line:
                                        yield json.loads(line)
                            
                            result = response_generator()
                        else:
                            # Parse JSON response
                            result = response.json()
                        
                        future.set_result(result)
                        break
                        
                    except requests.RequestException as e:
                        retry_count += 1
                        
                        if retry_count > self.max_retries:
                            future.set_exception(e)
                            break
                        
                        # Calculate backoff delay
                        delay = min(
                            self.initial_retry_delay * (self.backoff_factor ** (retry_count - 1)),
                            self.max_retry_delay
                        )
                        
                        # Sleep with backoff
                        time.sleep(delay)
                    
                    except Exception as e:
                        future.set_exception(e)
                        break
                
                with self.queue_lock:
                    self.active_requests -= 1
                
                self.request_queue.task_done()
                
            except Exception as e:
                print(f"Error in queue processor: {e}")
    
    def make_post_request_ollama(self, endpoint_url, data, stream=False):
        """Make a request to Ollama API with queue and backoff"""
        # Generate unique request ID
        request_id = str(uuid.uuid4())
        
        # Queue system with proper concurrency management
        future = Future()
        
        # Add to queue
        self.request_queue.put((future, endpoint_url, data, stream, request_id))
        
        # Get result (blocks until request is processed)
        return future.result()
        
    def chat(self, model, messages, options=None):
        """Send a chat request to Ollama API"""
        # Construct the proper endpoint URL
        endpoint_url = f"{self.ollama_api_url}/chat"
        
        # Format messages for Ollama API
        formatted_messages = self._format_messages(messages)
        
        # Prepare request data
        data = {
            "model": model,
            "messages": formatted_messages,
            "stream": False
        }
        
        # Add options if provided
        if options:
            data["options"] = options
        
        # Make request with queue and backoff
        response = self.make_post_request_ollama(endpoint_url, data)
        
        # Process and normalize response
        return {
            "text": response.get("message", {}).get("content", ""),
            "model": model,
            "usage": self._extract_usage(response),
            "implementation_type": "(REAL)"
        }

    def stream_chat(self, model, messages, options=None):
        """Stream a chat request from Ollama API"""
        # Construct the proper endpoint URL
        endpoint_url = f"{self.ollama_api_url}/chat"
        
        # Format messages for Ollama API
        formatted_messages = self._format_messages(messages)
        
        # Prepare request data
        data = {
            "model": model,
            "messages": formatted_messages,
            "stream": True
        }
        
        # Add options if provided
        if options:
            data["options"] = options
        
        # Make streaming request
        response_stream = self.make_post_request_ollama(endpoint_url, data, stream=True)
        
        # Process streaming response
        for chunk in response_stream:
            yield {
                "text": chunk.get("message", {}).get("content", ""),
                "done": chunk.get("done", False),
                "model": model
            }
            
    def _format_messages(self, messages):
        """Format messages for Ollama API"""
        formatted_messages = []
        
        for message in messages:
            role = message.get("role", "user")
            content = message.get("content", "")
            
            # Map standard roles to Ollama roles
            if role == "assistant":
                role = "assistant"
            elif role == "system":
                role = "system"
            else:
                role = "user"
            
            formatted_messages.append({
                "role": role,
                "content": content
            })
        
        return formatted_messages

    def _extract_usage(self, response):
        """Extract usage information from response"""
        return {
            "prompt_tokens": response.get("prompt_eval_count", 0),
            "completion_tokens": response.get("eval_count", 0),
            "total_tokens": response.get("prompt_eval_count", 0) + response.get("eval_count", 0)
        }

    def list_models(self):
        """List available models in Ollama"""
        endpoint_url = f"{self.ollama_api_url}/tags"
        
        try:
            response = requests.get(
                endpoint_url,
                timeout=self.metadata.get("timeout", 30)
            )
            response.raise_for_status()
            
            data = response.json()
            return data.get("models", [])
        except Exception as e:
            print(f"Error listing models: {e}")
            return []
            
    def create_ollama_endpoint_handler(self, endpoint_url=None):
        """Create an endpoint handler for Ollama"""
        if not endpoint_url:
            endpoint_url = self.ollama_api_url
        
        async def endpoint_handler(prompt, **kwargs):
            """Handle requests to Ollama endpoint"""
            # Get model from kwargs or default
            model = kwargs.get("model", self.metadata.get("ollama_model", self.default_model))
            
            # Create messages from prompt
            messages = [{"role": "user", "content": prompt}]
            
            # Extract options
            options = {}
            for key in ["temperature", "top_p", "top_k", "repeat_penalty"]:
                if key in kwargs:
                    options[key] = kwargs[key]
            
            # Make request
            try:
                response = self.chat(model, messages, options)
                return response
            except Exception as e:
                print(f"Error calling Ollama endpoint: {e}")
                return {"text": f"Error: {str(e)}", "implementation_type": "(ERROR)"}
        
        return endpoint_handler
        
    def test_ollama_endpoint(self, endpoint_url=None):
        """Test the Ollama endpoint"""
        if not endpoint_url:
            endpoint_url = f"{self.ollama_api_url}/chat"
            
        model = self.metadata.get("ollama_model", self.default_model)
        messages = [{"role": "user", "content": "Testing the Ollama API. Please respond with a short message."}]
        
        try:
            response = self.chat(model, messages)
            return "text" in response and response.get("implementation_type") == "(REAL)"
        except Exception as e:
            print(f"Error testing Ollama endpoint: {e}")
            return False''',
        
        "hf_tgi": '''import os
import json
import time
import uuid
import threading
import requests
from concurrent.futures import Future
from queue import Queue
from dotenv import load_dotenv

class hf_tgi:
    def __init__(self, resources=None, metadata=None):
        self.resources = resources if resources else {}
        self.metadata = metadata if metadata else {}
        
        # Get HF API token from metadata or environment
        self.api_token = self._get_api_token()
        
        # Initialize queue and backoff systems
        self.max_concurrent_requests = 5
        self.queue_size = 100
        self.request_queue = Queue(maxsize=self.queue_size)
        self.active_requests = 0
        self.queue_lock = threading.RLock()
        
        # Start queue processor
        self.queue_processor = threading.Thread(target=self._process_queue)
        self.queue_processor.daemon = True
        self.queue_processor.start()
        
        # Initialize backoff configuration
        self.max_retries = 5
        self.initial_retry_delay = 1
        self.backoff_factor = 2
        self.max_retry_delay = 16
        
        # Default model
        self.default_model = "mistralai/Mistral-7B-Instruct-v0.2"
        
        return None

    def _get_api_token(self):
        """Get Hugging Face API token from metadata or environment"""
        # Try to get from metadata
        api_token = self.metadata.get("hf_api_key") or self.metadata.get("hf_api_token")
        if api_token:
            return api_token
        
        # Try to get from environment
        env_token = os.environ.get("HF_API_KEY") or os.environ.get("HF_API_TOKEN")
        if env_token:
            return env_token
        
        # Try to load from dotenv
        try:
            load_dotenv()
            env_token = os.environ.get("HF_API_KEY") or os.environ.get("HF_API_TOKEN")
            if env_token:
                return env_token
        except ImportError:
            pass
        
        # Return None if no token found (will allow unauthenticated requests)
        return None
        
    def _process_queue(self):
        """Process queued requests with proper concurrency management"""
        while True:
            try:
                future, endpoint_url, data, api_token, stream, request_id = self.request_queue.get()
                
                with self.queue_lock:
                    self.active_requests += 1
                
                # Process with retry logic
                retry_count = 0
                while retry_count <= self.max_retries:
                    try:
                        # Construct headers
                        headers = {"Content-Type": "application/json"}
                        if api_token:
                            headers["Authorization"] = f"Bearer {api_token}"
                        
                        # Make request with proper error handling
                        if stream:
                            response = requests.post(
                                endpoint_url,
                                json=data,
                                headers=headers,
                                stream=True,
                                timeout=self.metadata.get("timeout", 30)
                            )
                        else:
                            response = requests.post(
                                endpoint_url,
                                json=data,
                                headers=headers,
                                timeout=self.metadata.get("timeout", 30)
                            )
                        
                        # Handle specific status codes
                        if response.status_code == 401:
                            raise ValueError(f"Authentication error: Invalid API token (401)")
                        elif response.status_code == 404:
                            raise ValueError(f"Model not found: {endpoint_url} (404)")
                        elif response.status_code == 429:
                            retry_count += 1
                            
                            # Get retry delay from headers or use default
                            retry_after = int(response.headers.get("Retry-After", self.initial_retry_delay * (self.backoff_factor ** (retry_count - 1))))
                            time.sleep(min(retry_after, self.max_retry_delay))
                            continue
                        
                        # Check for other HTTP errors
                        response.raise_for_status()
                        
                        # Return response based on stream mode
                        if stream:
                            # Create a streaming generator
                            def response_generator():
                                buffer = ""
                                for chunk in response.iter_content(chunk_size=1):
                                    if chunk:
                                        buffer += chunk.decode('utf-8')
                                        if buffer.endswith('\\n'):
                                            try:
                                                yield json.loads(buffer.strip())
                                            except json.JSONDecodeError:
                                                pass
                                            buffer = ""
                            
                            result = response_generator()
                        else:
                            # Parse JSON response
                            result = response.json()
                        
                        future.set_result(result)
                        break
                        
                    except requests.RequestException as e:
                        retry_count += 1
                        
                        if retry_count > self.max_retries:
                            future.set_exception(e)
                            break
                        
                        # Calculate backoff delay
                        delay = min(
                            self.initial_retry_delay * (self.backoff_factor ** (retry_count - 1)),
                            self.max_retry_delay
                        )
                        
                        # Sleep with backoff
                        time.sleep(delay)
                    
                    except Exception as e:
                        future.set_exception(e)
                        break
                
                with self.queue_lock:
                    self.active_requests -= 1
                
                self.request_queue.task_done()
                
            except Exception as e:
                print(f"Error in queue processor: {e}")
    
    def make_post_request_hf_tgi(self, endpoint_url, data, api_token=None, stream=False):
        """Make a request to HF API with queue and backoff"""
        # Use provided token or default
        if api_token is None:
            api_token = self.api_token
        
        # Generate unique request ID
        request_id = str(uuid.uuid4())
        
        # Queue system with proper concurrency management
        future = Future()
        
        # Add to queue
        self.request_queue.put((future, endpoint_url, data, api_token, stream, request_id))
        
        # Get result (blocks until request is processed)
        return future.result()
        
    def generate_text(self, model_id, inputs, parameters=None, api_token=None):
        """Generate text using HF TGI API"""
        # Format endpoint URL for the model
        endpoint_url = f"https://api-inference.huggingface.co/models/{model_id}"
        
        # Prepare data
        data = {"inputs": inputs}
        if parameters:
            data["parameters"] = parameters
        
        # Make request with queue and backoff
        response = self.make_post_request_hf_tgi(endpoint_url, data, api_token)
        
        # Process response based on format
        if isinstance(response, list):
            # Some models return list of generated texts
            return response[0] if response else {"generated_text": ""}
        elif isinstance(response, dict):
            # Some models return dict with generated_text key
            return response
        else:
            # Default fallback
            return {"generated_text": str(response)}

    def stream_generate(self, model_id, inputs, parameters=None, api_token=None):
        """Stream text generation from HF TGI API"""
        # Format endpoint URL for the model
        endpoint_url = f"https://api-inference.huggingface.co/models/{model_id}"
        
        # Prepare data
        data = {
            "inputs": inputs,
            "stream": True
        }
        if parameters:
            data["parameters"] = parameters
        
        # Make streaming request
        response_stream = self.make_post_request_hf_tgi(endpoint_url, data, api_token, stream=True)
        
        # Yield each chunk
        for chunk in response_stream:
            yield chunk
            
    def chat(self, model, messages, options=None):
        """Format chat messages and generate text"""
        # Format messages into a prompt
        prompt = self._format_chat_messages(messages)
        
        # Extract parameters from options
        parameters = {}
        if options:
            # Map common parameters
            if "temperature" in options:
                parameters["temperature"] = options["temperature"]
            if "max_tokens" in options:
                parameters["max_new_tokens"] = options["max_tokens"]
            if "top_p" in options:
                parameters["top_p"] = options["top_p"]
            if "top_k" in options:
                parameters["top_k"] = options["top_k"]
            
        # Generate text
        response = self.generate_text(model, prompt, parameters)
        
        # Extract text from response
        if isinstance(response, dict):
            text = response.get("generated_text", "")
        else:
            text = str(response)
            
        # Return standardized response
        return {
            "text": text,
            "model": model,
            "usage": self._estimate_usage(prompt, text),
            "implementation_type": "(REAL)"
        }
        
    def stream_chat(self, model, messages, options=None):
        """Stream chat responses"""
        # Format messages into a prompt
        prompt = self._format_chat_messages(messages)
        
        # Extract parameters from options
        parameters = {}
        if options:
            # Map common parameters
            if "temperature" in options:
                parameters["temperature"] = options["temperature"]
            if "max_tokens" in options:
                parameters["max_new_tokens"] = options["max_tokens"]
            if "top_p" in options:
                parameters["top_p"] = options["top_p"]
            if "top_k" in options:
                parameters["top_k"] = options["top_k"]
            
        # Create stream
        response_stream = self.stream_generate(model, prompt, parameters)
        
        # Process each chunk
        for chunk in response_stream:
            if isinstance(chunk, dict) and "token" in chunk:
                text = chunk.get("token", {}).get("text", "")
                done = chunk.get("generated_text", False) is not False
                
                yield {
                    "text": text,
                    "done": done,
                    "model": model
                }
            else:
                # Handle other response formats
                text = str(chunk)
                yield {
                    "text": text,
                    "done": False,
                    "model": model
                }
                
    def _format_chat_messages(self, messages):
        """Format chat messages into a prompt for HF models"""
        formatted_prompt = ""
        
        for message in messages:
            role = message.get("role", "user")
            content = message.get("content", "")
            
            if role == "system":
                formatted_prompt += f"<|system|>\n{content}\n"
            elif role == "assistant":
                formatted_prompt += f"<|assistant|>\n{content}\n"
            else:  # user or default
                formatted_prompt += f"<|user|>\n{content}\n"
                
        # Add final assistant marker for completion
        formatted_prompt += "<|assistant|>\n"
        
        return formatted_prompt
        
    def _estimate_usage(self, prompt, response):
        """Estimate token usage (rough approximation)"""
        # Very rough approximation: 4 chars ~= 1 token
        prompt_tokens = len(prompt) // 4
        completion_tokens = len(response) // 4
        
        return {
            "prompt_tokens": prompt_tokens,
            "completion_tokens": completion_tokens,
            "total_tokens": prompt_tokens + completion_tokens
        }
        
    def create_remote_text_generation_endpoint_handler(self, endpoint_url=None, api_key=None):
        """Create an endpoint handler for HF TGI remote inference"""
        async def endpoint_handler(prompt, **kwargs):
            """Handle requests to HF TGI endpoint"""
            # If no specific model endpoint provided, use standard API
            if not endpoint_url:
                # Extract model from kwargs or use default
                model = kwargs.get("model", self.default_model)
                
                # Create endpoint URL
                model_endpoint = f"https://api-inference.huggingface.co/models/{model}"
            else:
                model_endpoint = endpoint_url
                model = kwargs.get("model", self.default_model)
                
            # Extract parameters from kwargs
            max_new_tokens = kwargs.get("max_new_tokens", 1024)
            temperature = kwargs.get("temperature", 0.7)
            top_p = kwargs.get("top_p", 0.9)
            do_sample = kwargs.get("do_sample", True)
            
            # Prepare parameters
            parameters = {
                "max_new_tokens": max_new_tokens,
                "temperature": temperature,
                "top_p": top_p,
                "do_sample": do_sample,
                "return_full_text": kwargs.get("return_full_text", False)
            }
            
            # Check if prompt is a list of messages
            if isinstance(prompt, list):
                # Format as chat messages
                prompt_text = self._format_chat_messages(prompt)
            else:
                prompt_text = prompt
            
            # Use streaming if requested
            if kwargs.get("stream", False):
                async def process_stream():
                    streaming_response = self.stream_generate(model, prompt_text, parameters, api_key)
                    
                    for chunk in streaming_response:
                        if "token" in chunk:
                            yield {"text": chunk["token"]["text"], "done": False}
                        elif "generated_text" in chunk:
                            yield {"text": chunk["generated_text"], "done": True}
                        else:
                            yield {"text": str(chunk), "done": False}
                
                return process_stream()
            else:
                try:
                    # Make the request
                    response = self.generate_text(model, prompt_text, parameters, api_key)
                    
                    # Extract generated text
                    if isinstance(response, list):
                        generated_text = response[0].get("generated_text", "") if response else ""
                    elif isinstance(response, dict):
                        generated_text = response.get("generated_text", "")
                    else:
                        generated_text = str(response)
                    
                    usage = self._estimate_usage(prompt_text, generated_text)
                    
                    return {
                        "text": generated_text, 
                        "model": model,
                        "usage": usage,
                        "implementation_type": "(REAL)"
                    }
                except Exception as e:
                    print(f"Error calling HF TGI endpoint: {e}")
                    return {"text": f"Error: {str(e)}", "implementation_type": "(ERROR)"}
        
        return endpoint_handler
        
    def test_hf_tgi_endpoint(self, endpoint_url=None, api_token=None, model_id=None):
        """Test the HF TGI endpoint"""
        if not model_id:
            model_id = self.default_model
            
        if not endpoint_url:
            endpoint_url = f"https://api-inference.huggingface.co/models/{model_id}"
            
        if api_token is None:
            api_token = self.api_token
            
        try:
            response = self.generate_text(
                model_id, 
                "Testing the Hugging Face TGI API. Please respond with a short message.",
                {"max_new_tokens": 50}
            )
            
            if isinstance(response, dict) and "generated_text" in response:
                return True
            elif isinstance(response, list) and len(response) > 0:
                return True
            else:
                return False
        except Exception as e:
            print(f"Error testing HF TGI endpoint: {e}")
            return False''',
        
        "hf_tei": '''import os
import json
import time
import uuid
import threading
import requests
import numpy as np
from concurrent.futures import Future
from queue import Queue
from dotenv import load_dotenv

class hf_tei:
    def __init__(self, resources=None, metadata=None):
        self.resources = resources if resources else {}
        self.metadata = metadata if metadata else {}
        
        # Get HF API token from metadata or environment
        self.api_token = self._get_api_token()
        
        # Initialize queue and backoff systems
        self.max_concurrent_requests = 5
        self.queue_size = 100
        self.request_queue = Queue(maxsize=self.queue_size)
        self.active_requests = 0
        self.queue_lock = threading.RLock()
        
        # Start queue processor
        self.queue_processor = threading.Thread(target=self._process_queue)
        self.queue_processor.daemon = True
        self.queue_processor.start()
        
        # Initialize backoff configuration
        self.max_retries = 5
        self.initial_retry_delay = 1
        self.backoff_factor = 2
        self.max_retry_delay = 16
        
        # Default model
        self.default_model = "sentence-transformers/all-MiniLM-L6-v2"
        
        return None

    def _get_api_token(self):
        """Get Hugging Face API token from metadata or environment"""
        # Try to get from metadata
        api_token = self.metadata.get("hf_api_key") or self.metadata.get("hf_api_token")
        if api_token:
            return api_token
        
        # Try to get from environment
        env_token = os.environ.get("HF_API_KEY") or os.environ.get("HF_API_TOKEN")
        if env_token:
            return env_token
        
        # Try to load from dotenv
        try:
            load_dotenv()
            env_token = os.environ.get("HF_API_KEY") or os.environ.get("HF_API_TOKEN")
            if env_token:
                return env_token
        except ImportError:
            pass
        
        # Return None if no token found (will allow unauthenticated requests)
        return None
        
    def _process_queue(self):
        """Process queued requests with proper concurrency management"""
        while True:
            try:
                future, endpoint_url, data, api_token, request_id = self.request_queue.get()
                
                with self.queue_lock:
                    self.active_requests += 1
                
                # Process with retry logic
                retry_count = 0
                while retry_count <= self.max_retries:
                    try:
                        # Construct headers
                        headers = {"Content-Type": "application/json"}
                        if api_token:
                            headers["Authorization"] = f"Bearer {api_token}"
                        
                        # Make request with proper error handling
                        response = requests.post(
                            endpoint_url,
                            json=data,
                            headers=headers,
                            timeout=self.metadata.get("timeout", 30)
                        )
                        
                        # Handle specific status codes
                        if response.status_code == 401:
                            raise ValueError(f"Authentication error: Invalid API token (401)")
                        elif response.status_code == 404:
                            raise ValueError(f"Model not found: {endpoint_url} (404)")
                        elif response.status_code == 429:
                            retry_count += 1
                            
                            # Get retry delay from headers or use default
                            retry_after = int(response.headers.get("Retry-After", self.initial_retry_delay * (self.backoff_factor ** (retry_count - 1))))
                            time.sleep(min(retry_after, self.max_retry_delay))
                            continue
                        
                        # Check for other HTTP errors
                        response.raise_for_status()
                        
                        # Parse JSON response
                        result = response.json()
                        
                        future.set_result(result)
                        break
                        
                    except requests.RequestException as e:
                        retry_count += 1
                        
                        if retry_count > self.max_retries:
                            future.set_exception(e)
                            break
                        
                        # Calculate backoff delay
                        delay = min(
                            self.initial_retry_delay * (self.backoff_factor ** (retry_count - 1)),
                            self.max_retry_delay
                        )
                        
                        # Sleep with backoff
                        time.sleep(delay)
                    
                    except Exception as e:
                        future.set_exception(e)
                        break
                
                with self.queue_lock:
                    self.active_requests -= 1
                
                self.request_queue.task_done()
                
            except Exception as e:
                print(f"Error in queue processor: {e}")
    
    def make_post_request_hf_tei(self, endpoint_url, data, api_token=None):
        """Make a request to HF TEI API with queue and backoff"""
        # Use provided token or default
        if api_token is None:
            api_token = self.api_token
        
        # Generate unique request ID
        request_id = str(uuid.uuid4())
        
        # Queue system with proper concurrency management
        future = Future()
        
        # Add to queue
        self.request_queue.put((future, endpoint_url, data, api_token, request_id))
        
        # Get result (blocks until request is processed)
        return future.result()
        
    def generate_embedding(self, model_id, text, api_token=None):
        """Generate embeddings for a single text using HF TEI API"""
        # Format endpoint URL for the model
        endpoint_url = f"https://api-inference.huggingface.co/pipeline/feature-extraction/{model_id}"
        
        # Prepare data
        data = {"inputs": text}
        
        # Make request with queue and backoff
        response = self.make_post_request_hf_tei(endpoint_url, data, api_token)
        
        # Process response - normalize if needed
        return self.normalize_embedding(response)

    def batch_embed(self, model_id, texts, api_token=None):
        """Generate embeddings for multiple texts using HF TEI API"""
        # Format endpoint URL for the model
        endpoint_url = f"https://api-inference.huggingface.co/pipeline/feature-extraction/{model_id}"
        
        # Prepare data
        data = {"inputs": texts}
        
        # Make request with queue and backoff
        response = self.make_post_request_hf_tei(endpoint_url, data, api_token)
        
        # Process response - normalize if needed
        return [self.normalize_embedding(emb) for emb in response]

    def normalize_embedding(self, embedding):
        """Normalize embedding to unit length"""
        # Convert to numpy array if not already
        if not isinstance(embedding, np.ndarray):
            embedding = np.array(embedding)
        
        # Compute L2 norm
        norm = np.linalg.norm(embedding)
        
        # Normalize to unit length
        if norm > 0:
            normalized = embedding / norm
        else:
            normalized = embedding
        
        # Convert back to list for JSON serialization
        return normalized.tolist()

    def calculate_similarity(self, embedding1, embedding2):
        """Calculate cosine similarity between two embeddings"""
        # Convert to numpy arrays
        emb1 = np.array(embedding1)
        emb2 = np.array(embedding2)
        
        # Normalize if not already normalized
        norm1 = np.linalg.norm(emb1)
        norm2 = np.linalg.norm(emb2)
        
        if norm1 > 0:
            emb1 = emb1 / norm1
        if norm2 > 0:
            emb2 = emb2 / norm2
        
        # Calculate cosine similarity
        return np.dot(emb1, emb2)
        
    def create_remote_text_embedding_endpoint_handler(self, endpoint_url=None, api_key=None):
        """Create an endpoint handler for HF TEI remote inference"""
        async def endpoint_handler(text, **kwargs):
            """Handle requests to HF TEI endpoint"""
            try:
                # If no specific model endpoint provided, use standard API
                if not endpoint_url:
                    # Extract model from kwargs or use default
                    model = kwargs.get("model", self.default_model)
                    
                    # Create endpoint URL
                    model_endpoint = f"https://api-inference.huggingface.co/pipeline/feature-extraction/{model}"
                else:
                    model_endpoint = endpoint_url
                    model = kwargs.get("model", self.default_model)
                
                # Handle batch or single input
                if isinstance(text, list):
                    # Batch mode
                    embeddings = self.batch_embed(model, text, api_key)
                    
                    # Normalize if requested
                    if kwargs.get("normalize", True):
                        embeddings = [self.normalize_embedding(emb) for emb in embeddings]
                        
                    return {"embeddings": embeddings, "implementation_type": "(REAL)"}
                else:
                    # Single text mode
                    embedding = self.generate_embedding(model, text, api_key)
                    
                    # Normalize if requested
                    if kwargs.get("normalize", True):
                        embedding = self.normalize_embedding(embedding)
                        
                    return {"embedding": embedding, "implementation_type": "(REAL)"}
            except Exception as e:
                print(f"Error calling HF TEI endpoint: {e}")
                return {"error": str(e), "implementation_type": "(ERROR)"}
        
        return endpoint_handler
        
    def test_hf_tei_endpoint(self, endpoint_url=None, api_token=None, model_id=None):
        """Test the HF TEI endpoint"""
        if not model_id:
            model_id = self.default_model
            
        if not endpoint_url:
            endpoint_url = f"https://api-inference.huggingface.co/pipeline/feature-extraction/{model_id}"
            
        if api_token is None:
            api_token = self.api_token
            
        try:
            response = self.generate_embedding(
                model_id, 
                "Testing the Hugging Face TEI API."
            )
            
            if isinstance(response, list) and len(response) > 0:
                return True
            else:
                return False
        except Exception as e:
            print(f"Error testing HF TEI endpoint: {e}")
            return False''',
        
        "gemini": '''import os
import json
import time
import uuid
import threading
import requests
import base64
from concurrent.futures import Future
from queue import Queue
from dotenv import load_dotenv

class gemini:
    def __init__(self, resources=None, metadata=None):
        self.resources = resources if resources else {}
        self.metadata = metadata if metadata else {}
        
        # Get API key from metadata or environment
        self.api_key = self._get_api_key()
        
        # Set API base URL
        self.api_base = "https://generativelanguage.googleapis.com/v1"
        
        # Default model
        self.default_model = "gemini-1.5-pro"
        
        # Initialize queue and backoff systems
        self.max_concurrent_requests = 5
        self.queue_size = 100
        self.request_queue = Queue(maxsize=self.queue_size)
        self.active_requests = 0
        self.queue_lock = threading.RLock()
        
        # Start queue processor
        self.queue_processor = threading.Thread(target=self._process_queue)
        self.queue_processor.daemon = True
        self.queue_processor.start()
        
        # Initialize backoff configuration
        self.max_retries = 5
        self.initial_retry_delay = 1
        self.backoff_factor = 2
        self.max_retry_delay = 16
        
        # Request tracking
        self.request_tracking = True
        self.recent_requests = {}
        
        return None

    def _get_api_key(self):
        """Get Gemini API key from metadata or environment"""
        # Try to get from metadata
        api_key = self.metadata.get("gemini_api_key") or self.metadata.get("google_api_key")
        if api_key:
            return api_key
        
        # Try to get from environment
        env_key = os.environ.get("GEMINI_API_KEY") or os.environ.get("GOOGLE_API_KEY")
        if env_key:
            return env_key
        
        # Try to load from dotenv
        try:
            load_dotenv()
            env_key = os.environ.get("GEMINI_API_KEY") or os.environ.get("GOOGLE_API_KEY")
            if env_key:
                return env_key
        except ImportError:
            pass
        
        # Raise error if no key found
        raise ValueError("No Gemini API key found in metadata or environment")
        
    def _process_queue(self):
        """Process queued requests with proper concurrency management"""
        while True:
            try:
                future, data, stream, request_id = self.request_queue.get()
                
                with self.queue_lock:
                    self.active_requests += 1
                
                # Process with retry logic
                retry_count = 0
                while retry_count <= self.max_retries:
                    try:
                        # Prepare endpoint URL based on model
                        model = data.get("model", self.default_model)
                        if "model" in data:
                            del data["model"]  # Remove model from data as it goes in URL
                        
                        endpoint_url = f"{self.api_base}/models/{model}:{'streamGenerateContent' if stream else 'generateContent'}?key={self.api_key}"
                        
                        # Construct headers
                        headers = {
                            "Content-Type": "application/json",
                            "X-Request-ID": request_id
                        }
                        
                        # Make request with proper error handling
                        if stream:
                            response = requests.post(
                                endpoint_url,
                                json=data,
                                headers=headers,
                                stream=True,
                                timeout=self.metadata.get("timeout", 60)
                            )
                        else:
                            response = requests.post(
                                endpoint_url,
                                json=data,
                                headers=headers,
                                timeout=self.metadata.get("timeout", 60)
                            )
                        
                        # Handle specific status codes
                        if response.status_code == 401:
                            raise ValueError(f"Authentication error: Invalid API key (401)")
                        elif response.status_code == 404:
                            raise ValueError(f"Model not found: {model} (404)")
                        elif response.status_code == 429:
                            retry_count += 1
                            
                            # Calculate backoff delay
                            delay = min(
                                self.initial_retry_delay * (self.backoff_factor ** (retry_count - 1)),
                                self.max_retry_delay
                            )
                            
                            # Sleep with backoff
                            time.sleep(delay)
                            continue
                        
                        # Check for other HTTP errors
                        response.raise_for_status()
                        
                        # Return response based on stream mode
                        if stream:
                            # Create a streaming generator
                            def response_generator():
                                for line in response.iter_lines():
                                    if line:
                                        chunk = json.loads(line)
                                        yield chunk
                            
                            result = response_generator()
                        else:
                            # Parse JSON response
                            result = response.json()
                        
                        # Update tracking with response
                        if self.request_tracking and request_id in self.recent_requests:
                            self.recent_requests[request_id]["completed"] = True
                            self.recent_requests[request_id]["success"] = True
                            
                            # Clean up old requests
                            self._cleanup_old_requests()
                        
                        future.set_result(result)
                        break
                        
                    except requests.RequestException as e:
                        retry_count += 1
                        
                        if retry_count > self.max_retries:
                            # Update tracking with error
                            if self.request_tracking and request_id in self.recent_requests:
                                self.recent_requests[request_id]["completed"] = True
                                self.recent_requests[request_id]["success"] = False
                                self.recent_requests[request_id]["error"] = str(e)
                            
                            future.set_exception(e)
                            break
                        
                        # Calculate backoff delay
                        delay = min(
                            self.initial_retry_delay * (self.backoff_factor ** (retry_count - 1)),
                            self.max_retry_delay
                        )
                        
                        # Sleep with backoff
                        time.sleep(delay)
                    
                    except Exception as e:
                        # Update tracking with error
                        if self.request_tracking and request_id in self.recent_requests:
                            self.recent_requests[request_id]["completed"] = True
                            self.recent_requests[request_id]["success"] = False
                            self.recent_requests[request_id]["error"] = str(e)
                        
                        future.set_exception(e)
                        break
                
                with self.queue_lock:
                    self.active_requests -= 1
                
                self.request_queue.task_done()
                
            except Exception as e:
                print(f"Error in queue processor: {e}")

    def _cleanup_old_requests(self):
        """Clean up old request tracking data"""
        current_time = time.time()
        # Keep requests from last 30 minutes
        cutoff_time = current_time - 1800
        
        keys_to_remove = []
        for request_id, request_data in self.recent_requests.items():
            if request_data.get("timestamp", 0) < cutoff_time:
                keys_to_remove.append(request_id)
        
        for key in keys_to_remove:
            del self.recent_requests[key]
    
    def make_post_request_gemini(self, data, stream=False, request_id=None):
        """Make a request to Gemini API with queue and backoff"""
        # Generate unique request ID if not provided
        if request_id is None:
            request_id = str(uuid.uuid4())
        
        # Store request for tracking
        if self.request_tracking:
            self.recent_requests[request_id] = {
                "timestamp": time.time(),
                "data": data
            }
        
        # Queue system with proper concurrency management
        future = Future()
        
        # Add to queue
        self.request_queue.put((future, data, stream, request_id))
        
        # Get result (blocks until request is processed)
        return future.result()
        
    def chat(self, messages, model=None, **kwargs):
        """Send a chat request to Gemini API"""
        # Use specified model or default
        model = model or self.default_model
        
        # Format messages for Gemini API
        formatted_messages = self._format_messages(messages)
        
        # Prepare request data
        data = {
            "model": model,
            "contents": formatted_messages
        }
        
        # Add generation config if provided
        generation_config = {}
        for key in ["temperature", "topP", "topK", "maxOutputTokens"]:
            if key in kwargs:
                generation_config[key] = kwargs[key]
            elif key.lower() in kwargs:  # Handle snake_case keys too
                # Convert snake_case to camelCase
                snake_key = key.lower()
                generation_config[key] = kwargs[snake_key]
        
        if generation_config:
            data["generationConfig"] = generation_config
        
        # Make request with queue and backoff
        response = self.make_post_request_gemini(data)
        
        # Process and normalize response to match other APIs
        return {
            "text": self._extract_text(response),
            "model": model,
            "usage": self._extract_usage(response),
            "implementation_type": "(REAL)",
            "raw_response": response  # Include raw response for advanced use
        }

    def stream_chat(self, messages, model=None, **kwargs):
        """Stream a chat request from Gemini API"""
        # Use specified model or default
        model = model or self.default_model
        
        # Format messages for Gemini API
        formatted_messages = self._format_messages(messages)
        
        # Prepare request data
        data = {
            "model": model,
            "contents": formatted_messages
        }
        
        # Add generation config if provided
        generation_config = {}
        for key in ["temperature", "topP", "topK", "maxOutputTokens"]:
            if key in kwargs:
                generation_config[key] = kwargs[key]
            elif key.lower() in kwargs:  # Handle snake_case keys too
                # Convert snake_case to camelCase
                snake_key = key.lower()
                generation_config[key] = kwargs[snake_key]
        
        if generation_config:
            data["generationConfig"] = generation_config
        
        # Make streaming request
        response_stream = self.make_post_request_gemini(data, stream=True)
        
        # Process streaming response
        for chunk in response_stream:
            yield {
                "text": self._extract_text(chunk),
                "done": self._is_done(chunk),
                "model": model,
                "raw_chunk": chunk  # Include raw chunk for advanced use
            }

    def process_image(self, image_data, prompt, model=None, **kwargs):
        """Process an image with Gemini API"""
        # Use specified model or multimodal default
        model = model or "gemini-1.5-pro-vision"
        
        # Encode image data to base64
        if isinstance(image_data, bytes):
            encoded_image = base64.b64encode(image_data).decode('utf-8')
        else:
            # Assume it's already encoded
            encoded_image = image_data
        
        # Prepare content with text and image
        content = [
            {
                "role": "user",
                "parts": [
                    {"text": prompt},
                    {
                        "inline_data": {
                            "mime_type": kwargs.get("mime_type", "image/jpeg"),
                            "data": encoded_image
                        }
                    }
                ]
            }
        ]
        
        # Prepare request data
        data = {
            "model": model,
            "contents": content
        }
        
        # Add generation config if provided
        generation_config = {}
        for key in ["temperature", "topP", "topK", "maxOutputTokens"]:
            if key in kwargs:
                generation_config[key] = kwargs[key]
            elif key.lower() in kwargs:  # Handle snake_case keys too
                # Convert snake_case to camelCase
                snake_key = key.lower()
                generation_config[key] = kwargs[snake_key]
        
        if generation_config:
            data["generationConfig"] = generation_config
        
        # Make request with queue and backoff
        response = self.make_post_request_gemini(data)
        
        # Process and normalize response to match other APIs
        return {
            "text": self._extract_text(response),
            "model": model,
            "usage": self._extract_usage(response),
            "implementation_type": "(REAL)",
            "raw_response": response  # Include raw response for advanced use
        }
            
    def _format_messages(self, messages):
        """Format messages for Gemini API"""
        formatted_messages = []
        current_role = None
        current_parts = []
        
        for message in messages:
            role = message.get("role", "user")
            content = message.get("content", "")
            
            # Map standard roles to Gemini roles
            if role == "assistant":
                gemini_role = "model"
            elif role == "system":
                # For system messages, we add to user context
                gemini_role = "user"
            else:
                gemini_role = "user"
            
            # If role changes, add previous message
            if current_role and current_role != gemini_role and current_parts:
                formatted_messages.append({
                    "role": current_role,
                    "parts": current_parts
                })
                current_parts = []
            
            # Add content to parts
            current_role = gemini_role
            current_parts.append({"text": content})
        
        # Add final message
        if current_role and current_parts:
            formatted_messages.append({
                "role": current_role,
                "parts": current_parts
            })
        
        return formatted_messages

    def _extract_text(self, response):
        """Extract text from Gemini API response"""
        try:
            # Get candidates from response
            candidates = response.get("candidates", [])
            if not candidates:
                return ""
            
            # Get content from first candidate
            content = candidates[0].get("content", {})
            
            # Extract text from parts
            parts = content.get("parts", [])
            texts = [part.get("text", "") for part in parts if "text" in part]
            
            # Join all text parts
            return "".join(texts)
        except Exception as e:
            print(f"Error extracting text from response: {e}")
            return ""

    def _extract_usage(self, response):
        """Extract usage information from response"""
        try:
            # Get usage information from response
            usage = {"prompt_tokens": 0, "completion_tokens": 0, "total_tokens": 0}
            
            # Get candidates from response
            candidates = response.get("candidates", [])
            if not candidates:
                return usage
            
            # Get token count from first candidate
            token_count = candidates[0].get("tokenCount", {})
            
            # Extract token counts
            usage["prompt_tokens"] = token_count.get("inputTokens", 0)
            usage["completion_tokens"] = token_count.get("outputTokens", 0)
            usage["total_tokens"] = token_count.get("totalTokens", 0)
            
            return usage
        except Exception as e:
            print(f"Error extracting usage from response: {e}")
            return {"prompt_tokens": 0, "completion_tokens": 0, "total_tokens": 0}

    def _is_done(self, chunk):
        """Check if a streaming chunk indicates completion"""
        try:
            # Get candidates from chunk
            candidates = chunk.get("candidates", [])
            if not candidates:
                return False
            
            # Check finish reason from first candidate
            finish_reason = candidates[0].get("finishReason", None)
            
            # If finish reason is set, generation is done
            return finish_reason is not None
        except Exception:
            return False
            
    def create_gemini_endpoint_handler(self):
        """Create an endpoint handler for Gemini"""
        async def endpoint_handler(prompt, **kwargs):
            """Handle requests to Gemini endpoint"""
            try:
                # Extract model from kwargs or use default
                model = kwargs.get("model", self.default_model)
                
                # Check if prompt contains an image
                if isinstance(prompt, dict) and "image" in prompt:
                    # Process as image request
                    image_data = prompt["image"]
                    text_prompt = prompt.get("text", "Describe this image")
                    
                    response = self.process_image(image_data, text_prompt, model, **kwargs)
                    return response
                else:
                    # Create messages from prompt
                    if isinstance(prompt, list):
                        # Already formatted as messages
                        messages = prompt
                    else:
                        # Create a simple user message
                        messages = [{"role": "user", "content": prompt}]
                    
                    # Use streaming if requested
                    if kwargs.get("stream", False):
                        # For async streaming, need special handling
                        stream_response = self.stream_chat(messages, model, **kwargs)
                        
                        # Convert generator to async generator if needed
                        async def async_generator():
                            for chunk in stream_response:
                                yield chunk
                        
                        return async_generator()
                    else:
                        # Standard synchronous response
                        response = self.chat(messages, model, **kwargs)
                        return response
            except Exception as e:
                print(f"Error calling Gemini endpoint: {e}")
                return {"text": f"Error: {str(e)}", "implementation_type": "(ERROR)"}
        
        return endpoint_handler
        
    def test_gemini_endpoint(self, model=None):
        """Test the Gemini endpoint"""
        try:
            # Use specified model or default
            model = model or self.default_model
            
            # Create a simple message
            messages = [{"role": "user", "content": "Testing the Gemini API. Please respond with a short message."}]
            
            # Make the request
            response = self.chat(messages, model)
            
            # Check if the response contains text
            return "text" in response and response.get("implementation_type") == "(REAL)"
        except Exception as e:
            print(f"Error testing Gemini endpoint: {e}")
            return False'''
    }
    
    # Just use our standard templates
    templates = all_templates.copy()
    
    # Add the lower priority APIs directly
    # Add all remaining implementations
    templates["llvm"] = '''import os
import json
import time
import threading
import hashlib
import uuid
import requests
from concurrent.futures import Future
from queue import Queue
from dotenv import load_dotenv

class llvm:
    def __init__(self, resources=None, metadata=None):
        self.resources = resources if resources else {}
        self.metadata = metadata if metadata else {}
        
        # Get LLVM API endpoint from metadata or environment
        self.api_endpoint = self._get_api_endpoint()
        
        # Initialize queue and backoff systems
        self.max_concurrent_requests = 5
        self.queue_size = 100
        self.request_queue = Queue(maxsize=self.queue_size)
        self.active_requests = 0
        self.queue_lock = threading.RLock()
        
        # Start queue processor
        self.queue_processor = threading.Thread(target=self._process_queue)
        self.queue_processor.daemon = True
        self.queue_processor.start()
        
        # Initialize backoff configuration
        self.max_retries = 5
        self.initial_retry_delay = 1
        self.backoff_factor = 2
        self.max_retry_delay = 16
        
        # Request tracking
        self.request_tracking = True
        self.recent_requests = {}
        
        return None

    def _get_api_endpoint(self):
        """Get LLVM API endpoint from metadata or environment"""
        # Try to get from metadata
        api_endpoint = self.metadata.get("llvm_endpoint")
        if api_endpoint:
            return api_endpoint
        
        # Try to get from environment
        env_endpoint = os.environ.get("LLVM_API_ENDPOINT")
        if env_endpoint:
            return env_endpoint
        
        # Try to load from dotenv
        try:
            load_dotenv()
            env_endpoint = os.environ.get("LLVM_API_ENDPOINT")
            if env_endpoint:
                return env_endpoint
        except ImportError:
            pass
        
        # Return default if no endpoint found
        return "http://localhost:8080/v1"
        
    def _process_queue(self):
        """Process queued requests with proper concurrency management"""
        while True:
            try:
                future, endpoint_url, data, request_id = self.request_queue.get()
                
                with self.queue_lock:
                    self.active_requests += 1
                
                # Process with retry logic
                retry_count = 0
                while retry_count <= self.max_retries:
                    try:
                        # Construct headers
                        headers = {"Content-Type": "application/json"}
                        
                        # Make request with proper error handling
                        response = requests.post(
                            endpoint_url,
                            json=data,
                            headers=headers,
                            timeout=self.metadata.get("timeout", 30)
                        )
                        
                        # Check for HTTP errors
                        response.raise_for_status()
                        
                        # Parse JSON response
                        result = response.json()
                        
                        # Update tracking with response
                        if self.request_tracking:
                            self.recent_requests[request_id] = {
                                "timestamp": time.time(),
                                "endpoint": endpoint_url,
                                "status": "success",
                                "response_code": response.status_code
                            }
                        
                        future.set_result(result)
                        break
                        
                    except requests.RequestException as e:
                        retry_count += 1
                        
                        if retry_count > self.max_retries:
                            # Update tracking with error
                            if self.request_tracking:
                                self.recent_requests[request_id] = {
                                    "timestamp": time.time(),
                                    "endpoint": endpoint_url,
                                    "status": "error",
                                    "error": str(e)
                                }
                            
                            future.set_exception(e)
                            break
                        
                        # Calculate backoff delay
                        delay = min(
                            self.initial_retry_delay * (self.backoff_factor ** (retry_count - 1)),
                            self.max_retry_delay
                        )
                        
                        # Sleep with backoff
                        time.sleep(delay)
                    
                    except Exception as e:
                        # Update tracking with error
                        if self.request_tracking:
                            self.recent_requests[request_id] = {
                                "timestamp": time.time(),
                                "endpoint": endpoint_url,
                                "status": "error",
                                "error": str(e)
                            }
                        
                        future.set_exception(e)
                        break
                
                with self.queue_lock:
                    self.active_requests -= 1
                
                self.request_queue.task_done()
                
            except Exception as e:
                print(f"Error in queue processor: {e}")
    
    def make_post_request_llvm(self, endpoint_url, data, request_id=None):
        """Make a request to LLVM API with queue and backoff"""
        # Generate unique request ID if not provided
        if request_id is None:
            request_id = str(uuid.uuid4())
        
        # Queue system with proper concurrency management
        future = Future()
        
        # Add to queue
        self.request_queue.put((future, endpoint_url, data, request_id))
        
        # Get result (blocks until request is processed)
        return future.result()
        
    def execute_code(self, code, options=None):
        """Execute code using LLVM JIT compiler"""
        # Construct the proper endpoint URL
        endpoint_url = f"{self.api_endpoint}/execute"
        
        # Prepare request data
        data = {
            "code": code,
            "options": options or {}
        }
        
        # Make request with queue and backoff
        response = self.make_post_request_llvm(endpoint_url, data)
        
        # Process and normalize response
        return {
            "result": response.get("result", ""),
            "output": response.get("output", ""),
            "errors": response.get("errors", []),
            "execution_time": response.get("execution_time", 0),
            "implementation_type": "(REAL)"
        }

    def optimize_code(self, code, optimization_level=None, options=None):
        """Optimize code using LLVM optimizer"""
        # Construct the proper endpoint URL
        endpoint_url = f"{self.api_endpoint}/optimize"
        
        # Prepare request data
        data = {
            "code": code,
            "optimization_level": optimization_level or "O2",
            "options": options or {}
        }
        
        # Make request with queue and backoff
        response = self.make_post_request_llvm(endpoint_url, data)
        
        # Process and normalize response
        return {
            "optimized_code": response.get("optimized_code", ""),
            "optimization_passes": response.get("optimization_passes", []),
            "errors": response.get("errors", []),
            "implementation_type": "(REAL)"
        }
    
    def batch_execute(self, code_batch, options=None):
        """Execute multiple code snippets in batch"""
        # Construct the proper endpoint URL
        endpoint_url = f"{self.api_endpoint}/batch_execute"
        
        # Prepare request data
        data = {
            "code_batch": code_batch,
            "options": options or {}
        }
        
        # Make request with queue and backoff
        response = self.make_post_request_llvm(endpoint_url, data)
        
        # Process and normalize response
        return {
            "results": response.get("results", []),
            "errors": response.get("errors", []),
            "execution_times": response.get("execution_times", []),
            "implementation_type": "(REAL)"
        }
            
    def create_llvm_endpoint_handler(self, endpoint_url=None):
        """Create an endpoint handler for LLVM"""
        async def endpoint_handler(code, **kwargs):
            """Handle requests to LLVM endpoint"""
            # Use provided endpoint or default
            if not endpoint_url:
                actual_endpoint = self.api_endpoint
            else:
                actual_endpoint = endpoint_url
                
            # Extract options from kwargs
            options = {k: v for k, v in kwargs.items() if k not in ["optimization_level", "batch"]}
            
            # Check if batch operation
            if kwargs.get("batch", False) and isinstance(code, list):
                response = self.batch_execute(code, options)
            # Check if optimization request
            elif "optimization_level" in kwargs:
                response = self.optimize_code(code, kwargs["optimization_level"], options)
            else:
                # Standard execution
                response = self.execute_code(code, options)
            
            return response
        
        return endpoint_handler
        
    def test_llvm_endpoint(self, endpoint_url=None):
        """Test the LLVM endpoint"""
        if not endpoint_url:
            endpoint_url = f"{self.api_endpoint}/execute"
            
        # Simple C code to test execution
        test_code = """
        #include <stdio.h>
        int main() {
            printf("Hello from LLVM\\n");
            return 0;
        }
        """
        
        try:
            response = self.execute_code(test_code)
            return "result" in response and response.get("implementation_type") == "(REAL)"
        except Exception as e:
            print(f"Error testing LLVM endpoint: {e}")
            return False'''
    
    # Add all remaining API templates
    if api_name == "s3_kit":
        templates["s3_kit"] = '''import os
import time
import threading
import hashlib
import uuid
import boto3
from concurrent.futures import Future
from queue import Queue
from dotenv import load_dotenv

class s3_kit:
    def __init__(self, resources=None, metadata=None):
        self.resources = resources if resources else {}
        self.metadata = metadata if metadata else {}
        
        # Get S3 configuration from metadata or environment
        self.s3cfg = self._get_s3_config()
        
        # Initialize queue and backoff systems
        self.max_concurrent_requests = 5
        self.queue_size = 100
        self.request_queue = Queue(maxsize=self.queue_size)
        self.active_requests = 0
        self.queue_lock = threading.RLock()
        
        # Start queue processor
        self.queue_processor = threading.Thread(target=self._process_queue)
        self.queue_processor.daemon = True
        self.queue_processor.start()
        
        # Initialize backoff configuration
        self.max_retries = 5
        self.initial_retry_delay = 1
        self.backoff_factor = 2
        self.max_retry_delay = 16
        
        # Request tracking
        self.request_tracking = True
        self.recent_requests = {}
        
        return None

    def _get_s3_config(self):
        """Get S3 configuration from metadata or environment"""
        # Try to get from metadata
        if "s3cfg" in self.metadata:
            return self.metadata["s3cfg"]
        
        # Try to get from environment
        s3cfg = {
            "accessKey": os.environ.get("S3_ACCESS_KEY"),
            "secretKey": os.environ.get("S3_SECRET_KEY"),
            "endpoint": os.environ.get("S3_ENDPOINT", "https://s3.amazonaws.com")
        }
        
        # Check if we have essential keys
        if s3cfg["accessKey"] and s3cfg["secretKey"]:
            return s3cfg
        
        # Try to load from dotenv
        try:
            load_dotenv()
            s3cfg = {
                "accessKey": os.environ.get("S3_ACCESS_KEY"),
                "secretKey": os.environ.get("S3_SECRET_KEY"),
                "endpoint": os.environ.get("S3_ENDPOINT", "https://s3.amazonaws.com")
            }
            
            if s3cfg["accessKey"] and s3cfg["secretKey"]:
                return s3cfg
        except ImportError:
            pass
        
        # Return default with placeholders if no config found
        return {
            "accessKey": "default_access_key",
            "secretKey": "default_secret_key",
            "endpoint": "https://s3.amazonaws.com"
        }
        
    def _process_queue(self):
        """Process queued requests with proper concurrency management"""
        while True:
            try:
                future, operation, args, kwargs, request_id = self.request_queue.get()
                
                with self.queue_lock:
                    self.active_requests += 1
                
                # Process with retry logic
                retry_count = 0
                while retry_count <= self.max_retries:
                    try:
                        # Create S3 client
                        s3_client = self._get_s3_client()
                        
                        # Dispatch to appropriate operation
                        if operation == "upload_file":
                            result = s3_client.upload_file(**kwargs)
                        elif operation == "download_file":
                            result = s3_client.download_file(**kwargs)
                        elif operation == "list_objects":
                            result = s3_client.list_objects_v2(**kwargs)
                        elif operation == "delete_object":
                            result = s3_client.delete_object(**kwargs)
                        elif operation == "head_object":
                            result = s3_client.head_object(**kwargs)
                        else:
                            # For other operations, call the method dynamically
                            result = getattr(s3_client, operation)(**kwargs)
                        
                        # Update tracking with success
                        if self.request_tracking:
                            self.recent_requests[request_id] = {
                                "timestamp": time.time(),
                                "operation": operation,
                                "status": "success"
                            }
                        
                        future.set_result(result)
                        break
                        
                    except Exception as e:
                        retry_count += 1
                        
                        if retry_count > self.max_retries:
                            # Update tracking with error
                            if self.request_tracking:
                                self.recent_requests[request_id] = {
                                    "timestamp": time.time(),
                                    "operation": operation,
                                    "status": "error",
                                    "error": str(e)
                                }
                            
                            future.set_exception(e)
                            break
                        
                        # Calculate backoff delay
                        delay = min(
                            self.initial_retry_delay * (self.backoff_factor ** (retry_count - 1)),
                            self.max_retry_delay
                        )
                        
                        # Sleep with backoff
                        time.sleep(delay)
                
                with self.queue_lock:
                    self.active_requests -= 1
                
                self.request_queue.task_done()
                
            except Exception as e:
                print(f"Error in queue processor: {e}")
    
    def _get_s3_client(self):
        """Create an S3 client"""
        return boto3.client(
            's3',
            aws_access_key_id=self.s3cfg['accessKey'],
            aws_secret_access_key=self.s3cfg['secretKey'],
            endpoint_url=self.s3cfg['endpoint']
        )
    
    def _queue_operation(self, operation, **kwargs):
        """Queue an S3 operation with backoff retry"""
        # Generate unique request ID
        request_id = str(uuid.uuid4())
        
        # Queue system with proper concurrency management
        future = Future()
        
        # Add to queue
        self.request_queue.put((future, operation, [], kwargs, request_id))
        
        # Get result (blocks until request is processed)
        return future.result()
            
    def upload_file(self, file_path, bucket, key):
        """Upload a file to S3"""
        return self._queue_operation(
            "upload_file",
            Filename=file_path,
            Bucket=bucket,
            Key=key
        )
    
    def download_file(self, bucket, key, file_path):
        """Download a file from S3"""
        return self._queue_operation(
            "download_file",
            Bucket=bucket,
            Key=key,
            Filename=file_path
        )
    
    def list_objects(self, bucket, prefix=None):
        """List objects in a bucket"""
        kwargs = {"Bucket": bucket}
        if prefix:
            kwargs["Prefix"] = prefix
            
        return self._queue_operation("list_objects_v2", **kwargs)
    
    def delete_object(self, bucket, key):
        """Delete an object from S3"""
        return self._queue_operation(
            "delete_object",
            Bucket=bucket,
            Key=key
        )
    
    def head_object(self, bucket, key):
        """Get object metadata"""
        return self._queue_operation(
            "head_object",
            Bucket=bucket,
            Key=key
        )
    
    def create_presigned_url(self, bucket, key, expiration=3600):
        """Create a presigned URL for an object"""
        s3_client = self._get_s3_client()
        return s3_client.generate_presigned_url(
            'get_object',
            Params={'Bucket': bucket, 'Key': key},
            ExpiresIn=expiration
        )
        
    def create_s3_endpoint_handler(self, endpoint_url=None):
        """Create an endpoint handler for S3 operations"""
        # Use provided endpoint or default from config
        if endpoint_url:
            self.s3cfg["endpoint"] = endpoint_url
            
        async def endpoint_handler(operation, **kwargs):
            """Handle requests to S3 endpoint"""
            try:
                if operation == "upload_file":
                    return self.upload_file(
                        kwargs.get("file_path"),
                        kwargs.get("bucket"),
                        kwargs.get("key")
                    )
                elif operation == "download_file":
                    return self.download_file(
                        kwargs.get("bucket"),
                        kwargs.get("key"),
                        kwargs.get("file_path")
                    )
                elif operation == "list_objects":
                    return self.list_objects(
                        kwargs.get("bucket"),
                        kwargs.get("prefix")
                    )
                elif operation == "delete_object":
                    return self.delete_object(
                        kwargs.get("bucket"),
                        kwargs.get("key")
                    )
                elif operation == "head_object":
                    return self.head_object(
                        kwargs.get("bucket"),
                        kwargs.get("key")
                    )
                elif operation == "create_presigned_url":
                    return self.create_presigned_url(
                        kwargs.get("bucket"),
                        kwargs.get("key"),
                        kwargs.get("expiration", 3600)
                    )
                else:
                    return {"error": f"Unsupported operation: {operation}"}
            except Exception as e:
                print(f"Error handling S3 operation: {e}")
                return {"error": str(e)}
        
        return endpoint_handler
    
    def test_s3_endpoint(self, endpoint_url=None):
        """Test the S3 endpoint"""
        if endpoint_url:
            self.s3cfg["endpoint"] = endpoint_url
            
        try:
            # Create a simple test - just list buckets
            s3_client = self._get_s3_client()
            response = s3_client.list_buckets()
            return True
        except Exception as e:
            print(f"Error testing S3 endpoint: {e}")
            return False'''
    elif api_name == "opea":
        templates["opea"] = '''import os
import json
import time
import threading
import hashlib
import uuid
import requests
from concurrent.futures import Future
from queue import Queue
from dotenv import load_dotenv

class opea:
    def __init__(self, resources=None, metadata=None):
        self.resources = resources if resources else {}
        self.metadata = metadata if metadata else {}
        
        # Get OPEA API endpoint from metadata or environment
        self.api_endpoint = self._get_api_endpoint()
        
        # Initialize queue and backoff systems
        self.max_concurrent_requests = 5
        self.queue_size = 100
        self.request_queue = Queue(maxsize=self.queue_size)
        self.active_requests = 0
        self.queue_lock = threading.RLock()
        
        # Start queue processor
        self.queue_processor = threading.Thread(target=self._process_queue)
        self.queue_processor.daemon = True
        self.queue_processor.start()
        
        # Initialize backoff configuration
        self.max_retries = 5
        self.initial_retry_delay = 1
        self.backoff_factor = 2
        self.max_retry_delay = 16
        
        # Request tracking
        self.request_tracking = True
        self.recent_requests = {}
        
        return None

    def _get_api_endpoint(self):
        """Get OPEA API endpoint from metadata or environment"""
        # Try to get from metadata
        api_endpoint = self.metadata.get("opea_endpoint")
        if api_endpoint:
            return api_endpoint
        
        # Try to get from environment
        env_endpoint = os.environ.get("OPEA_API_ENDPOINT")
        if env_endpoint:
            return env_endpoint
        
        # Try to load from dotenv
        try:
            load_dotenv()
            env_endpoint = os.environ.get("OPEA_API_ENDPOINT")
            if env_endpoint:
                return env_endpoint
        except ImportError:
            pass
        
        # Return default if no endpoint found
        return "http://localhost:8000/v1"
        
    def _process_queue(self):
        """Process queued requests with proper concurrency management"""
        while True:
            try:
                future, endpoint_url, data, request_id = self.request_queue.get()
                
                with self.queue_lock:
                    self.active_requests += 1
                
                # Process with retry logic
                retry_count = 0
                while retry_count <= self.max_retries:
                    try:
                        # Construct headers
                        headers = {"Content-Type": "application/json"}
                        
                        # Make request with proper error handling
                        response = requests.post(
                            endpoint_url,
                            json=data,
                            headers=headers,
                            timeout=self.metadata.get("timeout", 30)
                        )
                        
                        # Check for HTTP errors
                        response.raise_for_status()
                        
                        # Parse JSON response
                        result = response.json()
                        
                        # Update tracking with response
                        if self.request_tracking:
                            self.recent_requests[request_id] = {
                                "timestamp": time.time(),
                                "endpoint": endpoint_url,
                                "status": "success",
                                "response_code": response.status_code
                            }
                        
                        future.set_result(result)
                        break
                        
                    except requests.RequestException as e:
                        retry_count += 1
                        
                        if retry_count > self.max_retries:
                            # Update tracking with error
                            if self.request_tracking:
                                self.recent_requests[request_id] = {
                                    "timestamp": time.time(),
                                    "endpoint": endpoint_url,
                                    "status": "error",
                                    "error": str(e)
                                }
                            
                            future.set_exception(e)
                            break
                        
                        # Calculate backoff delay
                        delay = min(
                            self.initial_retry_delay * (self.backoff_factor ** (retry_count - 1)),
                            self.max_retry_delay
                        )
                        
                        # Sleep with backoff
                        time.sleep(delay)
                    
                    except Exception as e:
                        # Update tracking with error
                        if self.request_tracking:
                            self.recent_requests[request_id] = {
                                "timestamp": time.time(),
                                "endpoint": endpoint_url,
                                "status": "error",
                                "error": str(e)
                            }
                        
                        future.set_exception(e)
                        break
                
                with self.queue_lock:
                    self.active_requests -= 1
                
                self.request_queue.task_done()
                
            except Exception as e:
                print(f"Error in queue processor: {e}")
    
    def make_post_request_opea(self, endpoint_url, data, request_id=None):
        """Make a request to OPEA API with queue and backoff"""
        # Generate unique request ID if not provided
        if request_id is None:
            request_id = str(uuid.uuid4())
        
        # Queue system with proper concurrency management
        future = Future()
        
        # Add to queue
        self.request_queue.put((future, endpoint_url, data, request_id))
        
        # Get result (blocks until request is processed)
        return future.result()
        
    def chat(self, messages, model=None, **kwargs):
        """Send a chat request to OPEA API"""
        # Construct the proper endpoint URL
        endpoint_url = f"{self.api_endpoint}/chat/completions"
        
        # Use provided model or default
        model = model or kwargs.get("model", "gpt-3.5-turbo")
        
        # Prepare request data
        data = {
            "model": model,
            "messages": messages
        }
        
        # Add optional parameters
        for key in ["temperature", "max_tokens", "top_p", "frequency_penalty", "presence_penalty", "stream"]:
            if key in kwargs:
                data[key] = kwargs[key]
        
        # Make request with queue and backoff
        response = self.make_post_request_opea(endpoint_url, data)
        
        # Extract text from response
        if "choices" in response and len(response["choices"]) > 0:
            text = response["choices"][0].get("message", {}).get("content", "")
        else:
            text = ""
        
        # Process and normalize response
        return {
            "text": text,
            "model": model,
            "usage": response.get("usage", {}),
            "implementation_type": "(REAL)",
            "raw_response": response  # Include raw response for advanced use
        }

    def stream_chat(self, messages, model=None, **kwargs):
        """Stream a chat request from OPEA API"""
        # Not implemented in this version - would need SSE streaming support
        raise NotImplementedError("Streaming not yet implemented for OPEA")
    
    def make_stream_request_opea(self, endpoint_url, data, request_id=None):
        """Make a streaming request to OPEA API"""
        # Not implemented in this version - would need SSE streaming support
        raise NotImplementedError("Streaming not yet implemented for OPEA")
            
    def create_opea_endpoint_handler(self):
        """Create an endpoint handler for OPEA"""
        async def endpoint_handler(prompt, **kwargs):
            """Handle requests to OPEA endpoint"""
            try:
                # Create messages from prompt
                if isinstance(prompt, list):
                    # Already formatted as messages
                    messages = prompt
                else:
                    # Create a simple user message
                    messages = [{"role": "user", "content": prompt}]
                
                # Extract model from kwargs or use default
                model = kwargs.get("model", "gpt-3.5-turbo")
                
                # Extract other parameters
                params = {k: v for k, v in kwargs.items() if k not in ["model"]}
                
                # Use streaming if requested
                if kwargs.get("stream", False):
                    raise NotImplementedError("Streaming not yet implemented for OPEA")
                else:
                    # Standard synchronous response
                    response = self.chat(messages, model, **params)
                    return response
            except Exception as e:
                print(f"Error calling OPEA endpoint: {e}")
                return {"text": f"Error: {str(e)}", "implementation_type": "(ERROR)"}
        
        return endpoint_handler
        
    def test_opea_endpoint(self, endpoint_url=None):
        """Test the OPEA endpoint"""
        if not endpoint_url:
            endpoint_url = f"{self.api_endpoint}/chat/completions"
            
        try:
            # Create a simple message
            messages = [{"role": "user", "content": "Testing the OPEA API. Please respond with a short message."}]
            
            # Make the request
            response = self.chat(messages)
            
            # Check if the response contains text
            return "text" in response and response.get("implementation_type") == "(REAL)"
        except Exception as e:
            print(f"Error testing OPEA endpoint: {e}")
            return False'''
    elif api_name == "ovms":
        templates["ovms"] = '''import os
import json
import time
import threading
import hashlib
import uuid
import requests
import numpy as np
from concurrent.futures import Future
from queue import Queue
from dotenv import load_dotenv

class ovms:
    def __init__(self, resources=None, metadata=None):
        self.resources = resources if resources else {}
        self.metadata = metadata if metadata else {}
        
        # Get OVMS configuration from metadata or environment
        self.api_url = self.metadata.get("ovms_api_url", os.environ.get("OVMS_API_URL", "http://localhost:9000"))
        self.default_model = self.metadata.get("ovms_model", os.environ.get("OVMS_MODEL", "model"))
        self.default_version = self.metadata.get("ovms_version", os.environ.get("OVMS_VERSION", "latest"))
        self.default_precision = self.metadata.get("ovms_precision", os.environ.get("OVMS_PRECISION", "FP32"))
        
        # Initialize queue and backoff systems
        self.max_concurrent_requests = 5
        self.queue_size = 100
        self.request_queue = Queue(maxsize=self.queue_size)
        self.active_requests = 0
        self.queue_lock = threading.RLock()
        
        # Start queue processor
        self.queue_processor = threading.Thread(target=self._process_queue)
        self.queue_processor.daemon = True
        self.queue_processor.start()
        
        # Initialize backoff configuration
        self.max_retries = 5
        self.initial_retry_delay = 1
        self.backoff_factor = 2
        self.max_retry_delay = 16
        
        # Request tracking
        self.request_tracking = True
        self.recent_requests = {}
        
        return None
        
    def _process_queue(self):
        """Process queued requests with proper concurrency management"""
        while True:
            try:
                future, endpoint_url, data, request_id = self.request_queue.get()
                
                with self.queue_lock:
                    self.active_requests += 1
                
                # Process with retry logic
                retry_count = 0
                while retry_count <= self.max_retries:
                    try:
                        # Construct headers
                        headers = {"Content-Type": "application/json"}
                        
                        # Make request with proper error handling
                        response = requests.post(
                            endpoint_url,
                            json=data,
                            headers=headers,
                            timeout=self.metadata.get("timeout", 30)
                        )
                        
                        # Check for HTTP errors
                        response.raise_for_status()
                        
                        # Parse JSON response
                        result = response.json()
                        
                        # Update tracking with response
                        if self.request_tracking:
                            self.recent_requests[request_id] = {
                                "timestamp": time.time(),
                                "endpoint": endpoint_url,
                                "status": "success",
                                "response_code": response.status_code
                            }
                        
                        future.set_result(result)
                        break
                        
                    except requests.RequestException as e:
                        retry_count += 1
                        
                        if retry_count > self.max_retries:
                            # Update tracking with error
                            if self.request_tracking:
                                self.recent_requests[request_id] = {
                                    "timestamp": time.time(),
                                    "endpoint": endpoint_url,
                                    "status": "error",
                                    "error": str(e)
                                }
                            
                            future.set_exception(e)
                            break
                        
                        # Calculate backoff delay
                        delay = min(
                            self.initial_retry_delay * (self.backoff_factor ** (retry_count - 1)),
                            self.max_retry_delay
                        )
                        
                        # Sleep with backoff
                        time.sleep(delay)
                    
                    except Exception as e:
                        # Update tracking with error
                        if self.request_tracking:
                            self.recent_requests[request_id] = {
                                "timestamp": time.time(),
                                "endpoint": endpoint_url,
                                "status": "error",
                                "error": str(e)
                            }
                        
                        future.set_exception(e)
                        break
                
                with self.queue_lock:
                    self.active_requests -= 1
                
                self.request_queue.task_done()
                
            except Exception as e:
                print(f"Error in queue processor: {e}")
    
    def make_post_request_ovms(self, endpoint_url, data, request_id=None):
        """Make a request to OVMS API with queue and backoff"""
        # Generate unique request ID if not provided
        if request_id is None:
            request_id = str(uuid.uuid4())
        
        # Queue system with proper concurrency management
        future = Future()
        
        # Add to queue
        self.request_queue.put((future, endpoint_url, data, request_id))
        
        # Get result (blocks until request is processed)
        return future.result()
    
    def format_request(self, handler, data, model=None, version=None):
        """Format a request for OVMS"""
        # Use default model and version if not provided
        model = model or self.default_model
        version = version or self.default_version
        
        # Format data based on its type
        if isinstance(data, dict):
            # If data is already formatted as expected by OVMS, use it directly
            if "instances" in data:
                formatted_data = data
            # If data has a 'data' field, wrap it in the OVMS format
            elif "data" in data:
                formatted_data = {"instances": [data]}
            # Otherwise, create a standard format
            else:
                formatted_data = {"instances": [{"data": data}]}
        elif isinstance(data, list):
            # If it's a list of objects with 'data' field, format as instances
            if len(data) > 0 and isinstance(data[0], dict) and "data" in data[0]:
                formatted_data = {"instances": data}
            # If it's a nested list (e.g., a batch of inputs)
            elif len(data) > 0 and isinstance(data[0], list):
                formatted_data = {"instances": [{"data": item} for item in data]}
            # Otherwise, treat as a single data array
            else:
                formatted_data = {"instances": [{"data": data}]}
        else:
            # For other data types, convert to list and wrap in standard format
            formatted_data = {"instances": [{"data": [data]}]}
        
        # Add model version if specified
        if version and version != "latest":
            formatted_data["version"] = version
        
        # Make the request
        return handler(formatted_data)
    
    def infer(self, model=None, data=None, version=None, precision=None):
        """Run inference on a model"""
        # Use defaults if not provided
        model = model or self.default_model
        version = version or self.default_version
        precision = precision or self.default_precision
        
        # Construct endpoint URL
        endpoint_url = f"{self.api_url}/v1/models/{model}:predict"
        
        # Create a handler function for this request
        def handler(formatted_data):
            return self.make_post_request_ovms(endpoint_url, formatted_data)
        
        # Format and send the request
        response = self.format_request(handler, data, model, version)
        
        # Process response
        if "predictions" in response:
            # Return just the predictions for simplicity
            return response["predictions"]
        else:
            # Return the full response if not in expected format
            return response
    
    def batch_infer(self, model=None, data_batch=None, version=None, precision=None):
        """Run batch inference on a model"""
        # Use defaults if not provided
        model = model or self.default_model
        version = version or self.default_version
        precision = precision or self.default_precision
        
        # Construct endpoint URL
        endpoint_url = f"{self.api_url}/v1/models/{model}:predict"
        
        # Format batch data
        if isinstance(data_batch, list):
            # Format each item in the batch
            formatted_data = {"instances": []}
            
            for item in data_batch:
                if isinstance(item, dict) and "data" in item:
                    # Already in the right format
                    formatted_data["instances"].append(item)
                else:
                    # Convert to standard format
                    formatted_data["instances"].append({"data": item})
        else:
            # Not a batch, treat as single instance
            formatted_data = {"instances": [{"data": data_batch}]}
        
        # Add version if specified
        if version and version != "latest":
            formatted_data["version"] = version
        
        # Make the request
        response = self.make_post_request_ovms(endpoint_url, formatted_data)
        
        # Process response
        if "predictions" in response:
            # Return just the predictions for simplicity
            return response["predictions"]
        else:
            # Return the full response if not in expected format
            return response
    
    def get_model_metadata(self, model=None, version=None):
        """Get model metadata"""
        # Use defaults if not provided
        model = model or self.default_model
        version = version or self.default_version
        
        # Construct endpoint URL
        endpoint_url = f"{self.api_url}/v1/models/{model}"
        if version and version != "latest":
            endpoint_url += f"/versions/{version}"
        endpoint_url += "/metadata"
        
        # Make the request
        try:
            response = requests.get(
                endpoint_url,
                headers={"Content-Type": "application/json"},
                timeout=self.metadata.get("timeout", 30)
            )
            response.raise_for_status()
            return response.json()
        except Exception as e:
            print(f"Error getting model metadata: {e}")
            return {"error": str(e)}
    
    def get_model_status(self, model=None, version=None):
        """Get model status"""
        # Use defaults if not provided
        model = model or self.default_model
        version = version or self.default_version
        
        # Construct endpoint URL
        endpoint_url = f"{self.api_url}/v1/models/{model}"
        if version and version != "latest":
            endpoint_url += f"/versions/{version}"
        
        # Make the request
        try:
            response = requests.get(
                endpoint_url,
                headers={"Content-Type": "application/json"},
                timeout=self.metadata.get("timeout", 30)
            )
            response.raise_for_status()
            return response.json()
        except Exception as e:
            print(f"Error getting model status: {e}")
            return {"error": str(e)}
    
    def get_server_statistics(self):
        """Get server statistics"""
        # Construct endpoint URL
        endpoint_url = f"{self.api_url}/v1/statistics"
        
        # Make the request
        try:
            response = requests.get(
                endpoint_url,
                headers={"Content-Type": "application/json"},
                timeout=self.metadata.get("timeout", 30)
            )
            response.raise_for_status()
            return response.json()
        except Exception as e:
            print(f"Error getting server statistics: {e}")
            return {"error": str(e)}
            
    def create_ovms_endpoint_handler(self, endpoint_url=None):
        """Create an endpoint handler for OVMS"""
        async def endpoint_handler(data, **kwargs):
            """Handle requests to OVMS endpoint"""
            try:
                # Use specified model or default
                model = kwargs.get("model", self.default_model)
                
                # Use specified version or default
                version = kwargs.get("version", self.default_version)
                
                # Use specified endpoint or construct from model
                if not endpoint_url:
                    # Construct endpoint URL for the model
                    actual_endpoint = f"{self.api_url}/v1/models/{model}:predict"
                else:
                    actual_endpoint = endpoint_url
                
                # Check if this is a batch request
                is_batch = kwargs.get("batch", False)
                
                if is_batch:
                    # Process as batch
                    return self.batch_infer(model, data, version)
                else:
                    # Process as single inference
                    return self.infer(model, data, version)
            except Exception as e:
                print(f"Error calling OVMS endpoint: {e}")
                return {"error": str(e), "implementation_type": "(ERROR)"}
        
        return endpoint_handler
        
    def test_ovms_endpoint(self, endpoint_url=None, model_name=None):
        """Test the OVMS endpoint"""
        # Use provided values or defaults
        model_name = model_name or self.default_model
        
        if not endpoint_url:
            endpoint_url = f"{self.api_url}/v1/models/{model_name}:predict"
            
        try:
            # Simple test data
            test_data = {"instances": [{"data": [1.0, 2.0, 3.0, 4.0]}]}
            
            # Make the request
            response = self.make_post_request_ovms(endpoint_url, test_data)
            
            # Check if the response contains predictions
            return "predictions" in response
        except Exception as e:
            print(f"Error testing OVMS endpoint: {e}")
            return False'''
    
    if api_name not in templates:
        print(f"Error: Unknown API '{api_name}'")
        return False
        
    os.makedirs(output_dir, exist_ok=True)
    file_path = os.path.join(output_dir, f"{api_name}.py")
    
    try:
        with open(file_path, 'w') as f:
            f.write(templates[api_name])
        print(f"Created implementation for {api_name} at {file_path}")
        return True
    except Exception as e:
        print(f"Error creating implementation file: {e}")
        return False

def main():
    parser = argparse.ArgumentParser(description="Fix all API backends with environment variables, queue, and backoff")
    parser.add_argument("--dry-run", "-d", action="store_true", help="Only print what would be done without making changes")
    parser.add_argument("--api", "-a", help="Only fix specific API backend", 
                        choices=["openai", "groq", "claude", "gemini", "ollama", "hf_tgi", "hf_tei", "llvm", "opea", "ovms", "s3_kit", "all"])
    parser.add_argument("--create-implementation", "-c", action="store_true", help="Create new REAL implementations for the selected APIs")
    parser.add_argument("--output-dir", "-o", help="Output directory for created implementations", default=None)
    
    args = parser.parse_args()
    
    # Define implementation templates for lower priority APIs
    llvm_template = '''import os
import json
import time
import threading
import hashlib
import uuid
import requests
from concurrent.futures import Future
from queue import Queue
from dotenv import load_dotenv

class llvm:
    def __init__(self, resources=None, metadata=None):
        self.resources = resources if resources else {}
        self.metadata = metadata if metadata else {}
        
        # Get LLVM API endpoint from metadata or environment
        self.api_endpoint = self._get_api_endpoint()
        
        # Initialize queue and backoff systems
        self.max_concurrent_requests = 5
        self.queue_size = 100
        self.request_queue = Queue(maxsize=self.queue_size)
        self.active_requests = 0
        self.queue_lock = threading.RLock()
        
        # Start queue processor
        self.queue_processor = threading.Thread(target=self._process_queue)
        self.queue_processor.daemon = True
        self.queue_processor.start()
        
        # Initialize backoff configuration
        self.max_retries = 5
        self.initial_retry_delay = 1
        self.backoff_factor = 2
        self.max_retry_delay = 16
        
        # Request tracking
        self.request_tracking = True
        self.recent_requests = {}
        
        return None

    def _get_api_endpoint(self):
        """Get LLVM API endpoint from metadata or environment"""
        # Try to get from metadata
        api_endpoint = self.metadata.get("llvm_endpoint")
        if api_endpoint:
            return api_endpoint
        
        # Try to get from environment
        env_endpoint = os.environ.get("LLVM_API_ENDPOINT")
        if env_endpoint:
            return env_endpoint
        
        # Try to load from dotenv
        try:
            load_dotenv()
            env_endpoint = os.environ.get("LLVM_API_ENDPOINT")
            if env_endpoint:
                return env_endpoint
        except ImportError:
            pass
        
        # Return default if no endpoint found
        return "http://localhost:8080/v1"
        
    def _process_queue(self):
        """Process queued requests with proper concurrency management"""
        while True:
            try:
                future, endpoint_url, data, request_id = self.request_queue.get()
                
                with self.queue_lock:
                    self.active_requests += 1
                
                # Process with retry logic
                retry_count = 0
                while retry_count <= self.max_retries:
                    try:
                        # Construct headers
                        headers = {"Content-Type": "application/json"}
                        
                        # Make request with proper error handling
                        response = requests.post(
                            endpoint_url,
                            json=data,
                            headers=headers,
                            timeout=self.metadata.get("timeout", 30)
                        )
                        
                        # Check for HTTP errors
                        response.raise_for_status()
                        
                        # Parse JSON response
                        result = response.json()
                        
                        # Update tracking with response
                        if self.request_tracking:
                            self.recent_requests[request_id] = {
                                "timestamp": time.time(),
                                "endpoint": endpoint_url,
                                "status": "success",
                                "response_code": response.status_code
                            }
                        
                        future.set_result(result)
                        break
                        
                    except requests.RequestException as e:
                        retry_count += 1
                        
                        if retry_count > self.max_retries:
                            # Update tracking with error
                            if self.request_tracking:
                                self.recent_requests[request_id] = {
                                    "timestamp": time.time(),
                                    "endpoint": endpoint_url,
                                    "status": "error",
                                    "error": str(e)
                                }
                            
                            future.set_exception(e)
                            break
                        
                        # Calculate backoff delay
                        delay = min(
                            self.initial_retry_delay * (self.backoff_factor ** (retry_count - 1)),
                            self.max_retry_delay
                        )
                        
                        # Sleep with backoff
                        time.sleep(delay)
                    
                    except Exception as e:
                        # Update tracking with error
                        if self.request_tracking:
                            self.recent_requests[request_id] = {
                                "timestamp": time.time(),
                                "endpoint": endpoint_url,
                                "status": "error",
                                "error": str(e)
                            }
                        
                        future.set_exception(e)
                        break
                
                with self.queue_lock:
                    self.active_requests -= 1
                
                self.request_queue.task_done()
                
            except Exception as e:
                print(f"Error in queue processor: {e}")
    
    def make_post_request_llvm(self, endpoint_url, data, request_id=None):
        """Make a request to LLVM API with queue and backoff"""
        # Generate unique request ID if not provided
        if request_id is None:
            request_id = str(uuid.uuid4())
        
        # Queue system with proper concurrency management
        future = Future()
        
        # Add to queue
        self.request_queue.put((future, endpoint_url, data, request_id))
        
        # Get result (blocks until request is processed)
        return future.result()
        
    def execute_code(self, code, options=None):
        """Execute code using LLVM JIT compiler"""
        # Construct the proper endpoint URL
        endpoint_url = f"{self.api_endpoint}/execute"
        
        # Prepare request data
        data = {
            "code": code,
            "options": options or {}
        }
        
        # Make request with queue and backoff
        response = self.make_post_request_llvm(endpoint_url, data)
        
        # Process and normalize response
        return {
            "result": response.get("result", ""),
            "output": response.get("output", ""),
            "errors": response.get("errors", []),
            "execution_time": response.get("execution_time", 0),
            "implementation_type": "(REAL)"
        }

    def optimize_code(self, code, optimization_level=None, options=None):
        """Optimize code using LLVM optimizer"""
        # Construct the proper endpoint URL
        endpoint_url = f"{self.api_endpoint}/optimize"
        
        # Prepare request data
        data = {
            "code": code,
            "optimization_level": optimization_level or "O2",
            "options": options or {}
        }
        
        # Make request with queue and backoff
        response = self.make_post_request_llvm(endpoint_url, data)
        
        # Process and normalize response
        return {
            "optimized_code": response.get("optimized_code", ""),
            "optimization_passes": response.get("optimization_passes", []),
            "errors": response.get("errors", []),
            "implementation_type": "(REAL)"
        }
    
    def batch_execute(self, code_batch, options=None):
        """Execute multiple code snippets in batch"""
        # Construct the proper endpoint URL
        endpoint_url = f"{self.api_endpoint}/batch_execute"
        
        # Prepare request data
        data = {
            "code_batch": code_batch,
            "options": options or {}
        }
        
        # Make request with queue and backoff
        response = self.make_post_request_llvm(endpoint_url, data)
        
        # Process and normalize response
        return {
            "results": response.get("results", []),
            "errors": response.get("errors", []),
            "execution_times": response.get("execution_times", []),
            "implementation_type": "(REAL)"
        }
            
    def create_llvm_endpoint_handler(self, endpoint_url=None):
        """Create an endpoint handler for LLVM"""
        async def endpoint_handler(code, **kwargs):
            """Handle requests to LLVM endpoint"""
            # Use provided endpoint or default
            if not endpoint_url:
                actual_endpoint = self.api_endpoint
            else:
                actual_endpoint = endpoint_url
                
            # Extract options from kwargs
            options = {k: v for k, v in kwargs.items() if k not in ["optimization_level", "batch"]}
            
            # Check if batch operation
            if kwargs.get("batch", False) and isinstance(code, list):
                response = self.batch_execute(code, options)
            # Check if optimization request
            elif "optimization_level" in kwargs:
                response = self.optimize_code(code, kwargs["optimization_level"], options)
            else:
                # Standard execution
                response = self.execute_code(code, options)
            
            return response
        
        return endpoint_handler
        
    def test_llvm_endpoint(self, endpoint_url=None):
        """Test the LLVM endpoint"""
        if not endpoint_url:
            endpoint_url = f"{self.api_endpoint}/execute"
            
        # Simple C code to test execution
        test_code = """
        #include <stdio.h>
        int main() {
            printf("Hello from LLVM\\n");
            return 0;
        }
        """
        
        try:
            response = self.execute_code(test_code)
            return "result" in response and response.get("implementation_type") == "(REAL)"
        except Exception as e:
            print(f"Error testing LLVM endpoint: {e}")
            return False'''
            
    s3_kit_template = '''import os
import time
import threading
import hashlib
import uuid
import boto3
from concurrent.futures import Future
from queue import Queue
from dotenv import load_dotenv

class s3_kit:
    def __init__(self, resources=None, metadata=None):
        self.resources = resources if resources else {}
        self.metadata = metadata if metadata else {}
        
        # Get S3 configuration from metadata or environment
        self.s3cfg = self._get_s3_config()
        
        # Initialize queue and backoff systems
        self.max_concurrent_requests = 5
        self.queue_size = 100
        self.request_queue = Queue(maxsize=self.queue_size)
        self.active_requests = 0
        self.queue_lock = threading.RLock()
        
        # Start queue processor
        self.queue_processor = threading.Thread(target=self._process_queue)
        self.queue_processor.daemon = True
        self.queue_processor.start()
        
        # Initialize backoff configuration
        self.max_retries = 5
        self.initial_retry_delay = 1
        self.backoff_factor = 2
        self.max_retry_delay = 16
        
        # Request tracking
        self.request_tracking = True
        self.recent_requests = {}
        
        return None

    def _get_s3_config(self):
        """Get S3 configuration from metadata or environment"""
        # Try to get from metadata
        if "s3cfg" in self.metadata:
            return self.metadata["s3cfg"]
        
        # Try to get from environment
        s3cfg = {
            "accessKey": os.environ.get("S3_ACCESS_KEY"),
            "secretKey": os.environ.get("S3_SECRET_KEY"),
            "endpoint": os.environ.get("S3_ENDPOINT", "https://s3.amazonaws.com")
        }
        
        # Check if we have essential keys
        if s3cfg["accessKey"] and s3cfg["secretKey"]:
            return s3cfg
        
        # Try to load from dotenv
        try:
            load_dotenv()
            s3cfg = {
                "accessKey": os.environ.get("S3_ACCESS_KEY"),
                "secretKey": os.environ.get("S3_SECRET_KEY"),
                "endpoint": os.environ.get("S3_ENDPOINT", "https://s3.amazonaws.com")
            }
            
            if s3cfg["accessKey"] and s3cfg["secretKey"]:
                return s3cfg
        except ImportError:
            pass
        
        # Return default with placeholders if no config found
        return {
            "accessKey": "default_access_key",
            "secretKey": "default_secret_key",
            "endpoint": "https://s3.amazonaws.com"
        }
        
    def _process_queue(self):
        """Process queued requests with proper concurrency management"""
        while True:
            try:
                future, operation, args, kwargs, request_id = self.request_queue.get()
                
                with self.queue_lock:
                    self.active_requests += 1
                
                # Process with retry logic
                retry_count = 0
                while retry_count <= self.max_retries:
                    try:
                        # Create S3 client
                        s3_client = self._get_s3_client()
                        
                        # Dispatch to appropriate operation
                        if operation == "upload_file":
                            result = s3_client.upload_file(**kwargs)
                        elif operation == "download_file":
                            result = s3_client.download_file(**kwargs)
                        elif operation == "list_objects":
                            result = s3_client.list_objects_v2(**kwargs)
                        elif operation == "delete_object":
                            result = s3_client.delete_object(**kwargs)
                        elif operation == "head_object":
                            result = s3_client.head_object(**kwargs)
                        else:
                            # For other operations, call the method dynamically
                            result = getattr(s3_client, operation)(**kwargs)
                        
                        # Update tracking with success
                        if self.request_tracking:
                            self.recent_requests[request_id] = {
                                "timestamp": time.time(),
                                "operation": operation,
                                "status": "success"
                            }
                        
                        future.set_result(result)
                        break
                        
                    except Exception as e:
                        retry_count += 1
                        
                        if retry_count > self.max_retries:
                            # Update tracking with error
                            if self.request_tracking:
                                self.recent_requests[request_id] = {
                                    "timestamp": time.time(),
                                    "operation": operation,
                                    "status": "error",
                                    "error": str(e)
                                }
                            
                            future.set_exception(e)
                            break
                        
                        # Calculate backoff delay
                        delay = min(
                            self.initial_retry_delay * (self.backoff_factor ** (retry_count - 1)),
                            self.max_retry_delay
                        )
                        
                        # Sleep with backoff
                        time.sleep(delay)
                
                with self.queue_lock:
                    self.active_requests -= 1
                
                self.request_queue.task_done()
                
            except Exception as e:
                print(f"Error in queue processor: {e}")
    
    def _get_s3_client(self):
        """Create an S3 client"""
        return boto3.client(
            's3',
            aws_access_key_id=self.s3cfg['accessKey'],
            aws_secret_access_key=self.s3cfg['secretKey'],
            endpoint_url=self.s3cfg['endpoint']
        )
    
    def _queue_operation(self, operation, **kwargs):
        """Queue an S3 operation with backoff retry"""
        # Generate unique request ID
        request_id = str(uuid.uuid4())
        
        # Queue system with proper concurrency management
        future = Future()
        
        # Add to queue
        self.request_queue.put((future, operation, [], kwargs, request_id))
        
        # Get result (blocks until request is processed)
        return future.result()
            
    def upload_file(self, file_path, bucket, key):
        """Upload a file to S3"""
        return self._queue_operation(
            "upload_file",
            Filename=file_path,
            Bucket=bucket,
            Key=key
        )
    
    def download_file(self, bucket, key, file_path):
        """Download a file from S3"""
        return self._queue_operation(
            "download_file",
            Bucket=bucket,
            Key=key,
            Filename=file_path
        )
    
    def list_objects(self, bucket, prefix=None):
        """List objects in a bucket"""
        kwargs = {"Bucket": bucket}
        if prefix:
            kwargs["Prefix"] = prefix
            
        return self._queue_operation("list_objects_v2", **kwargs)
    
    def delete_object(self, bucket, key):
        """Delete an object from S3"""
        return self._queue_operation(
            "delete_object",
            Bucket=bucket,
            Key=key
        )
    
    def head_object(self, bucket, key):
        """Get object metadata"""
        return self._queue_operation(
            "head_object",
            Bucket=bucket,
            Key=key
        )
    
    def create_presigned_url(self, bucket, key, expiration=3600):
        """Create a presigned URL for an object"""
        s3_client = self._get_s3_client()
        return s3_client.generate_presigned_url(
            'get_object',
            Params={'Bucket': bucket, 'Key': key},
            ExpiresIn=expiration
        )
        
    def create_s3_endpoint_handler(self, endpoint_url=None):
        """Create an endpoint handler for S3 operations"""
        # Use provided endpoint or default from config
        if endpoint_url:
            self.s3cfg["endpoint"] = endpoint_url
            
        async def endpoint_handler(operation, **kwargs):
            """Handle requests to S3 endpoint"""
            try:
                if operation == "upload_file":
                    return self.upload_file(
                        kwargs.get("file_path"),
                        kwargs.get("bucket"),
                        kwargs.get("key")
                    )
                elif operation == "download_file":
                    return self.download_file(
                        kwargs.get("bucket"),
                        kwargs.get("key"),
                        kwargs.get("file_path")
                    )
                elif operation == "list_objects":
                    return self.list_objects(
                        kwargs.get("bucket"),
                        kwargs.get("prefix")
                    )
                elif operation == "delete_object":
                    return self.delete_object(
                        kwargs.get("bucket"),
                        kwargs.get("key")
                    )
                elif operation == "head_object":
                    return self.head_object(
                        kwargs.get("bucket"),
                        kwargs.get("key")
                    )
                elif operation == "create_presigned_url":
                    return self.create_presigned_url(
                        kwargs.get("bucket"),
                        kwargs.get("key"),
                        kwargs.get("expiration", 3600)
                    )
                else:
                    return {"error": f"Unsupported operation: {operation}"}
            except Exception as e:
                print(f"Error handling S3 operation: {e}")
                return {"error": str(e)}
        
        return endpoint_handler
    
    def test_s3_endpoint(self, endpoint_url=None):
        """Test the S3 endpoint"""
        if endpoint_url:
            self.s3cfg["endpoint"] = endpoint_url
            
        try:
            # Create a simple test - just list buckets
            s3_client = self._get_s3_client()
            response = s3_client.list_buckets()
            return True
        except Exception as e:
            print(f"Error testing S3 endpoint: {e}")
            return False'''

    opea_template = '''import os
import json
import time
import threading
import hashlib
import uuid
import requests
from concurrent.futures import Future
from queue import Queue
from dotenv import load_dotenv

class opea:
    def __init__(self, resources=None, metadata=None):
        self.resources = resources if resources else {}
        self.metadata = metadata if metadata else {}
        
        # Get OPEA API endpoint from metadata or environment
        self.api_endpoint = self._get_api_endpoint()
        
        # Initialize queue and backoff systems
        self.max_concurrent_requests = 5
        self.queue_size = 100
        self.request_queue = Queue(maxsize=self.queue_size)
        self.active_requests = 0
        self.queue_lock = threading.RLock()
        
        # Start queue processor
        self.queue_processor = threading.Thread(target=self._process_queue)
        self.queue_processor.daemon = True
        self.queue_processor.start()
        
        # Initialize backoff configuration
        self.max_retries = 5
        self.initial_retry_delay = 1
        self.backoff_factor = 2
        self.max_retry_delay = 16
        
        # Request tracking
        self.request_tracking = True
        self.recent_requests = {}
        
        return None

    def _get_api_endpoint(self):
        """Get OPEA API endpoint from metadata or environment"""
        # Try to get from metadata
        api_endpoint = self.metadata.get("opea_endpoint")
        if api_endpoint:
            return api_endpoint
        
        # Try to get from environment
        env_endpoint = os.environ.get("OPEA_API_ENDPOINT")
        if env_endpoint:
            return env_endpoint
        
        # Try to load from dotenv
        try:
            load_dotenv()
            env_endpoint = os.environ.get("OPEA_API_ENDPOINT")
            if env_endpoint:
                return env_endpoint
        except ImportError:
            pass
        
        # Return default if no endpoint found
        return "http://localhost:8000/v1"
        
    def _process_queue(self):
        """Process queued requests with proper concurrency management"""
        while True:
            try:
                future, endpoint_url, data, request_id = self.request_queue.get()
                
                with self.queue_lock:
                    self.active_requests += 1
                
                # Process with retry logic
                retry_count = 0
                while retry_count <= self.max_retries:
                    try:
                        # Construct headers
                        headers = {"Content-Type": "application/json"}
                        
                        # Make request with proper error handling
                        response = requests.post(
                            endpoint_url,
                            json=data,
                            headers=headers,
                            timeout=self.metadata.get("timeout", 30)
                        )
                        
                        # Check for HTTP errors
                        response.raise_for_status()
                        
                        # Parse JSON response
                        result = response.json()
                        
                        # Update tracking with response
                        if self.request_tracking:
                            self.recent_requests[request_id] = {
                                "timestamp": time.time(),
                                "endpoint": endpoint_url,
                                "status": "success",
                                "response_code": response.status_code
                            }
                        
                        future.set_result(result)
                        break
                        
                    except requests.RequestException as e:
                        retry_count += 1
                        
                        if retry_count > self.max_retries:
                            # Update tracking with error
                            if self.request_tracking:
                                self.recent_requests[request_id] = {
                                    "timestamp": time.time(),
                                    "endpoint": endpoint_url,
                                    "status": "error",
                                    "error": str(e)
                                }
                            
                            future.set_exception(e)
                            break
                        
                        # Calculate backoff delay
                        delay = min(
                            self.initial_retry_delay * (self.backoff_factor ** (retry_count - 1)),
                            self.max_retry_delay
                        )
                        
                        # Sleep with backoff
                        time.sleep(delay)
                    
                    except Exception as e:
                        # Update tracking with error
                        if self.request_tracking:
                            self.recent_requests[request_id] = {
                                "timestamp": time.time(),
                                "endpoint": endpoint_url,
                                "status": "error",
                                "error": str(e)
                            }
                        
                        future.set_exception(e)
                        break
                
                with self.queue_lock:
                    self.active_requests -= 1
                
                self.request_queue.task_done()
                
            except Exception as e:
                print(f"Error in queue processor: {e}")
    
    def make_post_request_opea(self, endpoint_url, data, request_id=None):
        """Make a request to OPEA API with queue and backoff"""
        # Generate unique request ID if not provided
        if request_id is None:
            request_id = str(uuid.uuid4())
        
        # Queue system with proper concurrency management
        future = Future()
        
        # Add to queue
        self.request_queue.put((future, endpoint_url, data, request_id))
        
        # Get result (blocks until request is processed)
        return future.result()
        
    def chat(self, messages, model=None, **kwargs):
        """Send a chat request to OPEA API"""
        # Construct the proper endpoint URL
        endpoint_url = f"{self.api_endpoint}/chat/completions"
        
        # Use provided model or default
        model = model or kwargs.get("model", "gpt-3.5-turbo")
        
        # Prepare request data
        data = {
            "model": model,
            "messages": messages
        }
        
        # Add optional parameters
        for key in ["temperature", "max_tokens", "top_p", "frequency_penalty", "presence_penalty", "stream"]:
            if key in kwargs:
                data[key] = kwargs[key]
        
        # Make request with queue and backoff
        response = self.make_post_request_opea(endpoint_url, data)
        
        # Extract text from response
        if "choices" in response and len(response["choices"]) > 0:
            text = response["choices"][0].get("message", {}).get("content", "")
        else:
            text = ""
        
        # Process and normalize response
        return {
            "text": text,
            "model": model,
            "usage": response.get("usage", {}),
            "implementation_type": "(REAL)",
            "raw_response": response  # Include raw response for advanced use
        }

    def stream_chat(self, messages, model=None, **kwargs):
        """Stream a chat request from OPEA API"""
        # Not implemented in this version - would need SSE streaming support
        raise NotImplementedError("Streaming not yet implemented for OPEA")
    
    def make_stream_request_opea(self, endpoint_url, data, request_id=None):
        """Make a streaming request to OPEA API"""
        # Not implemented in this version - would need SSE streaming support
        raise NotImplementedError("Streaming not yet implemented for OPEA")
            
    def create_opea_endpoint_handler(self):
        """Create an endpoint handler for OPEA"""
        async def endpoint_handler(prompt, **kwargs):
            """Handle requests to OPEA endpoint"""
            try:
                # Create messages from prompt
                if isinstance(prompt, list):
                    # Already formatted as messages
                    messages = prompt
                else:
                    # Create a simple user message
                    messages = [{"role": "user", "content": prompt}]
                
                # Extract model from kwargs or use default
                model = kwargs.get("model", "gpt-3.5-turbo")
                
                # Extract other parameters
                params = {k: v for k, v in kwargs.items() if k not in ["model"]}
                
                # Use streaming if requested
                if kwargs.get("stream", False):
                    raise NotImplementedError("Streaming not yet implemented for OPEA")
                else:
                    # Standard synchronous response
                    response = self.chat(messages, model, **params)
                    return response
            except Exception as e:
                print(f"Error calling OPEA endpoint: {e}")
                return {"text": f"Error: {str(e)}", "implementation_type": "(ERROR)"}
        
        return endpoint_handler
        
    def test_opea_endpoint(self, endpoint_url=None):
        """Test the OPEA endpoint"""
        if not endpoint_url:
            endpoint_url = f"{self.api_endpoint}/chat/completions"
            
        try:
            # Create a simple message
            messages = [{"role": "user", "content": "Testing the OPEA API. Please respond with a short message."}]
            
            # Make the request
            response = self.chat(messages)
            
            # Check if the response contains text
            return "text" in response and response.get("implementation_type") == "(REAL)"
        except Exception as e:
            print(f"Error testing OPEA endpoint: {e}")
            return False'''

    ovms_template = '''import os
import json
import time
import threading
import hashlib
import uuid
import requests
import numpy as np
from concurrent.futures import Future
from queue import Queue
from dotenv import load_dotenv

class ovms:
    def __init__(self, resources=None, metadata=None):
        self.resources = resources if resources else {}
        self.metadata = metadata if metadata else {}
        
        # Get OVMS configuration from metadata or environment
        self.api_url = self.metadata.get("ovms_api_url", os.environ.get("OVMS_API_URL", "http://localhost:9000"))
        self.default_model = self.metadata.get("ovms_model", os.environ.get("OVMS_MODEL", "model"))
        self.default_version = self.metadata.get("ovms_version", os.environ.get("OVMS_VERSION", "latest"))
        self.default_precision = self.metadata.get("ovms_precision", os.environ.get("OVMS_PRECISION", "FP32"))
        
        # Initialize queue and backoff systems
        self.max_concurrent_requests = 5
        self.queue_size = 100
        self.request_queue = Queue(maxsize=self.queue_size)
        self.active_requests = 0
        self.queue_lock = threading.RLock()
        
        # Start queue processor
        self.queue_processor = threading.Thread(target=self._process_queue)
        self.queue_processor.daemon = True
        self.queue_processor.start()
        
        # Initialize backoff configuration
        self.max_retries = 5
        self.initial_retry_delay = 1
        self.backoff_factor = 2
        self.max_retry_delay = 16
        
        # Request tracking
        self.request_tracking = True
        self.recent_requests = {}
        
        return None
        
    def _process_queue(self):
        """Process queued requests with proper concurrency management"""
        while True:
            try:
                future, endpoint_url, data, request_id = self.request_queue.get()
                
                with self.queue_lock:
                    self.active_requests += 1
                
                # Process with retry logic
                retry_count = 0
                while retry_count <= self.max_retries:
                    try:
                        # Construct headers
                        headers = {"Content-Type": "application/json"}
                        
                        # Make request with proper error handling
                        response = requests.post(
                            endpoint_url,
                            json=data,
                            headers=headers,
                            timeout=self.metadata.get("timeout", 30)
                        )
                        
                        # Check for HTTP errors
                        response.raise_for_status()
                        
                        # Parse JSON response
                        result = response.json()
                        
                        # Update tracking with response
                        if self.request_tracking:
                            self.recent_requests[request_id] = {
                                "timestamp": time.time(),
                                "endpoint": endpoint_url,
                                "status": "success",
                                "response_code": response.status_code
                            }
                        
                        future.set_result(result)
                        break
                        
                    except requests.RequestException as e:
                        retry_count += 1
                        
                        if retry_count > self.max_retries:
                            # Update tracking with error
                            if self.request_tracking:
                                self.recent_requests[request_id] = {
                                    "timestamp": time.time(),
                                    "endpoint": endpoint_url,
                                    "status": "error",
                                    "error": str(e)
                                }
                            
                            future.set_exception(e)
                            break
                        
                        # Calculate backoff delay
                        delay = min(
                            self.initial_retry_delay * (self.backoff_factor ** (retry_count - 1)),
                            self.max_retry_delay
                        )
                        
                        # Sleep with backoff
                        time.sleep(delay)
                    
                    except Exception as e:
                        # Update tracking with error
                        if self.request_tracking:
                            self.recent_requests[request_id] = {
                                "timestamp": time.time(),
                                "endpoint": endpoint_url,
                                "status": "error",
                                "error": str(e)
                            }
                        
                        future.set_exception(e)
                        break
                
                with self.queue_lock:
                    self.active_requests -= 1
                
                self.request_queue.task_done()
                
            except Exception as e:
                print(f"Error in queue processor: {e}")
    
    def make_post_request_ovms(self, endpoint_url, data, request_id=None):
        """Make a request to OVMS API with queue and backoff"""
        # Generate unique request ID if not provided
        if request_id is None:
            request_id = str(uuid.uuid4())
        
        # Queue system with proper concurrency management
        future = Future()
        
        # Add to queue
        self.request_queue.put((future, endpoint_url, data, request_id))
        
        # Get result (blocks until request is processed)
        return future.result()
    
    def format_request(self, handler, data, model=None, version=None):
        """Format a request for OVMS"""
        # Use default model and version if not provided
        model = model or self.default_model
        version = version or self.default_version
        
        # Format data based on its type
        if isinstance(data, dict):
            # If data is already formatted as expected by OVMS, use it directly
            if "instances" in data:
                formatted_data = data
            # If data has a 'data' field, wrap it in the OVMS format
            elif "data" in data:
                formatted_data = {"instances": [data]}
            # Otherwise, create a standard format
            else:
                formatted_data = {"instances": [{"data": data}]}
        elif isinstance(data, list):
            # If it's a list of objects with 'data' field, format as instances
            if len(data) > 0 and isinstance(data[0], dict) and "data" in data[0]:
                formatted_data = {"instances": data}
            # If it's a nested list (e.g., a batch of inputs)
            elif len(data) > 0 and isinstance(data[0], list):
                formatted_data = {"instances": [{"data": item} for item in data]}
            # Otherwise, treat as a single data array
            else:
                formatted_data = {"instances": [{"data": data}]}
        else:
            # For other data types, convert to list and wrap in standard format
            formatted_data = {"instances": [{"data": [data]}]}
        
        # Add model version if specified
        if version and version != "latest":
            formatted_data["version"] = version
        
        # Make the request
        return handler(formatted_data)
    
    def infer(self, model=None, data=None, version=None, precision=None):
        """Run inference on a model"""
        # Use defaults if not provided
        model = model or self.default_model
        version = version or self.default_version
        precision = precision or self.default_precision
        
        # Construct endpoint URL
        endpoint_url = f"{self.api_url}/v1/models/{model}:predict"
        
        # Create a handler function for this request
        def handler(formatted_data):
            return self.make_post_request_ovms(endpoint_url, formatted_data)
        
        # Format and send the request
        response = self.format_request(handler, data, model, version)
        
        # Process response
        if "predictions" in response:
            # Return just the predictions for simplicity
            return response["predictions"]
        else:
            # Return the full response if not in expected format
            return response
    
    def batch_infer(self, model=None, data_batch=None, version=None, precision=None):
        """Run batch inference on a model"""
        # Use defaults if not provided
        model = model or self.default_model
        version = version or self.default_version
        precision = precision or self.default_precision
        
        # Construct endpoint URL
        endpoint_url = f"{self.api_url}/v1/models/{model}:predict"
        
        # Format batch data
        if isinstance(data_batch, list):
            # Format each item in the batch
            formatted_data = {"instances": []}
            
            for item in data_batch:
                if isinstance(item, dict) and "data" in item:
                    # Already in the right format
                    formatted_data["instances"].append(item)
                else:
                    # Convert to standard format
                    formatted_data["instances"].append({"data": item})
        else:
            # Not a batch, treat as single instance
            formatted_data = {"instances": [{"data": data_batch}]}
        
        # Add version if specified
        if version and version != "latest":
            formatted_data["version"] = version
        
        # Make the request
        response = self.make_post_request_ovms(endpoint_url, formatted_data)
        
        # Process response
        if "predictions" in response:
            # Return just the predictions for simplicity
            return response["predictions"]
        else:
            # Return the full response if not in expected format
            return response
    
    def get_model_metadata(self, model=None, version=None):
        """Get model metadata"""
        # Use defaults if not provided
        model = model or self.default_model
        version = version or self.default_version
        
        # Construct endpoint URL
        endpoint_url = f"{self.api_url}/v1/models/{model}"
        if version and version != "latest":
            endpoint_url += f"/versions/{version}"
        endpoint_url += "/metadata"
        
        # Make the request
        try:
            response = requests.get(
                endpoint_url,
                headers={"Content-Type": "application/json"},
                timeout=self.metadata.get("timeout", 30)
            )
            response.raise_for_status()
            return response.json()
        except Exception as e:
            print(f"Error getting model metadata: {e}")
            return {"error": str(e)}
    
    def get_model_status(self, model=None, version=None):
        """Get model status"""
        # Use defaults if not provided
        model = model or self.default_model
        version = version or self.default_version
        
        # Construct endpoint URL
        endpoint_url = f"{self.api_url}/v1/models/{model}"
        if version and version != "latest":
            endpoint_url += f"/versions/{version}"
        
        # Make the request
        try:
            response = requests.get(
                endpoint_url,
                headers={"Content-Type": "application/json"},
                timeout=self.metadata.get("timeout", 30)
            )
            response.raise_for_status()
            return response.json()
        except Exception as e:
            print(f"Error getting model status: {e}")
            return {"error": str(e)}
    
    def get_server_statistics(self):
        """Get server statistics"""
        # Construct endpoint URL
        endpoint_url = f"{self.api_url}/v1/statistics"
        
        # Make the request
        try:
            response = requests.get(
                endpoint_url,
                headers={"Content-Type": "application/json"},
                timeout=self.metadata.get("timeout", 30)
            )
            response.raise_for_status()
            return response.json()
        except Exception as e:
            print(f"Error getting server statistics: {e}")
            return {"error": str(e)}
            
    def create_ovms_endpoint_handler(self, endpoint_url=None):
        """Create an endpoint handler for OVMS"""
        async def endpoint_handler(data, **kwargs):
            """Handle requests to OVMS endpoint"""
            try:
                # Use specified model or default
                model = kwargs.get("model", self.default_model)
                
                # Use specified version or default
                version = kwargs.get("version", self.default_version)
                
                # Use specified endpoint or construct from model
                if not endpoint_url:
                    # Construct endpoint URL for the model
                    actual_endpoint = f"{self.api_url}/v1/models/{model}:predict"
                else:
                    actual_endpoint = endpoint_url
                
                # Check if this is a batch request
                is_batch = kwargs.get("batch", False)
                
                if is_batch:
                    # Process as batch
                    return self.batch_infer(model, data, version)
                else:
                    # Process as single inference
                    return self.infer(model, data, version)
            except Exception as e:
                print(f"Error calling OVMS endpoint: {e}")
                return {"error": str(e), "implementation_type": "(ERROR)"}
        
        return endpoint_handler
        
    def test_ovms_endpoint(self, endpoint_url=None, model_name=None):
        """Test the OVMS endpoint"""
        # Use provided values or defaults
        model_name = model_name or self.default_model
        
        if not endpoint_url:
            endpoint_url = f"{self.api_url}/v1/models/{model_name}:predict"
            
        try:
            # Simple test data
            test_data = {"instances": [{"data": [1.0, 2.0, 3.0, 4.0]}]}
            
            # Make the request
            response = self.make_post_request_ovms(endpoint_url, test_data)
            
            # Check if the response contains predictions
            return "predictions" in response
        except Exception as e:
            print(f"Error testing OVMS endpoint: {e}")
            return False'''
    
    # Determine script directory
    script_dir = Path(__file__).parent
    
    # Create .env.example file
    create_env_file()
    
    # Fix specific API or all APIs
    success_count = 0
    failure_count = 0
    
    # Create REAL implementations if requested
    if args.create_implementation:
        # Determine output directory
        output_dir = args.output_dir
        if not output_dir:
            output_dir = script_dir.parent / "ipfs_accelerate_py" / "api_backends"
            
        print(f"\n=== Creating REAL API implementations in {output_dir} ===")
        
        # Determine which APIs to implement
        apis_to_implement = []
        if args.api == "all":
            apis_to_implement = ["ollama", "hf_tgi", "hf_tei", "gemini", "llvm", "opea", "ovms", "s3_kit"]
        elif args.api in ["ollama", "hf_tgi", "hf_tei", "gemini", "llvm", "opea", "ovms", "s3_kit"]:
            apis_to_implement = [args.api]
        
        # Define implementation templates dictionary
        templates = {}
        
        # Add templates from the create_api_implementation function
        if "ollama" in apis_to_implement:
            templates["ollama"] = templates.get("ollama", "")
        if "hf_tgi" in apis_to_implement:
            templates["hf_tgi"] = templates.get("hf_tgi", "")
        if "hf_tei" in apis_to_implement:
            templates["hf_tei"] = templates.get("hf_tei", "")
        if "gemini" in apis_to_implement:
            templates["gemini"] = templates.get("gemini", "")
        
        # Add templates for lower priority APIs
        if "llvm" in apis_to_implement:
            templates["llvm"] = llvm_template
        if "opea" in apis_to_implement:
            templates["opea"] = opea_template
        if "ovms" in apis_to_implement:
            templates["ovms"] = ovms_template
        if "s3_kit" in apis_to_implement:
            templates["s3_kit"] = s3_kit_template
            
        # Create implementations
        for api_name in apis_to_implement:
            if args.dry_run:
                print(f"Would create implementation for {api_name}")
            else:
                if create_api_implementation(api_name, output_dir):
                    success_count += 1
                else:
                    failure_count += 1
    
    # 1. Fix OpenAI API implementation
    if args.api in ["openai", "all"]:
        print("\n=== Fixing OpenAI API implementation ===")
        script_path = script_dir / "fix_openai_api_implementation.py"
        script_args = ["--dry-run"] if args.dry_run else []
        
        if run_script(script_path, script_args):
            success_count += 1
        else:
            failure_count += 1
    
    # 2. Update OpenAI API tests
    if args.api in ["openai", "all"] and not args.dry_run:
        print("\n=== Updating OpenAI API tests ===")
        script_path = script_dir / "update_openai_api_tests.py"
        
        if run_script(script_path):
            success_count += 1
        else:
            failure_count += 1
    
    # 3. Fix all other API backends using general script
    if args.api in ["groq", "claude", "gemini", "ollama", "hf_tgi", "hf_tei", "all"]:
        print("\n=== Fixing other API backends ===")
        script_path = script_dir / "add_queue_backoff.py"
        
        # Determine which APIs to fix
        api_arg = args.api if args.api != "all" else "all"
        script_args = ["--api", api_arg]
        
        if args.dry_run:
            script_args.append("--dry-run")
        
        if run_script(script_path, script_args):
            success_count += 1
        else:
            failure_count += 1
    
    # 4. Update API tests for all backends
    if args.api != "openai" and not args.dry_run:
        print("\n=== Updating other API tests ===")
        script_path = script_dir / "update_api_tests.py"
        
        # Determine which APIs to update
        api_arg = args.api if args.api != "all" else "all"
        script_args = ["--api", api_arg]
        
        if run_script(script_path, script_args):
            success_count += 1
        else:
            failure_count += 1
    
    # 5. Create and run test script for backoff and queue
    if not args.dry_run and args.api in ["ollama", "hf_tgi", "hf_tei", "gemini", "all"]:
        print("\n=== Testing API backoff and queue functionality ===")
        backoff_test_path = script_dir / "test_api_backoff_queue.py"
        
        # Run test for each API that needs it
        apis_to_test = []
        if args.api == "all":
            apis_to_test = ["ollama", "hf_tgi", "hf_tei", "gemini"]
        elif args.api in ["ollama", "hf_tgi", "hf_tei", "gemini"]:
            apis_to_test = [args.api]
            
        for api_name in apis_to_test:
            print(f"\nTesting {api_name} API backoff and queue...")
            if run_script(backoff_test_path, ["--api", api_name]):
                success_count += 1
            else:
                failure_count += 1
    
    # Print summary
    print("\n=== Summary ===")
    if args.dry_run:
        print("Dry run completed - no files were modified")
    else:
        print(f"Successfully completed {success_count} operations")
        if failure_count > 0:
            print(f"Failed to complete {failure_count} operations")
    
    # Final instructions
    if not args.dry_run and success_count > 0:
        print("\n=== Next Steps ===")
        print("1. Install required dependencies:")
        print("   pip install -r requirements_api.txt")
        print("2. Create a .env file with your API keys based on .env.example")
        print("3. Run tests to verify the implementation:")
        print("   python test_api_backend.py")
        
        if args.create_implementation:
            print("\n=== API Implementation Status ===")
            print("The following APIs now have REAL implementations:")
            if args.api == "all" or args.api == "ollama":
                print("✅ Ollama API: REAL implementation")
            if args.api == "all" or args.api == "hf_tgi":
                print("✅ Hugging Face TGI API: REAL implementation")
            if args.api == "all" or args.api == "hf_tei":
                print("✅ Hugging Face TEI API: REAL implementation")
            if args.api == "all" or args.api == "gemini":
                print("✅ Google Gemini API: REAL implementation")
            if args.api == "all" or args.api == "llvm":
                print("✅ LLVM API: REAL implementation")
            if args.api == "all" or args.api == "opea":
                print("✅ OPEA API: REAL implementation")
            if args.api == "all" or args.api == "ovms":
                print("✅ OVMS API: REAL implementation")
            if args.api == "all" or args.api == "s3_kit":
                print("✅ S3 Kit API: REAL implementation")
            
            print("\nYou can now test these implementations with:")
            print("   python -m test.test_api_backend --api [ollama|hf_tgi|hf_tei|gemini|llvm|opea|ovms|s3_kit]")

if __name__ == "__main__":
    main()