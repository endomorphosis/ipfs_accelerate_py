#!/usr/bin/env python3
"""
Test for bert model with hardware platform support
Generated by fixed_merged_test_generator_clean.py
"""

import os
import sys
import unittest
import importlib.util
import logging
import torch
import numpy as np
from transformers import AutoModel, AutoTokenizer, AutoConfig

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Hardware detection
HAS_CUDA = torch.cuda.is_available()
HAS_ROCM = (HAS_CUDA and hasattr(torch, '_C') and hasattr(torch._C, '_rocm_version')) or ('ROCM_HOME' in os.environ)
HAS_MPS = hasattr(torch, "mps") and hasattr(torch.mps, "is_available") and torch.mps.is_available()
HAS_OPENVINO = importlib.util.find_spec("openvino") is not None
HAS_QUALCOMM = importlib.util.find_spec("qnn_wrapper") is not None or importlib.util.find_spec("qti") is not None
HAS_WEBNN = importlib.util.find_spec("webnn") is not None or "WEBNN_AVAILABLE" in os.environ
HAS_WEBGPU = importlib.util.find_spec("webgpu") is not None or "WEBGPU_AVAILABLE" in os.environ

class TestBert(unittest.TestCase):
    """Test bert model with cross-platform hardware support."""
    
    def setUp(self):
        """Set up the test environment."""
        self.model_id = "bert"
        self.tokenizer = None
        self.model = None
        self.modality = "text"

    def test_cpu(self):
        """Test bert with cpu."""
        # Skip if hardware not available
        if not HAS_CPU: self.skipTest('CPU not available')
        
        # Set up device
        device = "cpu"

        
        try:
            # Initialize tokenizer
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_id)
            
            # Initialize model
            self.model = AutoModel.from_pretrained(self.model_id)
            
            # Move model to device if not CPU
            if device != "cpu":
                self.model = self.model.to(device)
            
            # Prepare input
            inputs = self.tokenizer("Test input for bert", return_tensors="pt")
            
            # Move inputs to device if not CPU
            if device != "cpu":
                inputs = {k: v.to(device) for k, v in inputs.items()}
            
            # Run inference
            with torch.no_grad():
                outputs = self.model(**inputs)
            
            # Verify outputs
            self.assertIsNotNone(outputs)
            self.assertIn("last_hidden_state", outputs)
            
            # Log success
            logger.info(f"Successfully tested {self.model_id} on cpu")

        except Exception as e:
            logger.error(f"Error testing {self.model_id} on cpu: {str(e)}")
            raise

if __name__ == "__main__":
    unittest.main()
