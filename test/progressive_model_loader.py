#!/usr/bin/env python3
"""
Progressive Model Loader for Web Platforms (June 2025)

This module implements a comprehensive progressive loading system for large language models
in web environments, enabling layer-by-layer loading, parallel component processing, and 
adaptive memory management:

- Layer-wise progressive loading with prioritization
- Component-based memory management for multimodal models
- Hot-swapping system for model components
- Background loading with web workers
- Memory tracking and visualization
- Adaptive loading based on device capabilities

Usage:
    from progressive_model_loader import (
        ProgressiveModelLoader,
        MultimodalComponentManager,
        load_model_progressively
    )
    
    # Initialize the loader
    loader = ProgressiveModelLoader(
        model_path="llama-7b", 
        device="webgpu",
        max_memory_mb=4000
    )
    
    # Load critical components first
    critical_components = await loader.load_critical_components()
    
    # Start background loading of remaining components
    loader.load_remaining_components_background()
    
    # Access model components (loaded on demand if not yet available)
    embeddings = loader.get_component("embeddings")
    layer_5 = loader.get_component("layers.5")
"""

import os
import sys
import time
import json
import asyncio
import logging
from pathlib import Path
from typing import Dict, List, Any, Optional, Union, Tuple, Set, Callable

# Import memory optimization components
try:
    from fixed_web_platform.webgpu_memory_optimization import (
        WebGPUMemoryOptimizer, 
        ProgressiveTensorLoader
    )
    MEMORY_OPTIMIZATION_AVAILABLE = True
except ImportError:
    MEMORY_OPTIMIZATION_AVAILABLE = False

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger("progressive_model_loader")

class ProgressiveModelLoader:
    """Loads model components progressively to optimize memory and startup time."""
    
    def __init__(
        self, 
        model_path: str, 
        device: str = "webgpu",
        config: Optional[Dict[str, Any]] = None,
        max_memory_mb: int = 4000,
        enable_background_loading: bool = True,
        component_cache_size: int = 10
    ):
        """
        Initialize the progressive model loader.
        
        Args:
            model_path: Path to the model
            device: Target device (webgpu, cpu, etc.)
            config: Configuration dictionary
            max_memory_mb: Maximum memory to use in MB
            enable_background_loading: Whether to enable background loading
            component_cache_size: Max number of components to keep loaded
        """
        self.model_path = model_path
        self.device = device
        self.config = config or {}
        self.max_memory_mb = max_memory_mb
        self.enable_background_loading = enable_background_loading
        self.component_cache_size = component_cache_size
        
        # Initialize component tracking
        self.loaded_components = {}
        self.loading_status = {}
        self.component_access_history = []
        self.loading_tasks = {}
        
        # Memory tracking
        self.memory_usage = {
            "current_mb": 0,
            "peak_mb": 0,
            "component_sizes": {},
            "timeline": []
        }
        
        # Component priority settings
        self.component_priorities = self._determine_component_priorities()
        
        # Background loading state
        self.background_loader_active = False
        self.background_queue = []
        
        # Browser-specific optimizations
        self.browser_optimizations = self._detect_browser_optimizations()
        
        # Memory optimizer integration
        if MEMORY_OPTIMIZATION_AVAILABLE:
            self.memory_optimizer = WebGPUMemoryOptimizer(
                total_memory_mb=max_memory_mb,
                offload_cpu=True
            )
            self.tensor_loader = ProgressiveTensorLoader(
                memory_optimizer=self.memory_optimizer,
                enable_streaming=enable_background_loading
            )
        else:
            self.memory_optimizer = None
            self.tensor_loader = None
            
        # Log initialization
        logger.info(f"Initialized progressive model loader for {model_path} on {device}")
        logger.info(f"Memory limit: {max_memory_mb}MB, Background loading: {enable_background_loading}")
    
    def _determine_component_priorities(self) -> Dict[str, int]:
        """
        Determine loading priorities for model components.
        
        Returns:
            Dictionary mapping component paths to priority values (lower = higher priority)
        """
        # Default priorities by component type
        priorities = {
            "embeddings": 10,             # Highest priority - needed first
            "first_layer": 20,            # Needed early
            "final_layer": 30,            # Needed early for output processing
            "lm_head": 30,                # Needed for token generation
            "middle_layers": 40,          # Can be loaded as needed
            "vision_encoder": 20,         # High priority for multimodal inputs
            "text_encoder": 30,           # Medium priority
            "cross_attention": 30,        # Medium priority
            "layernorm": 10,              # Small, load early
            "nonessential_components": 50 # Lowest priority
        }
        
        # Model-specific adjustments
        model_type = self._get_model_type()
        if "llama" in model_type.lower() or "qwen" in model_type.lower():
            priorities["kv_cache"] = 20  # Higher priority for generative models
            priorities["attention"] = 25  # Important for generation
            
        elif "clip" in model_type.lower() or "llava" in model_type.lower():
            priorities["vision_encoder"] = 10  # Highest priority for visual models
            priorities["attention"] = 25        # Important for cross-attention
            
        elif "t5" in model_type.lower() or "bert" in model_type.lower():
            priorities["attention"] = 20  # More important for encoder models
            
        # Create detailed component map from general categories
        component_priorities = {}
        
        # Add embeddings
        component_priorities["embeddings"] = priorities["embeddings"]
        
        # Add layers with position-based priorities
        num_layers = self.config.get("num_hidden_layers", 12)
        for i in range(num_layers):
            if i == 0:
                component_priorities[f"layers.{i}"] = priorities["first_layer"]
            elif i == num_layers - 1:
                component_priorities[f"layers.{i}"] = priorities["final_layer"]
            else:
                # Middle layers get progressively lower priority
                layer_priority = priorities["middle_layers"] + (i / num_layers) * 10
                component_priorities[f"layers.{i}"] = layer_priority
                
            # Add specific subcomponents within layers
            component_priorities[f"layers.{i}.attention"] = priorities["attention"]
            component_priorities[f"layers.{i}.layernorm"] = priorities["layernorm"]
            component_priorities[f"layers.{i}.mlp"] = priorities["middle_layers"] + 5
        
        # Add output components
        component_priorities["lm_head"] = priorities["lm_head"]
        
        # Add multimodal components if applicable
        if "vision" in model_type.lower() or "clip" in model_type.lower() or "llava" in model_type.lower():
            component_priorities["vision_encoder"] = priorities["vision_encoder"]
            component_priorities["text_encoder"] = priorities["text_encoder"]
            component_priorities["cross_attention"] = priorities["cross_attention"]
        
        return component_priorities
    
    def _get_model_type(self) -> str:
        """Determine model type from config or path."""
        if self.config and "model_type" in self.config:
            return self.config["model_type"]
        
        # Try to infer from path
        model_path = self.model_path.lower()
        if "llama" in model_path:
            return "llama"
        elif "qwen" in model_path:
            return "qwen"
        elif "t5" in model_path:
            return "t5"
        elif "clip" in model_path or "llava" in model_path:
            return "clip"
        elif "bert" in model_path:
            return "bert"
        
        # Default
        return "generic"
    
    def _detect_browser_optimizations(self) -> Dict[str, Any]:
        """
        Detect browser-specific optimizations.
        
        Returns:
            Dictionary with browser optimization settings
        """
        # In a real implementation, this would detect the browser and capabilities
        optimizations = {
            "use_web_workers": True,
            "use_shared_array_buffer": True,
            "use_offscreen_canvas": True,
            "use_dynamic_import": True,
            "parallel_components": True
        }
        
        # This is a placeholder for actual browser detection
        browser = "chrome"  # Simulated value
        
        if browser == "safari":
            # Safari has more limited capabilities
            optimizations.update({
                "use_shared_array_buffer": False,
                "use_offscreen_canvas": False
            })
        
        return optimizations
    
    async def load_critical_components(self) -> Dict[str, Any]:
        """
        Load only critical components needed to start inference.
        
        Returns:
            Dictionary of loaded critical components
        """
        logger.info("Loading critical components...")
        start_time = time.time()
        
        # Identify critical components (priority < 20)
        critical_components = [
            name for name, priority in self.component_priorities.items()
            if priority <= 20
        ]
        
        # Add the first layer (always needed)
        if "layers.0" not in critical_components:
            critical_components.append("layers.0")
        
        # Add LM head for generative models
        if "lm_head" not in critical_components:
            critical_components.append("lm_head")
        
        # Load components in parallel
        loading_tasks = []
        for component_name in critical_components:
            task = self._load_component(component_name)
            loading_tasks.append(task)
        
        # Wait for all critical components to load
        results = await asyncio.gather(*loading_tasks)
        
        # Map component names to loaded components
        loaded_critical = {}
        for i, component_name in enumerate(critical_components):
            loaded_critical[component_name] = results[i]
        
        # Update metrics
        elapsed_time = time.time() - start_time
        logger.info(f"Loaded {len(critical_components)} critical components in {elapsed_time:.2f}s")
        
        return loaded_critical
    
    def load_remaining_components_background(self) -> bool:
        """
        Start loading remaining components in the background.
        
        Returns:
            True if background loading started, False otherwise
        """
        if not self.enable_background_loading:
            logger.warning("Background loading disabled, not loading remaining components")
            return False
        
        # Don't start if already running
        if self.background_loader_active:
            logger.debug("Background loader already active")
            return True
        
        # Identify non-critical components that aren't loaded yet
        remaining_components = [
            name for name, priority in self.component_priorities.items()
            if name not in self.loaded_components and priority > 20  # Not critical
        ]
        
        # Sort by priority
        remaining_components.sort(key=lambda name: self.component_priorities.get(name, 100))
        
        if not remaining_components:
            logger.info("No remaining components to load")
            return False
        
        # Queue components for background loading
        self.background_queue = remaining_components
        self.background_loader_active = True
        
        # Start background loading
        if self.browser_optimizations.get("use_web_workers", False):
            # In a real implementation, this would use Web Workers
            logger.info(f"Starting background loading of {len(remaining_components)} components with Web Workers")
            
            # Simulated background loading
            asyncio.create_task(self._background_loading_task())
        else:
            # For browsers without Web Workers, load in the background but on main thread
            logger.info(f"Starting background loading of {len(remaining_components)} components on main thread")
            
            # Simulated background loading on main thread
            asyncio.create_task(self._background_loading_task())
        
        return True
    
    async def _background_loading_task(self):
        """Background loading task to load components gradually."""
        while self.background_queue and self.background_loader_active:
            # Get next component to load
            component_name = self.background_queue.pop(0)
            
            # Skip if already loaded
            if component_name in self.loaded_components:
                continue
                
            try:
                # Load the component
                await self._load_component(component_name)
                
                # Simulate delay to avoid blocking main thread
                await asyncio.sleep(0.05)  # Small delay
                
                # Check memory limits and possibly offload
                if self.memory_usage["current_mb"] > self.max_memory_mb * 0.8:
                    self._offload_least_used_components()
            except Exception as e:
                logger.error(f"Error loading component {component_name} in background: {e}")
        
        self.background_loader_active = False
        logger.info("Background loading completed")
    
    async def _load_component(self, component_name: str) -> Any:
        """
        Load a specific model component.
        
        Args:
            component_name: Name of the component to load
            
        Returns:
            The loaded component
        """
        # If already loaded, just return it
        if component_name in self.loaded_components:
            self._record_component_access(component_name)
            return self.loaded_components[component_name]
        
        # If currently loading, wait for it to complete
        if component_name in self.loading_tasks:
            logger.debug(f"Waiting for {component_name} to finish loading")
            return await self.loading_tasks[component_name]
        
        # Start loading
        logger.debug(f"Loading component {component_name}")
        self.loading_status[component_name] = "loading"
        
        # Create a task to track this loading operation
        loading_task = asyncio.create_task(self._actual_component_loading(component_name))
        self.loading_tasks[component_name] = loading_task
        
        try:
            # Wait for loading to complete
            component = await loading_task
            
            # Record successful loading
            self.loaded_components[component_name] = component
            self.loading_status[component_name] = "loaded"
            self._record_component_access(component_name)
            
            return component
        except Exception as e:
            # Record failure
            self.loading_status[component_name] = "error"
            logger.error(f"Error loading component {component_name}: {e}")
            raise
        finally:
            # Remove task reference
            if component_name in self.loading_tasks:
                del self.loading_tasks[component_name]
    
    async def _actual_component_loading(self, component_name: str) -> Any:
        """
        Perform the actual loading of a component.
        
        Args:
            component_name: Name of the component to load
            
        Returns:
            The loaded component
        """
        # Simulate loading time based on component size and priority
        priority = self.component_priorities.get(component_name, 50)
        is_critical = priority <= 20
        
        # In a real implementation, the components would be loaded from files
        # Here we simulate the process
        
        # Determine component type and size
        if "embeddings" in component_name:
            component_type = "embeddings"
            size_mb = 20  # Simulated size
        elif "layers" in component_name:
            component_type = "layer"
            # Extract layer index
            layer_parts = component_name.split(".")
            if len(layer_parts) >= 2 and layer_parts[0] == "layers":
                try:
                    layer_idx = int(layer_parts[1])
                    # First and last layers slightly larger due to special handling
                    if layer_idx == 0 or layer_idx == self.config.get("num_hidden_layers", 12) - 1:
                        size_mb = 25
                    else:
                        size_mb = 20
                except (ValueError, IndexError):
                    size_mb = 20
            else:
                size_mb = 20
                
            # Subcomponents are smaller
            if len(layer_parts) > 2:
                size_mb = 5
        elif "lm_head" in component_name:
            component_type = "lm_head"
            size_mb = 10
        elif "vision_encoder" in component_name:
            component_type = "vision_encoder"
            size_mb = 40
        elif "text_encoder" in component_name:
            component_type = "text_encoder"
            size_mb = 20
        else:
            component_type = "other"
            size_mb = 10
        
        # Update memory tracking
        self._update_memory_usage(component_name, size_mb, "alloc")
        
        # Simulate loading time (critical components load faster due to optimization)
        loading_time = 0.02 * size_mb
        if not is_critical:
            loading_time *= 1.5
        
        # Simulate actual loading
        await asyncio.sleep(loading_time)
        
        # Create simulated component
        component = {
            "name": component_name,
            "type": component_type,
            "size_mb": size_mb,
            "is_critical": is_critical,
            "loaded_at": time.time(),
            "device": self.device
        }
        
        return component
    
    def get_component(self, component_name: str) -> Any:
        """
        Get a component, loading it if needed.
        
        Args:
            component_name: Name of the component to get
            
        Returns:
            The requested component or a future if it's still loading
        """
        # If already loaded, return immediately
        if component_name in self.loaded_components:
            self._record_component_access(component_name)
            return self.loaded_components[component_name]
        
        # If currently loading, return the task
        if component_name in self.loading_tasks:
            return self.loading_tasks[component_name]
        
        # Not loaded and not loading, start loading now
        loading_task = asyncio.create_task(self._load_component(component_name))
        self.loading_tasks[component_name] = loading_task
        
        return loading_task
    
    def unload_component(self, component_name: str) -> bool:
        """
        Unload a component to free memory.
        
        Args:
            component_name: Name of the component to unload
            
        Returns:
            True if unloaded successfully, False otherwise
        """
        if component_name not in self.loaded_components:
            return False
        
        # Get component info
        component = self.loaded_components[component_name]
        
        # Prevent unloading critical components
        priority = self.component_priorities.get(component_name, 50)
        if priority <= 20:
            logger.warning(f"Cannot unload critical component: {component_name}")
            return False
        
        # Remove from loaded components
        del self.loaded_components[component_name]
        
        # Update memory tracking
        size_mb = component.get("size_mb", 0)
        self._update_memory_usage(component_name, size_mb, "free")
        
        # Update status
        self.loading_status[component_name] = "unloaded"
        
        logger.debug(f"Unloaded component {component_name}, freed {size_mb}MB")
        return True
    
    def _offload_least_used_components(self, required_mb: float = 0) -> float:
        """
        Offload least recently used components to free memory.
        
        Args:
            required_mb: Amount of memory needed in MB
            
        Returns:
            Amount of memory freed in MB
        """
        # Skip if no components loaded
        if not self.loaded_components:
            return 0
        
        # Find non-critical components
        offloadable_components = [
            name for name in self.loaded_components
            if self.component_priorities.get(name, 0) > 20  # Not critical
        ]
        
        if not offloadable_components:
            logger.warning("No offloadable components available")
            return 0
        
        # Sort by last access time (oldest first)
        offloadable_components.sort(
            key=lambda name: self.component_access_history.index(name) 
            if name in self.component_access_history else -1
        )
        
        # Determine how many components to offload
        current_memory = self.memory_usage["current_mb"]
        target_memory = self.max_memory_mb * 0.8  # Target 80% of max
        
        if required_mb > 0:
            target_memory = current_memory - required_mb
        
        # Offload components until we reach the target
        freed_mb = 0
        offloaded_count = 0
        
        for name in offloadable_components:
            if current_memory - freed_mb <= target_memory:
                break
                
            component = self.loaded_components.get(name)
            if not component:
                continue
                
            size_mb = component.get("size_mb", 0)
            if self.unload_component(name):
                freed_mb += size_mb
                offloaded_count += 1
                
                # Keep at least a few offloadable components
                if offloaded_count >= len(offloadable_components) - self.component_cache_size:
                    break
        
        if offloaded_count > 0:
            logger.info(f"Offloaded {offloaded_count} components, freed {freed_mb:.2f}MB")
            
        return freed_mb
    
    def _record_component_access(self, component_name: str):
        """
        Record an access to a component for LRU tracking.
        
        Args:
            component_name: Name of the accessed component
        """
        # Remove previous occurrence if any
        if component_name in self.component_access_history:
            self.component_access_history.remove(component_name)
            
        # Add to the end (most recently used)
        self.component_access_history.append(component_name)
        
        # Trim history if too long
        max_history = max(100, len(self.loaded_components) * 2)
        if len(self.component_access_history) > max_history:
            self.component_access_history = self.component_access_history[-max_history:]
    
    def _update_memory_usage(self, component_name: str, size_mb: float, operation: str):
        """
        Update memory usage tracking.
        
        Args:
            component_name: Name of the component
            size_mb: Size of the component in MB
            operation: Operation type ('alloc' or 'free')
        """
        # Update component size tracking
        self.memory_usage["component_sizes"][component_name] = size_mb
        
        # Update current memory usage
        if operation == "alloc":
            self.memory_usage["current_mb"] += size_mb
        elif operation == "free":
            self.memory_usage["current_mb"] -= size_mb
            
        # Update peak memory
        self.memory_usage["peak_mb"] = max(
            self.memory_usage["peak_mb"], 
            self.memory_usage["current_mb"]
        )
        
        # Add to timeline
        self.memory_usage["timeline"].append({
            "timestamp": time.time(),
            "component": component_name,
            "operation": operation,
            "size_mb": size_mb,
            "current_mb": self.memory_usage["current_mb"],
            "peak_mb": self.memory_usage["peak_mb"]
        })
    
    def get_loading_status(self) -> Dict[str, Any]:
        """
        Get the current loading status for all components.
        
        Returns:
            Dictionary with loading status information
        """
        # Count components by status
        status_counts = {
            "loaded": sum(1 for status in self.loading_status.values() if status == "loaded"),
            "loading": sum(1 for status in self.loading_status.values() if status == "loading"),
            "error": sum(1 for status in self.loading_status.values() if status == "error"),
            "unloaded": sum(1 for status in self.loading_status.values() if status == "unloaded"),
            "pending": len(self.background_queue) if self.background_loader_active else 0
        }
        
        # Calculate percentages
        total_components = len(self.component_priorities)
        loaded_pct = (status_counts["loaded"] / total_components) * 100 if total_components > 0 else 0
        
        return {
            "total_components": total_components,
            "loaded_count": status_counts["loaded"],
            "loading_count": status_counts["loading"],
            "error_count": status_counts["error"],
            "unloaded_count": status_counts["unloaded"],
            "pending_count": status_counts["pending"],
            "loaded_percent": loaded_pct,
            "background_active": self.background_loader_active,
            "memory_usage": {
                "current_mb": self.memory_usage["current_mb"],
                "peak_mb": self.memory_usage["peak_mb"],
                "max_mb": self.max_memory_mb,
                "utilization_percent": (self.memory_usage["current_mb"] / self.max_memory_mb) * 100 
                if self.max_memory_mb > 0 else 0
            },
            "timestamp": time.time()
        }


class MultimodalComponentManager:
    """Manages components for multimodal models with progressive loading."""
    
    def __init__(
        self, 
        model_config: Dict[str, Any],
        max_memory_mb: int = 4000,
        device: str = "webgpu"
    ):
        """
        Initialize the multimodal component manager.
        
        Args:
            model_config: Model configuration
            max_memory_mb: Maximum memory to use in MB
            device: Target device
        """
        self.model_config = model_config
        self.max_memory_mb = max_memory_mb
        self.device = device
        
        # Component tracking
        self.components = {}
        self.loaded_state = {}
        self.active_modality = None
        
        # Initialize loaders
        self.component_loaders = self._initialize_component_loaders()
        
        # Memory tracking
        self.memory_usage = {
            "current_mb": 0,
            "peak_mb": 0,
            "component_sizes": {},
            "timeline": []
        }
        
        # Browser compatibility
        self.parallel_loading_supported = self._check_parallel_loading_support()
        
        logger.info(f"Initialized multimodal component manager for {device}")
    
    def _initialize_component_loaders(self) -> Dict[str, ProgressiveModelLoader]:
        """
        Initialize component-specific loaders.
        
        Returns:
            Dictionary of component loaders
        """
        loaders = {}
        
        # Detect model type and determine components
        model_type = self.model_config.get("model_type", "").lower()
        
        if "clip" in model_type or "llava" in model_type:
            # Vision-language models have vision and text components
            loaders["vision_encoder"] = ProgressiveModelLoader(
                model_path=f"{self.model_config.get('model_path', '')}/vision_encoder",
                device=self.device,
                max_memory_mb=self.max_memory_mb * 0.4,  # 40% of memory
                config={"model_type": "vision"}
            )
            
            loaders["text_encoder"] = ProgressiveModelLoader(
                model_path=f"{self.model_config.get('model_path', '')}/text_encoder",
                device=self.device,
                max_memory_mb=self.max_memory_mb * 0.4,  # 40% of memory
                config={"model_type": "text"}
            )
            
            # For LLaVA, also add the LLM component
            if "llava" in model_type:
                loaders["llm"] = ProgressiveModelLoader(
                    model_path=f"{self.model_config.get('model_path', '')}/llm",
                    device=self.device,
                    max_memory_mb=self.max_memory_mb * 0.6,  # 60% of memory
                    config={"model_type": "llm"}
                )
        
        elif "whisper" in model_type or "audio" in model_type:
            # Audio models
            loaders["audio_encoder"] = ProgressiveModelLoader(
                model_path=f"{self.model_config.get('model_path', '')}/audio_encoder",
                device=self.device,
                max_memory_mb=self.max_memory_mb * 0.3,  # 30% of memory
                config={"model_type": "audio"}
            )
            
            loaders["text_decoder"] = ProgressiveModelLoader(
                model_path=f"{self.model_config.get('model_path', '')}/text_decoder",
                device=self.device,
                max_memory_mb=self.max_memory_mb * 0.5,  # 50% of memory
                config={"model_type": "text"}
            )
        
        elif "t5" in model_type or "bart" in model_type:
            # Encoder-decoder models
            loaders["encoder"] = ProgressiveModelLoader(
                model_path=f"{self.model_config.get('model_path', '')}/encoder",
                device=self.device,
                max_memory_mb=self.max_memory_mb * 0.5,  # 50% of memory
                config={"model_type": "encoder"}
            )
            
            loaders["decoder"] = ProgressiveModelLoader(
                model_path=f"{self.model_config.get('model_path', '')}/decoder",
                device=self.device,
                max_memory_mb=self.max_memory_mb * 0.5,  # 50% of memory
                config={"model_type": "decoder"}
            )
        
        elif "bert" in model_type or "roberta" in model_type:
            # Single-component encoder models
            loaders["encoder"] = ProgressiveModelLoader(
                model_path=self.model_config.get('model_path', ''),
                device=self.device,
                max_memory_mb=self.max_memory_mb,
                config={"model_type": model_type}
            )
        
        elif "llama" in model_type or "qwen" in model_type or "gpt" in model_type:
            # Single-component decoder models
            loaders["decoder"] = ProgressiveModelLoader(
                model_path=self.model_config.get('model_path', ''),
                device=self.device,
                max_memory_mb=self.max_memory_mb,
                config={"model_type": model_type}
            )
        
        else:
            # Generic case - single component
            loaders["model"] = ProgressiveModelLoader(
                model_path=self.model_config.get('model_path', ''),
                device=self.device,
                max_memory_mb=self.max_memory_mb,
                config={"model_type": "generic"}
            )
        
        return loaders
    
    def _check_parallel_loading_support(self) -> bool:
        """
        Check if parallel loading is supported in the current environment.
        
        Returns:
            True if supported, False otherwise
        """
        # This would be a real check in a production environment
        # For now, assume it's supported except on Safari
        browser = "chrome"  # Simulated value
        
        if browser == "safari":
            return False
        
        return True
    
    async def load_components_parallel(self, components=None) -> Dict[str, Any]:
        """
        Load multiple model components in parallel.
        
        Args:
            components: List of component names to load (or None for all)
            
        Returns:
            Dictionary of loaded components
        """
        if components is None:
            components = list(self.component_loaders.keys())
        
        if not self.parallel_loading_supported:
            logger.warning("Parallel loading not supported in this environment, falling back to sequential")
            return await self.load_components_sequential(components)
        
        logger.info(f"Loading {len(components)} components in parallel...")
        start_time = time.time()
        
        # Create tasks for loading critical components of each component
        loading_tasks = []
        for component_name in components:
            if component_name in self.component_loaders:
                loader = self.component_loaders[component_name]
                task = loader.load_critical_components()
                loading_tasks.append((component_name, task))
        
        # Wait for all components to load critical parts
        results = {}
        for component_name, task in loading_tasks:
            try:
                component_result = await task
                results[component_name] = component_result
                self.components[component_name] = component_result
                self.loaded_state[component_name] = "loaded_critical"
                
                # Start background loading for the rest
                loader = self.component_loaders[component_name]
                loader.load_remaining_components_background()
                
                # Update memory usage
                size_mb = sum(comp.get("size_mb", 0) for comp in component_result.values())
                self._update_memory_usage(component_name, size_mb, "alloc")
                
            except Exception as e:
                logger.error(f"Error loading component {component_name}: {e}")
                self.loaded_state[component_name] = "error"
        
        elapsed_time = time.time() - start_time
        logger.info(f"Loaded critical parts of {len(results)} components in {elapsed_time:.2f}s")
        
        return results
    
    async def load_components_sequential(self, components=None) -> Dict[str, Any]:
        """
        Load multiple model components sequentially.
        
        Args:
            components: List of component names to load (or None for all)
            
        Returns:
            Dictionary of loaded components
        """
        if components is None:
            components = list(self.component_loaders.keys())
        
        logger.info(f"Loading {len(components)} components sequentially...")
        start_time = time.time()
        
        results = {}
        for component_name in components:
            if component_name in self.component_loaders:
                try:
                    loader = self.component_loaders[component_name]
                    component_result = await loader.load_critical_components()
                    
                    results[component_name] = component_result
                    self.components[component_name] = component_result
                    self.loaded_state[component_name] = "loaded_critical"
                    
                    # Start background loading for the rest
                    loader.load_remaining_components_background()
                    
                    # Update memory usage
                    size_mb = sum(comp.get("size_mb", 0) for comp in component_result.values())
                    self._update_memory_usage(component_name, size_mb, "alloc")
                    
                except Exception as e:
                    logger.error(f"Error loading component {component_name}: {e}")
                    self.loaded_state[component_name] = "error"
        
        elapsed_time = time.time() - start_time
        logger.info(f"Loaded critical parts of {len(results)} components in {elapsed_time:.2f}s")
        
        return results
    
    def unload_inactive_components(self, active_modality=None) -> float:
        """
        Unload components that aren't currently needed.
        
        Args:
            active_modality: The currently active modality (or None to infer)
            
        Returns:
            Amount of memory freed in MB
        """
        if active_modality:
            self.active_modality = active_modality
        
        if not self.active_modality:
            logger.warning("No active modality specified, cannot unload inactive components")
            return 0
        
        # Determine which components to keep based on active modality
        keep_components = self._get_components_for_modality(self.active_modality)
        
        # Unload other components
        freed_mb = 0
        for component_name in list(self.components.keys()):
            if component_name not in keep_components:
                logger.info(f"Unloading inactive component: {component_name}")
                if component_name in self.component_loaders:
                    size_mb = 0
                    if component_name in self.memory_usage["component_sizes"]:
                        size_mb = self.memory_usage["component_sizes"][component_name]
                    
                    # Mark as unloaded
                    self.loaded_state[component_name] = "unloaded"
                    
                    # Remove from loaded components
                    if component_name in self.components:
                        del self.components[component_name]
                    
                    # Update memory tracking
                    self._update_memory_usage(component_name, size_mb, "free")
                    freed_mb += size_mb
        
        if freed_mb > 0:
            logger.info(f"Freed {freed_mb:.2f}MB by unloading inactive components")
        
        return freed_mb
    
    def swap_component(self, old_component: str, new_component: str) -> bool:
        """
        Hot-swap model components to optimize memory.
        
        Args:
            old_component: Component to unload
            new_component: Component to load
            
        Returns:
            True if successful, False otherwise
        """
        if old_component not in self.component_loaders:
            logger.error(f"Cannot swap: old component {old_component} not found")
            return False
            
        if new_component not in self.component_loaders:
            logger.error(f"Cannot swap: new component {new_component} not found")
            return False
        
        logger.info(f"Hot-swapping component {old_component} for {new_component}")
        
        # Unload old component
        old_size_mb = 0
        if old_component in self.memory_usage["component_sizes"]:
            old_size_mb = self.memory_usage["component_sizes"][old_component]
        
        # Mark as unloaded
        self.loaded_state[old_component] = "unloaded"
        
        # Remove from loaded components
        if old_component in self.components:
            del self.components[old_component]
        
        # Update memory tracking
        self._update_memory_usage(old_component, old_size_mb, "free")
        
        # Queue new component for loading
        asyncio.create_task(self._load_component_for_swap(new_component))
        
        return True
    
    async def _load_component_for_swap(self, component_name: str):
        """
        Load a component after a hot-swap.
        
        Args:
            component_name: Name of the component to load
        """
        try:
            if component_name in self.component_loaders:
                loader = self.component_loaders[component_name]
                component_result = await loader.load_critical_components()
                
                self.components[component_name] = component_result
                self.loaded_state[component_name] = "loaded_critical"
                
                # Start background loading for the rest
                loader.load_remaining_components_background()
                
                # Update memory usage
                size_mb = sum(comp.get("size_mb", 0) for comp in component_result.values())
                self._update_memory_usage(component_name, size_mb, "alloc")
                
                logger.info(f"Successfully loaded {component_name} after hot-swap")
        except Exception as e:
            logger.error(f"Error loading component {component_name} after hot-swap: {e}")
            self.loaded_state[component_name] = "error"
    
    def _get_components_for_modality(self, modality: str) -> List[str]:
        """
        Get list of components needed for a specific modality.
        
        Args:
            modality: Modality to get components for
            
        Returns:
            List of component names
        """
        if modality == "text":
            return ["text_encoder", "llm", "decoder", "encoder"]
        elif modality == "vision":
            return ["vision_encoder", "llm"]
        elif modality == "audio":
            return ["audio_encoder", "text_decoder"]
        elif modality == "multimodal":
            # All components needed
            return list(self.component_loaders.keys())
        else:
            # Default to all components
            return list(self.component_loaders.keys())
    
    def _update_memory_usage(self, component_name: str, size_mb: float, operation: str):
        """
        Update memory usage tracking.
        
        Args:
            component_name: Name of the component
            size_mb: Size of the component in MB
            operation: Operation type ('alloc' or 'free')
        """
        # Update component size tracking
        self.memory_usage["component_sizes"][component_name] = size_mb
        
        # Update current memory usage
        if operation == "alloc":
            self.memory_usage["current_mb"] += size_mb
        elif operation == "free":
            self.memory_usage["current_mb"] -= size_mb
            
        # Update peak memory
        self.memory_usage["peak_mb"] = max(
            self.memory_usage["peak_mb"], 
            self.memory_usage["current_mb"]
        )
        
        # Add to timeline
        self.memory_usage["timeline"].append({
            "timestamp": time.time(),
            "component": component_name,
            "operation": operation,
            "size_mb": size_mb,
            "current_mb": self.memory_usage["current_mb"],
            "peak_mb": self.memory_usage["peak_mb"]
        })
    
    def get_loading_status(self) -> Dict[str, Any]:
        """
        Get the current loading status for all components.
        
        Returns:
            Dictionary with loading status information
        """
        component_status = {}
        for component_name, loader in self.component_loaders.items():
            if component_name in self.loaded_state:
                component_status[component_name] = self.loaded_state[component_name]
            else:
                component_status[component_name] = "not_loaded"
        
        return {
            "components": component_status,
            "active_modality": self.active_modality,
            "parallel_loading_supported": self.parallel_loading_supported,
            "memory_usage": {
                "current_mb": self.memory_usage["current_mb"],
                "peak_mb": self.memory_usage["peak_mb"],
                "max_mb": self.max_memory_mb,
                "utilization_percent": (self.memory_usage["current_mb"] / self.max_memory_mb) * 100 
                if self.max_memory_mb > 0 else 0
            },
            "timestamp": time.time()
        }


async def load_model_progressively(
    model_path: str,
    device: str = "webgpu",
    config: Optional[Dict[str, Any]] = None,
    max_memory_mb: int = 4000,
    multimodal: bool = False
) -> Dict[str, Any]:
    """
    Load a model progressively with optimized memory usage.
    
    Args:
        model_path: Path to the model
        device: Target device
        config: Optional model configuration
        max_memory_mb: Maximum memory to use in MB
        multimodal: Whether this is a multimodal model
        
    Returns:
        Dictionary with model components and loading metrics
    """
    start_time = time.time()
    
    if not config:
        config = {"model_path": model_path}
    else:
        config["model_path"] = model_path
    
    if multimodal:
        # Use multimodal component manager
        loader = MultimodalComponentManager(
            model_config=config,
            max_memory_mb=max_memory_mb,
            device=device
        )
        
        # Determine which components to load initially
        if "modality" in config:
            initial_modality = config["modality"]
            components = loader._get_components_for_modality(initial_modality)
            loader.active_modality = initial_modality
        else:
            # Default to all components for multimodal models
            components = list(loader.component_loaders.keys())
        
        # Load components
        if loader.parallel_loading_supported:
            loaded_components = await loader.load_components_parallel(components)
        else:
            loaded_components = await loader.load_components_sequential(components)
    else:
        # Use standard progressive loader
        loader = ProgressiveModelLoader(
            model_path=model_path,
            device=device,
            config=config,
            max_memory_mb=max_memory_mb
        )
        
        # Load critical components
        loaded_components = await loader.load_critical_components()
        
        # Start background loading
        loader.load_remaining_components_background()
    
    elapsed_time = time.time() - start_time
    
    # Return loaded model with info
    return {
        "model": loaded_components,
        "loader": loader,
        "loading_info": {
            "initial_load_time_seconds": elapsed_time,
            "model_path": model_path,
            "device": device,
            "multimodal": multimodal,
            "memory_usage_mb": loader.memory_usage["current_mb"],
            "max_memory_mb": max_memory_mb,
            "status": loader.get_loading_status()
        }
    }


if __name__ == "__main__":
    # Example usage
    import asyncio
    
    async def main():
        print("Progressive Model Loading Module - Example Usage")
        print("===============================================")
        
        # Example: Loading a model progressively
        print("\n1. Example: Loading a standard model")
        model_result = await load_model_progressively(
            model_path="llama-7b",
            device="webgpu",
            max_memory_mb=4000
        )
        
        print(f"Loaded initial components in {model_result['loading_info']['initial_load_time_seconds']:.2f} seconds")
        print(f"Current memory usage: {model_result['loading_info']['memory_usage_mb']:.2f}MB")
        
        # Example: Loading a multimodal model
        print("\n2. Example: Loading a multimodal model")
        multimodal_config = {
            "model_type": "llava",
            "modality": "vision",
            "hidden_size": 4096,
            "num_hidden_layers": 32
        }
        
        multimodal_result = await load_model_progressively(
            model_path="llava-13b",
            device="webgpu",
            config=multimodal_config,
            max_memory_mb=4000,
            multimodal=True
        )
        
        print(f"Loaded initial components in {multimodal_result['loading_info']['initial_load_time_seconds']:.2f} seconds")
        print(f"Current memory usage: {multimodal_result['loading_info']['memory_usage_mb']:.2f}MB")
        
        # Wait a moment to let background loading proceed
        print("\nWaiting 1 second for background loading to proceed...")
        await asyncio.sleep(1)
        
        # Check status after background loading has started
        print("\n3. Status after background loading started:")
        loader = model_result["loader"]
        status = loader.get_loading_status()
        
        print(f"Loaded components: {status['loaded_count']}/{status['total_components']} ({status['loaded_percent']:.2f}%)")
        print(f"Memory usage: {status['memory_usage']['current_mb']:.2f}MB / {status['memory_usage']['max_mb']}MB ({status['memory_usage']['utilization_percent']:.2f}%)")
    
    asyncio.run(main())