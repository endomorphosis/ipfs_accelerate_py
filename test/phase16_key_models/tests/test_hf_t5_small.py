#!/usr/bin/env python3
"""
Test for t5-small model with hardware platform support
Generated by fixed_merged_test_generator.py
"""

import os
import sys
import unittest
import importlib.util
import logging
import torch
import numpy as np
from transformers import AutoModel, AutoTokenizer, AutoConfig, AutoFeatureExtractor, AutoProcessor, AutoImageProcessor, AutoModelForImageClassification, AutoModelForAudioClassification, AutoModelForVideoClassification

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Hardware detection
HAS_CUDA = torch.cuda.is_available()
HAS_ROCM = (HAS_CUDA and hasattr(torch, '_C') and hasattr(torch._C, '_rocm_version')) or ('ROCM_HOME' in os.environ)
HAS_MPS = hasattr(torch, "mps") and hasattr(torch.mps, "is_available") and torch.mps.is_available()
HAS_OPENVINO = importlib.util.find_spec("openvino") is not None
HAS_QUALCOMM = importlib.util.find_spec("qnn_wrapper") is not None or importlib.util.find_spec("qti") is not None
HAS_WEBNN = importlib.util.find_spec("webnn") is not None or "WEBNN_AVAILABLE" in os.environ
HAS_WEBGPU = importlib.util.find_spec("webgpu") is not None or "WEBGPU_AVAILABLE" in os.environ

# Try to import centralized hardware detection
try:
    from centralized_hardware_detection import hardware_detection
    HAS_CENTRALIZED_DETECTION = True
except ImportError:
    HAS_CENTRALIZED_DETECTION = False

class TestT5SmallModels(unittest.TestCase):
    """Test t5-small model with cross-platform hardware support."""
    
    def setUp(self):
        """Set up the test environment."""
        self.model_id = "t5-small"
        self.tokenizer = None
        self.model = None
        self.modality = "text"
        
    def run_tests(self):
        """Run all tests for this model."""
        unittest.main()

    def test_cpu(self):
        """Test t5-small with cpu."""
        # Skip if hardware not available
        if not HAS_CPU: self.skipTest('CPU not available')
        
        # Set up device
        device = "cpu"

        
        try:
            # Initialize tokenizer and model based on modality
            if 'text' == 'audio':
                from transformers import AutoFeatureExtractor, AutoModelForAudioClassification
                self.processor = AutoFeatureExtractor.from_pretrained(self.model_id)
                self.model = AutoModelForAudioClassification.from_pretrained(self.model_id)
            elif 'text' == 'vision':
                from transformers import AutoImageProcessor, AutoModelForImageClassification
                self.processor = AutoImageProcessor.from_pretrained(self.model_id)
                self.model = AutoModelForImageClassification.from_pretrained(self.model_id)
            elif 'text' == 'multimodal':
                from transformers import AutoProcessor, AutoModel
                self.processor = AutoProcessor.from_pretrained(self.model_id)
                self.model = AutoModel.from_pretrained(self.model_id)
            elif 'text' == 'video':
                from transformers import AutoProcessor, AutoModelForVideoClassification
                self.processor = AutoProcessor.from_pretrained(self.model_id)
                self.model = AutoModelForVideoClassification.from_pretrained(self.model_id)
            else:
                # Default to text models
                self.tokenizer = AutoTokenizer.from_pretrained(self.model_id)
                self.model = AutoModel.from_pretrained(self.model_id)
            
            # Move model to device if not CPU
            if device != "cpu":
                self.model = self.model.to(device)
            
            # Prepare input based on modality
            if 'text' == 'text':
                inputs = self.tokenizer("Test input for t5-small", return_tensors="pt")
            elif 'text' == 'audio':
                import numpy as np
                sample_rate = 16000
                dummy_audio = np.random.random(sample_rate)
                inputs = self.processor(dummy_audio, sampling_rate=sample_rate, return_tensors="pt")
            elif 'text' == 'vision':
                import numpy as np
                from PIL import Image
                dummy_image = Image.new('RGB', (224, 224), color='white')
                inputs = self.processor(images=dummy_image, return_tensors="pt")
            elif 'text' == 'multimodal' or 'text' == 'video':
                import numpy as np
                from PIL import Image
                dummy_image = Image.new('RGB', (224, 224), color='white')
                inputs = self.processor(images=dummy_image, text="Test input", return_tensors="pt")
            else:
                inputs = self.tokenizer("Test input for t5-small", return_tensors="pt")
            
            # Move inputs to device if not CPU
            if device != "cpu":
                inputs = {k: v.to(device) for k, v in inputs.items()}
            
            # Run inference
            with torch.no_grad():
                outputs = self.model(**inputs)
            
            # Verify outputs based on model type
            self.assertIsNotNone(outputs)
            # Different models return different output structures
            if 'text' == 'text':
                if hasattr(outputs, 'last_hidden_state'):
                    self.assertIsNotNone(outputs.last_hidden_state)
                else:
                    # Some models might have alternative output structures
                    self.assertTrue(any(key in outputs for key in ['last_hidden_state', 'hidden_states', 'logits']))
            elif 'text' in ['audio', 'vision', 'video']:
                if hasattr(outputs, 'logits'):
                    self.assertIsNotNone(outputs.logits)
                else:
                    # Some models might have alternative output structures
                    self.assertTrue(any(key in outputs for key in ['logits', 'embedding', 'last_hidden_state']))
            elif 'text' == 'multimodal':
                # CLIP, LLAVA, etc. might have different output structures
                self.assertTrue(any(hasattr(outputs, attr) for attr in ['text_embeds', 'image_embeds', 'last_hidden_state', 'logits']))
            
            # Log success
            logger.info(f"Successfully tested {self.model_id} on cpu")

        except Exception as e:
            logger.error(f"Error testing {self.model_id} on cpu: {str(e)}")
            raise

    def test_cuda(self):
        """Test t5-small with cuda."""
        # Skip if hardware not available
        if not HAS_CUDA: self.skipTest('CUDA not available')
        
        # Set up device
        device = "cuda"

        
        try:
            # Initialize tokenizer and model based on modality
            if 'text' == 'audio':
                from transformers import AutoFeatureExtractor, AutoModelForAudioClassification
                self.processor = AutoFeatureExtractor.from_pretrained(self.model_id)
                self.model = AutoModelForAudioClassification.from_pretrained(self.model_id)
            elif 'text' == 'vision':
                from transformers import AutoImageProcessor, AutoModelForImageClassification
                self.processor = AutoImageProcessor.from_pretrained(self.model_id)
                self.model = AutoModelForImageClassification.from_pretrained(self.model_id)
            elif 'text' == 'multimodal':
                from transformers import AutoProcessor, AutoModel
                self.processor = AutoProcessor.from_pretrained(self.model_id)
                self.model = AutoModel.from_pretrained(self.model_id)
            elif 'text' == 'video':
                from transformers import AutoProcessor, AutoModelForVideoClassification
                self.processor = AutoProcessor.from_pretrained(self.model_id)
                self.model = AutoModelForVideoClassification.from_pretrained(self.model_id)
            else:
                # Default to text models
                self.tokenizer = AutoTokenizer.from_pretrained(self.model_id)
                self.model = AutoModel.from_pretrained(self.model_id)
            
            # Move model to device if not CPU
            if device != "cpu":
                self.model = self.model.to(device)
            
            # Prepare input based on modality
            if 'text' == 'text':
                inputs = self.tokenizer("Test input for t5-small", return_tensors="pt")
            elif 'text' == 'audio':
                import numpy as np
                sample_rate = 16000
                dummy_audio = np.random.random(sample_rate)
                inputs = self.processor(dummy_audio, sampling_rate=sample_rate, return_tensors="pt")
            elif 'text' == 'vision':
                import numpy as np
                from PIL import Image
                dummy_image = Image.new('RGB', (224, 224), color='white')
                inputs = self.processor(images=dummy_image, return_tensors="pt")
            elif 'text' == 'multimodal' or 'text' == 'video':
                import numpy as np
                from PIL import Image
                dummy_image = Image.new('RGB', (224, 224), color='white')
                inputs = self.processor(images=dummy_image, text="Test input", return_tensors="pt")
            else:
                inputs = self.tokenizer("Test input for t5-small", return_tensors="pt")
            
            # Move inputs to device if not CPU
            if device != "cpu":
                inputs = {k: v.to(device) for k, v in inputs.items()}
            
            # Run inference
            with torch.no_grad():
                outputs = self.model(**inputs)
            
            # Verify outputs based on model type
            self.assertIsNotNone(outputs)
            # Different models return different output structures
            if 'text' == 'text':
                if hasattr(outputs, 'last_hidden_state'):
                    self.assertIsNotNone(outputs.last_hidden_state)
                else:
                    # Some models might have alternative output structures
                    self.assertTrue(any(key in outputs for key in ['last_hidden_state', 'hidden_states', 'logits']))
            elif 'text' in ['audio', 'vision', 'video']:
                if hasattr(outputs, 'logits'):
                    self.assertIsNotNone(outputs.logits)
                else:
                    # Some models might have alternative output structures
                    self.assertTrue(any(key in outputs for key in ['logits', 'embedding', 'last_hidden_state']))
            elif 'text' == 'multimodal':
                # CLIP, LLAVA, etc. might have different output structures
                self.assertTrue(any(hasattr(outputs, attr) for attr in ['text_embeds', 'image_embeds', 'last_hidden_state', 'logits']))
            
            # Log success
            logger.info(f"Successfully tested {self.model_id} on cuda")

        except Exception as e:
            logger.error(f"Error testing {self.model_id} on cuda: {str(e)}")
            raise

if __name__ == "__main__":
    unittest.main()
