#!/usr/bin/env python3
"""
Predictive Performance System Demo for the IPFS Accelerate framework.

This script demonstrates how to use the enhanced Predictive Performance System
to make hardware recommendations and predict performance metrics for various models
across different hardware platforms.

Usage:
  python predictive_performance_demo.py --train
  python predictive_performance_demo.py --predict-all
  python predictive_performance_demo.py --compare
"""

import os
import sys
import json
import time
import argparse
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple, Union
from datetime import datetime

# Import the core modules
from model_performance_predictor import (
    load_benchmark_data,
    preprocess_data,
    train_prediction_models,
    save_prediction_models,
    load_prediction_models,
    predict_performance,
    generate_prediction_matrix,
    visualize_predictions,
    PREDICTION_METRICS,
    MODEL_CATEGORIES,
    HARDWARE_CATEGORIES
)

# Configure paths
PROJECT_ROOT = Path(os.path.dirname(os.path.abspath(__file__)))
BENCHMARK_DIR = PROJECT_ROOT / "benchmark_results"
DEMO_OUTPUT_DIR = PROJECT_ROOT / "predictive_performance_demo_output"
DEMO_OUTPUT_DIR.mkdir(exist_ok=True, parents=True)

# Configure model and hardware test cases
TEST_MODELS = [
    {"name": "bert-base-uncased", "category": "text_embedding"},
    {"name": "t5-small", "category": "text_generation"},
    {"name": "facebook/opt-125m", "category": "text_generation"},
    {"name": "openai/whisper-tiny", "category": "audio"},
    {"name": "google/vit-base-patch16-224", "category": "vision"},
    {"name": "openai/clip-vit-base-patch32", "category": "multimodal"}
]

TEST_HARDWARE = ["cpu", "cuda", "mps", "openvino", "webgpu"]
TEST_BATCH_SIZES = [1, 8, 32]
TEST_PRECISIONS = ["fp32", "fp16"]

def print_header(title: str):
    """Print a formatted header."""
    print("\n" + "=" * 80)
    print(f" {title} ".center(80, "="))
    print("=" * 80 + "\n")

def train_demo_models():
    """Train the predictive performance models for the demo."""
    print_header("Training Predictive Performance Models")
    
    # Load benchmark data
    db_path = os.environ.get("BENCHMARK_DB_PATH", str(BENCHMARK_DIR / "benchmark_db.duckdb"))
    print(f"Loading benchmark data from {db_path}...")
    df = load_benchmark_data(db_path)
    
    if df.empty:
        print("No benchmark data available. Please run benchmarks first or check database path.")
        sys.exit(1)
    
    print(f"Loaded {len(df)} benchmark records")
    
    # Preprocess data
    print("Preprocessing benchmark data...")
    df, preprocessing_info = preprocess_data(df)
    
    if df.empty:
        print("Error preprocessing data")
        sys.exit(1)
    
    # Train models with advanced features
    print("Training prediction models with advanced features...")
    start_time = time.time()
    
    models = train_prediction_models(
        df, 
        preprocessing_info,
        test_size=0.2,
        random_state=42,
        hyperparameter_tuning=True,
        use_ensemble=True
    )
    
    training_time = time.time() - start_time
    
    if not models:
        print("Error training models")
        sys.exit(1)
    
    # Print model metrics
    print(f"\nTraining completed in {training_time:.2f} seconds")
    
    for target in PREDICTION_METRICS:
        if target in models:
            metrics = models[target].get("metrics", {})
            print(f"\nModel metrics for {target}:")
            print(f"  Test RÂ²: {metrics.get('test_r2', 'N/A'):.4f}")
            print(f"  MAPE: {metrics.get('mape', 'N/A'):.2%}")
            print(f"  RMSE: {metrics.get('rmse', 'N/A'):.4f}")
            
            # Print top feature importances if available
            if "feature_importance" in metrics and isinstance(metrics["feature_importance"], dict):
                importances = metrics["feature_importance"]
                print("  Top feature importances:")
                sorted_features = sorted(importances.items(), key=lambda x: x[1], reverse=True)[:5]
                for feature, importance in sorted_features:
                    print(f"    {feature}: {importance:.4f}")
    
    # Save models
    output_dir = DEMO_OUTPUT_DIR / "models"
    model_dir = save_prediction_models(models, str(output_dir))
    
    if not model_dir:
        print("Error saving models")
        sys.exit(1)
    
    print(f"\nTrained prediction models saved to {model_dir}")
    
    return models

def predict_all_combinations(models: Dict[str, Any]):
    """Predict performance for all test combinations."""
    print_header("Predicting Performance for All Test Combinations")
    
    # Create results container
    results = {}
    
    # Track time
    start_time = time.time()
    
    # Make predictions for all combinations
    total_combinations = len(TEST_MODELS) * len(TEST_HARDWARE) * len(TEST_BATCH_SIZES) * len(TEST_PRECISIONS)
    print(f"Making {total_combinations} predictions...")
    
    for model_info in TEST_MODELS:
        model_name = model_info["name"]
        model_category = model_info["category"]
        model_short_name = model_name.split("/")[-1]
        
        results[model_short_name] = {}
        
        for hardware in TEST_HARDWARE:
            results[model_short_name][hardware] = {}
            
            for batch_size in TEST_BATCH_SIZES:
                results[model_short_name][hardware][batch_size] = {}
                
                for precision in TEST_PRECISIONS:
                    # Skip incompatible combinations
                    if precision == "fp16" and hardware == "cpu":
                        continue
                    
                    # Make prediction
                    prediction = predict_performance(
                        models=models,
                        model_name=model_name,
                        model_category=model_category,
                        hardware=hardware,
                        batch_size=batch_size,
                        precision=precision,
                        mode="inference",
                        calculate_uncertainty=True
                    )
                    
                    # Store prediction
                    if prediction:
                        results[model_short_name][hardware][batch_size][precision] = prediction
    
    prediction_time = time.time() - start_time
    print(f"Completed predictions in {prediction_time:.2f} seconds")
    
    # Save results
    output_file = DEMO_OUTPUT_DIR / "prediction_results.json"
    with open(output_file, "w") as f:
        json.dump(results, f, indent=2)
    
    print(f"Saved predictions to {output_file}")
    
    # Print some sample results
    print("\nSample prediction results:")
    
    for model_short_name in list(results.keys())[:2]:
        for hardware in list(results[model_short_name].keys())[:2]:
            batch_size = TEST_BATCH_SIZES[0]
            precision = TEST_PRECISIONS[0]
            
            if precision in results[model_short_name][hardware][batch_size]:
                prediction = results[model_short_name][hardware][batch_size][precision]
                
                print(f"\nModel: {model_short_name}, Hardware: {hardware}, Batch Size: {batch_size}, Precision: {precision}")
                
                for metric in PREDICTION_METRICS:
                    if metric in prediction.get("predictions", {}):
                        value = prediction["predictions"][metric]
                        
                        if metric in prediction.get("uncertainties", {}):
                            uncertainty = prediction["uncertainties"][metric]
                            confidence = uncertainty.get("confidence", 0.0) * 100
                            
                            print(f"  {metric}: {value:.2f} (confidence: {confidence:.1f}%)")
                            print(f"    Range: {uncertainty.get('lower_bound', 0.0):.2f} - {uncertainty.get('upper_bound', 0.0):.2f}")
                        else:
                            print(f"  {metric}: {value:.2f}")
                
                print(f"  Overall Confidence: {prediction.get('confidence_score', 0.0) * 100:.1f}%")
    
    return results

def generate_comparison_visuals(results: Dict[str, Any]):
    """Generate comparison visualizations from prediction results."""
    print_header("Generating Comparison Visualizations")
    
    # Create output directory
    vis_dir = DEMO_OUTPUT_DIR / "visualizations"
    vis_dir.mkdir(exist_ok=True, parents=True)
    
    # Set plot style
    sns.set_style("whitegrid")
    plt.rcParams["figure.figsize"] = (12, 8)
    
    # 1. Hardware comparison for each model (throughput)
    print("Generating hardware comparison charts...")
    
    for model_short_name in results:
        data = []
        
        for hardware in results[model_short_name]:
            batch_size = 8  # Use fixed batch size for this comparison
            
            if batch_size in results[model_short_name][hardware]:
                for precision in results[model_short_name][hardware][batch_size]:
                    prediction = results[model_short_name][hardware][batch_size][precision]
                    
                    if "predictions" in prediction and "throughput" in prediction["predictions"]:
                        throughput = prediction["predictions"]["throughput"]
                        
                        # Get confidence if available
                        confidence = 1.0
                        if "uncertainties" in prediction and "throughput" in prediction["uncertainties"]:
                            confidence = prediction["uncertainties"]["throughput"].get("confidence", 1.0)
                        
                        data.append({
                            "hardware": hardware,
                            "precision": precision,
                            "throughput": throughput,
                            "confidence": confidence
                        })
        
        if data:
            # Create DataFrame
            df = pd.DataFrame(data)
            
            # Create plot
            plt.figure()
            
            # Use confidence for alpha
            sns.barplot(x="hardware", y="throughput", hue="precision", data=df,
                        alpha=df["confidence"].values)
            
            plt.title(f"Throughput Comparison for {model_short_name} (Batch Size = 8)")
            plt.xlabel("Hardware")
            plt.ylabel("Throughput (samples/sec)")
            plt.xticks(rotation=45)
            plt.legend(title="Precision")
            plt.tight_layout()
            
            # Save plot
            output_file = vis_dir / f"{model_short_name}_hardware_comparison.png"
            plt.savefig(output_file)
            plt.close()
    
    # 2. Batch size scaling for each model and hardware (throughput)
    print("Generating batch size scaling charts...")
    
    for model_short_name in results:
        for hardware in results[model_short_name]:
            data = []
            
            for batch_size in results[model_short_name][hardware]:
                for precision in results[model_short_name][hardware][batch_size]:
                    prediction = results[model_short_name][hardware][batch_size][precision]
                    
                    if "predictions" in prediction and "throughput" in prediction["predictions"]:
                        throughput = prediction["predictions"]["throughput"]
                        
                        data.append({
                            "batch_size": batch_size,
                            "precision": precision,
                            "throughput": throughput
                        })
            
            if data:
                # Create DataFrame
                df = pd.DataFrame(data)
                
                # Create plot
                plt.figure()
                
                # Create line plot
                for precision in df["precision"].unique():
                    df_precision = df[df["precision"] == precision].sort_values("batch_size")
                    plt.plot(df_precision["batch_size"], df_precision["throughput"], marker='o', label=precision)
                
                plt.title(f"Throughput Scaling with Batch Size for {model_short_name} on {hardware}")
                plt.xlabel("Batch Size")
                plt.ylabel("Throughput (samples/sec)")
                plt.legend(title="Precision")
                plt.grid(True)
                plt.tight_layout()
                
                # Save plot
                output_file = vis_dir / f"{model_short_name}_{hardware}_batch_scaling.png"
                plt.savefig(output_file)
                plt.close()
    
    # 3. Model comparison across hardware (latency)
    print("Generating model comparison charts...")
    
    for hardware in TEST_HARDWARE:
        data = []
        
        for model_short_name in results:
            if hardware in results[model_short_name]:
                batch_size = 1  # Use batch size 1 for latency comparison
                
                if batch_size in results[model_short_name][hardware]:
                    precision = "fp32"  # Use fp32 for consistent comparison
                    
                    if precision in results[model_short_name][hardware][batch_size]:
                        prediction = results[model_short_name][hardware][batch_size][precision]
                        
                        if "predictions" in prediction and "latency_mean" in prediction["predictions"]:
                            latency = prediction["predictions"]["latency_mean"]
                            
                            data.append({
                                "model": model_short_name,
                                "latency": latency
                            })
        
        if data:
            # Create DataFrame
            df = pd.DataFrame(data)
            
            # Create plot
            plt.figure()
            
            # Sort by latency
            df = df.sort_values("latency")
            
            # Create bar plot
            sns.barplot(x="model", y="latency", data=df)
            
            plt.title(f"Latency Comparison for Models on {hardware}")
            plt.xlabel("Model")
            plt.ylabel("Latency (ms)")
            plt.xticks(rotation=45, ha="right")
            plt.tight_layout()
            
            # Save plot
            output_file = vis_dir / f"{hardware}_model_latency_comparison.png"
            plt.savefig(output_file)
            plt.close()
    
    # 4. Generate uncertainty visualization for one model
    print("Generating uncertainty visualization...")
    
    model_short_name = list(results.keys())[0]
    hardware = "cuda" if "cuda" in results[model_short_name] else list(results[model_short_name].keys())[0]
    
    data = []
    
    for batch_size in results[model_short_name][hardware]:
        for precision in results[model_short_name][hardware][batch_size]:
            prediction = results[model_short_name][hardware][batch_size][precision]
            
            if "predictions" in prediction and "throughput" in prediction["predictions"]:
                throughput = prediction["predictions"]["throughput"]
                
                # Get uncertainty if available
                lower_bound = throughput * 0.85
                upper_bound = throughput * 1.15
                
                if "uncertainties" in prediction and "throughput" in prediction["uncertainties"]:
                    uncertainty = prediction["uncertainties"]["throughput"]
                    lower_bound = uncertainty.get("lower_bound", lower_bound)
                    upper_bound = uncertainty.get("upper_bound", upper_bound)
                
                data.append({
                    "batch_size": batch_size,
                    "precision": precision,
                    "throughput": throughput,
                    "lower_bound": lower_bound,
                    "upper_bound": upper_bound
                })
    
    if data:
        # Create DataFrame
        df = pd.DataFrame(data)
        
        # Create plot
        plt.figure()
        
        # Plot with error bars for uncertainty
        for precision in df["precision"].unique():
            df_precision = df[df["precision"] == precision].sort_values("batch_size")
            plt.errorbar(
                df_precision["batch_size"],
                df_precision["throughput"],
                yerr=[
                    df_precision["throughput"] - df_precision["lower_bound"],
                    df_precision["upper_bound"] - df_precision["throughput"]
                ],
                marker='o',
                label=precision,
                capsize=5
            )
        
        plt.title(f"Throughput with Uncertainty for {model_short_name} on {hardware}")
        plt.xlabel("Batch Size")
        plt.ylabel("Throughput (samples/sec)")
        plt.legend(title="Precision")
        plt.grid(True)
        plt.tight_layout()
        
        # Save plot
        output_file = vis_dir / f"{model_short_name}_{hardware}_uncertainty.png"
        plt.savefig(output_file)
        plt.close()
    
    # 5. Generate comprehensive matrix
    print("Generating comprehensive performance matrix...")
    
    # Create directory for full matrix file
    matrix_dir = DEMO_OUTPUT_DIR / "matrix"
    matrix_dir.mkdir(exist_ok=True, parents=True)
    
    # Load models
    models_dir = DEMO_OUTPUT_DIR / "models" / "latest"
    models = load_prediction_models(str(models_dir))
    
    if not models:
        print("Error loading models for matrix generation")
        return
    
    # Generate matrix
    matrix = generate_prediction_matrix(
        models=models,
        model_configs=TEST_MODELS,
        hardware_platforms=TEST_HARDWARE,
        batch_sizes=TEST_BATCH_SIZES,
        precision_options=TEST_PRECISIONS,
        mode="inference",
        output_file=str(matrix_dir / "prediction_matrix.json")
    )
    
    if not matrix:
        print("Error generating prediction matrix")
        return
    
    # Generate visualizations
    visualization_files = visualize_predictions(
        matrix=matrix,
        metric="throughput",
        output_dir=str(vis_dir)
    )
    
    visualization_files.extend(visualize_predictions(
        matrix=matrix,
        metric="latency_mean",
        output_dir=str(vis_dir)
    ))
    
    visualization_files.extend(visualize_predictions(
        matrix=matrix,
        metric="memory_usage",
        output_dir=str(vis_dir)
    ))
    
    print(f"Generated {len(visualization_files)} visualizations")
    print("\nVisualization files:")
    for file in visualization_files:
        print(f"  - {file}")
    
    print(f"\nAll visualizations saved to {vis_dir}")

def main():
    """Main function."""
    parser = argparse.ArgumentParser(description="Predictive Performance System Demo")
    
    group = parser.add_mutually_exclusive_group(required=True)
    group.add_argument("--train", action="store_true", help="Train prediction models")
    group.add_argument("--predict-all", action="store_true", help="Make predictions for all test combinations")
    group.add_argument("--compare", action="store_true", help="Generate comparison visualizations")
    group.add_argument("--full-demo", action="store_true", help="Run full demonstration (train, predict, visualize)")
    
    parser.add_argument("--output-dir", help="Directory to save output files")
    parser.add_argument("--db-path", help="Path to benchmark database")
    
    args = parser.parse_args()
    
    # Set output directory if specified
    if args.output_dir:
        global DEMO_OUTPUT_DIR
        DEMO_OUTPUT_DIR = Path(args.output_dir)
        DEMO_OUTPUT_DIR.mkdir(exist_ok=True, parents=True)
    
    # Set database path if specified
    if args.db_path:
        os.environ["BENCHMARK_DB_PATH"] = args.db_path
    
    if args.train or args.full_demo:
        models = train_demo_models()
    else:
        # Load models
        models_dir = DEMO_OUTPUT_DIR / "models" / "latest"
        models = load_prediction_models(str(models_dir))
        
        if not models:
            print("Error loading models. Please run with --train first.")
            sys.exit(1)
    
    if args.predict_all or args.full_demo:
        results = predict_all_combinations(models)
    else:
        # Load results
        results_file = DEMO_OUTPUT_DIR / "prediction_results.json"
        
        if not results_file.exists():
            print("No prediction results found. Please run with --predict-all first.")
            sys.exit(1)
            
        with open(results_file, "r") as f:
            results = json.load(f)
    
    if args.compare or args.full_demo:
        generate_comparison_visuals(results)
    
    if args.full_demo:
        print_header("Full Demonstration Completed")
        print(f"All demonstration files saved to {DEMO_OUTPUT_DIR}")
        print("\nTo explore the results, check:")
        print(f"  - Models: {DEMO_OUTPUT_DIR / 'models'}")
        print(f"  - Prediction results: {DEMO_OUTPUT_DIR / 'prediction_results.json'}")
        print(f"  - Visualizations: {DEMO_OUTPUT_DIR / 'visualizations'}")
        print(f"  - Performance matrix: {DEMO_OUTPUT_DIR / 'matrix' / 'prediction_matrix.json'}")

if __name__ == "__main__":
    main()