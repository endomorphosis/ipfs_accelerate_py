#!/usr/bin/env python3
# Generated by TemplateRenderer on 2025-03-16 15:46:01
# Model: bert-base-uncased
# Template: text_embedding_benchmark (78704076-d9c5-4ce3-b455-cfa109dec772)
# Hardware: cuda
# Type: benchmark

#!/usr/bin/env python3
"""
Template for testing text embedding models (BERT, RoBERTa, etc.) on different hardware platforms.

This template includes placeholders that will be replaced with actual values during test generation:
- bert-base-uncased: Name of the model (e.g., "bert-base-uncased")
- text_embedding: Family of the model (e.g., "text_embedding")
- cuda: Type of hardware to run on (e.g., "cpu", "cuda", "rocm")
- 1: Batch size for testing (e.g., 1, 4, 8)
"""

import os
import time
import logging
import torch
from transformers import AutoModel, AutoTokenizer

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger("test_text_embedding")

def test_bert_base_uncased_on_cuda():
    """Test bert-base-uncased model on cuda hardware with batch size 1."""
    
    # Record start time for performance measurement
    start_time = time.time()
    
    # Log test configuration
    logger.info(f"Testing model: bert-base-uncased")
    logger.info(f"Hardware: cuda")
    logger.info(f"Batch size: 1")
    
    try:
        # Initialize tokenizer and model
        tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
        model = AutoModel.from_pretrained("bert-base-uncased")
        
        # Move model to appropriate device
        device = "cuda"
        if device != "cpu":
            model = model.to(device)
        
        # Log model information
        logger.info(f"Model loaded successfully: {model.__class__.__name__}")
        
        # Prepare input text
        input_text = "This is a sample text for embedding model testing with batch size 1"
        
        # Tokenize input
        inputs = tokenizer(input_text, return_tensors="pt")
        if device != "cpu":
            inputs = {k: v.to(device) for k, v in inputs.items()}
        
        # Create batch by repeating the input
        batch_inputs = {k: v.repeat(1, 1) for k, v in inputs.items()}
        
        # Measure inference time
        inference_start = time.time()
        
        # Run inference
        with torch.no_grad():
            outputs = model(**batch_inputs)
        
        # Calculate inference time
        inference_time = time.time() - inference_start
        
        # Extract embeddings
        embeddings = outputs.last_hidden_state
        
        # Log results
        logger.info(f"Inference completed in {inference_time:.4f} seconds")
        logger.info(f"Output shape: {embeddings.shape}")
        
        # Measure memory usage
        if device == "cuda":
            memory_usage = torch.cuda.max_memory_allocated() / (1024 ** 2)  # MB
        else:
            import psutil
            memory_usage = psutil.Process(os.getpid()).memory_info().rss / (1024 ** 2)  # MB
            
        logger.info(f"Memory usage: {memory_usage:.2f} MB")
        
        # Calculate total execution time
        execution_time = time.time() - start_time
        
        # Return test results
        return {
            "test_id": "642a6b88-94d5-4da6-ace6-d14e9aac200a",  # Will be replaced with a UUID during generation
            "model_name": "bert-base-uncased",
            "model_family": "text_embedding",
            "hardware_type": "cuda",
            "batch_size": 1,
            "execution_time": execution_time,
            "inference_time": inference_time,
            "memory_usage": memory_usage,
            "embedding_shape": embeddings.shape,
            "success": True
        }
        
    except Exception as e:
        # Log error and return failure result
        logger.error(f"Test failed: {str(e)}")
        
        # Calculate total execution time
        execution_time = time.time() - start_time
        
        return {
            "test_id": "642a6b88-94d5-4da6-ace6-d14e9aac200a",
            "model_name": "bert-base-uncased",
            "model_family": "text_embedding",
            "hardware_type": "cuda",
            "batch_size": 1,
            "execution_time": execution_time,
            "success": False,
            "error_message": str(e)
        }

if __name__ == "__main__":
    # This allows the template to be run directly for testing
    result = test_bert_base_uncased_on_cuda()
    print(f"Test result: {'Success' if result['success'] else 'Failure'}")
    if result['success']:
        print(f"Execution time: {result['execution_time']:.4f} seconds")
        print(f"Inference time: {result['inference_time']:.4f} seconds")
        print(f"Memory usage: {result['memory_usage']:.2f} MB")
        print(f"Embedding shape: {result['embedding_shape']}")