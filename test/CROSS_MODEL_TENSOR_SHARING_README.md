# Cross-Model Tensor Sharing System

> **Implementation Completed: March 28, 2025**

## Overview

The Cross-Model Tensor Sharing system enables efficient sharing of tensors between multiple models, significantly improving memory efficiency and performance for multi-model workloads. This feature has been added to the IPFS Accelerate JavaScript SDK ahead of schedule, as part of our ongoing enhancements to the TypeScript SDK.

## Key Features

- **Memory Reduction**: Up to 30% memory reduction for common multi-model workflows
- **Inference Speedup**: Up to 30% faster inference when reusing cached embeddings
- **Reference Counting**: Intelligent memory management with automatic cleanup
- **Zero-Copy Views**: Create views into tensors without duplicating memory
- **Persistence**: Store shared tensors using IndexedDB for offline use
- **WebNN Integration**: Seamless integration with WebNN hardware acceleration

## Compatible Model Combinations

| Tensor Type | Compatible Models | Description |
|-------------|------------------|-------------|
| text_embedding | BERT, T5, LLAMA, BART | Text embeddings for NLP models |
| vision_embedding | ViT, CLIP, DETR | Vision embeddings for image models |
| audio_embedding | Whisper, Wav2Vec2, CLAP | Audio embeddings for speech/audio models |
| vision_text_joint | CLIP, LLaVA, BLIP | Joint embeddings for multimodal models |
| audio_text_joint | CLAP, Whisper-Text | Joint embeddings for audio-text models |

## Implementation Files

- **[CROSS_MODEL_TENSOR_SHARING_GUIDE.md](CROSS_MODEL_TENSOR_SHARING_GUIDE.md)** - Comprehensive documentation
- **[ipfs_accelerate_js_tensor_sharing_integration.ts](ipfs_accelerate_js_tensor_sharing_integration.ts)** - Integration with Storage Manager
- **[ipfs_accelerate_js_tensor_sharing_integration.test.ts](ipfs_accelerate_js_tensor_sharing_integration.test.ts)** - Unit tests
- **[ipfs_accelerate_js_tensor_sharing_example.ts](ipfs_accelerate_js_tensor_sharing_example.ts)** - Basic example usage
- **[ipfs_accelerate_js_multimodal_tensor_sharing_example.ts](ipfs_accelerate_js_multimodal_tensor_sharing_example.ts)** - Advanced multimodal example
- **[TensorSharingDemo.html](TensorSharingDemo.html)** - Interactive browser demo
- **[ipfs_accelerate_js/src/tensor/shared_tensor.ts](ipfs_accelerate_js/src/tensor/shared_tensor.ts)** - Core implementation

## Usage Example

```typescript
import { TensorSharingIntegration, WebNNBackend } from 'ipfs-accelerate-js';

// Create a WebNN backend
const webnnBackend = new WebNNBackend({
  enableLogging: true,
  preferredDeviceType: 'gpu'
});

// Initialize the tensor sharing integration
const integration = new TensorSharingIntegration({
  enablePersistence: true,
  enableLogging: true,
  maxMemoryMb: 2048 // 2GB limit
});

// Initialize with WebNN backend
await integration.initialize(webnnBackend);

// Create a shared tensor (e.g., BERT text embedding)
const textEmbedding = await integration.registerSharedTensor(
  "text_embedding",
  [1, 768], // [batch_size, embedding_dim]
  textEmbeddingData, // Float32Array
  "cpu",
  "bert-base-uncased", // Producer model
  null // No initial consumers
);

// Share with other models
const result = await integration.shareTensorBetweenModels(
  "text_embedding",
  "bert-base-uncased", // Source model
  ["t5-base", "bart-large"] // Target models
);
```

## Performance Benchmarks

| Scenario | Without Sharing | With Sharing | Improvement |
|----------|-----------------|--------------|-------------|
| BERT+T5+DistilBERT | 12.3 MB | 8.1 MB | 34% memory reduction |
| ViT+CLIP | 9.2 MB | 6.8 MB | 26% memory reduction |
| BERT â†’ T5 inference | 42 ms | 33 ms | 21% faster inference |
| Multi-model pipeline | 85 ms | 63 ms | 26% faster pipeline |

## Examples

### Basic Tensor Sharing

The basic example demonstrates sharing text and vision embeddings between different models:

```typescript
// From ipfs_accelerate_js_tensor_sharing_example.ts
// Share text embedding with T5 model
const t5Result = await integration.shareTensorBetweenModels(
  "text_embedding",
  "bert-base-uncased", // Source model
  ["t5-base"] // Target model
);

// Share vision embedding with CLIP model
const clipResult = await integration.shareTensorBetweenModels(
  "vision_embedding",
  "vit-base-patch16-224", // Source model
  ["clip-vit-base-patch16"] // Target model
);

// Create a view for a smaller model
const embeddingView = await integration.createTensorView(
  "text_embedding", // Parent tensor
  "text_embedding_half", // View name
  [0, 0], // Start offset
  [1, 384], // Half the embedding size
  "distilbert-base-uncased" // Model using this view
);
```

### Advanced Multimodal Example

The advanced example demonstrates a realistic multimodal workflow:

```typescript
// From ipfs_accelerate_js_multimodal_tensor_sharing_example.ts
// Process the image with ViT
const imageEmbedding = await vitModel.encode(imageData);
const imageSharedTensor = await integration.registerSharedTensor(
  "image_embedding",
  vitModel.shape,
  imageEmbedding,
  "webgpu", // Store on GPU for efficiency
  "vit-base-patch16", // Producer model
  ["clip-vit-base", "captioning-model"] // Initial consumers
);

// Process the text with BERT
const textEmbedding = await bertModel.encode(text);
const textSharedTensor = await integration.registerSharedTensor(
  "text_embedding",
  bertModel.shape,
  textEmbedding,
  "cpu", // Keep on CPU initially
  "bert-base-uncased", // Producer model
  ["clip-vit-base", "qa-model"] // Initial consumers
);

// Create a joint embedding with CLIP
const imageEmbeddingForClip = await integration.getSharedTensor("image_embedding", "clip-vit-base");
const textEmbeddingForClip = await integration.getSharedTensor("text_embedding", "clip-vit-base");

const jointEmbedding = await clipModel.createJointEmbedding(
  imageEmbeddingForClip.data as Float32Array,
  textEmbeddingForClip.data as Float32Array
);

// Use the shared image embedding to generate a caption
const imageEmbeddingForCaption = await integration.getSharedTensor("image_embedding", "captioning-model");
const caption = await captioningModel.generateCaption(imageEmbeddingForCaption.data as Float32Array);

// Use the shared text embedding to answer a question
const textEmbeddingForQA = await integration.getSharedTensor("text_embedding", "qa-model");
const answer = await qaModel.answerQuestion(textEmbeddingForQA.data as Float32Array, question);
```

### Memory Management

```typescript
// Analyze sharing opportunities
const opportunities = integration.analyzeSharingOpportunities();

// Get memory usage by tensor
const tensorMemoryUsage = integration.getTensorMemoryUsage();

// Get memory usage by model
const modelMemoryUsage = integration.getModelMemoryUsage();

// Get optimization recommendations
const recommendations = integration.getOptimizationRecommendations();

// Optimize memory usage
const optimizationResult = await integration.optimizeMemoryUsage();

// Save to persistent storage
const syncResult = await integration.synchronizePersistentStorage();
```

## Next Steps

- Integration with WebGPU compute shader operations (Planned - April 2025)
- Advanced operation fusion for better performance (Planned - April 2025)
- Browser-specific optimizations for improved sharing (Planned - April 2025)
- Zero-copy sharing between WebGPU and WebNN backends (Planned - April 2025)
- Automated sharing pattern detection (Planned - May 2025)

For complete details, see the [CROSS_MODEL_TENSOR_SHARING_GUIDE.md](CROSS_MODEL_TENSOR_SHARING_GUIDE.md) documentation.