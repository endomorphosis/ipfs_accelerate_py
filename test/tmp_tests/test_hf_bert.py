#!/usr/bin/env python3
"""
Test for bert model with hardware platform support
Generated by fixed_merged_test_generator.py
"""

import os
import sys
import unittest
import importlib.util
import logging
import torch
import numpy as np
from transformers import AutoModel, AutoTokenizer, AutoConfig, AutoFeatureExtractor, AutoProcessor, AutoImageProcessor, AutoModelForImageClassification, AutoModelForAudioClassification, AutoModelForVideoClassification

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Hardware detection
HAS_CUDA = torch.cuda.is_available()
HAS_ROCM = (HAS_CUDA and hasattr(torch, '_C') and hasattr(torch._C, '_rocm_version')) or ('ROCM_HOME' in os.environ)
HAS_MPS = hasattr(torch, "mps") and hasattr(torch.mps, "is_available") and torch.mps.is_available()
HAS_OPENVINO = importlib.util.find_spec("openvino") is not None
HAS_QUALCOMM = importlib.util.find_spec("qnn_wrapper") is not None or importlib.util.find_spec("qti") is not None
HAS_WEBNN = importlib.util.find_spec("webnn") is not None or "WEBNN_AVAILABLE" in os.environ
HAS_WEBGPU = importlib.util.find_spec("webgpu") is not None or "WEBGPU_AVAILABLE" in os.environ

# Try to import centralized hardware detection
try:
    from centralized_hardware_detection import hardware_detection
    HAS_CENTRALIZED_DETECTION = True
except ImportError:
    HAS_CENTRALIZED_DETECTION = False

class TestBertModels(unittest.TestCase):
    """Test bert model with cross-platform hardware support."""
    
    def setUp(self):
        """Set up the test environment."""
        self.model_id = "bert-base-uncased"
        self.tokenizer = None
        self.model = None
        self.processor = None
        self.modality = "text"
        
        # Detect hardware capabilities if available
        if HAS_CENTRALIZED_DETECTION:
            self.hardware_capabilities = hardware_detection.detect_hardware_capabilities()
        else:
            self.hardware_capabilities = {
                "cuda": HAS_CUDA,
                "rocm": HAS_ROCM,
                "mps": HAS_MPS,
                "openvino": HAS_OPENVINO,
                "qualcomm": HAS_QUALCOMM,
                "webnn": HAS_WEBNN,
                "webgpu": HAS_WEBGPU
            }
        
    def run_tests(self):
        """Run all tests for this model."""
        unittest.main()

    def test_cpu(self):
        """Test bert with cpu."""
        # Skip if hardware not available
        if not HAS_CPU: self.skipTest('CPU not available')
        
        # Set up device
        device = "cpu"

        
        try:
            # Initialize tokenizer and model based on modality
            if 'text' == 'audio':
                from transformers import AutoFeatureExtractor, AutoModelForAudioClassification
                self.processor = AutoFeatureExtractor.from_pretrained(self.model_id)
                self.model = AutoModelForAudioClassification.from_pretrained(self.model_id)
            elif 'text' == 'vision':
                from transformers import AutoImageProcessor, AutoModelForImageClassification
                self.processor = AutoImageProcessor.from_pretrained(self.model_id)
                self.model = AutoModelForImageClassification.from_pretrained(self.model_id)
            elif 'text' == 'multimodal':
                from transformers import AutoProcessor, AutoModel
                self.processor = AutoProcessor.from_pretrained(self.model_id)
                self.model = AutoModel.from_pretrained(self.model_id)
            elif 'text' == 'video':
                from transformers import AutoProcessor, AutoModelForVideoClassification
                self.processor = AutoProcessor.from_pretrained(self.model_id)
                self.model = AutoModelForVideoClassification.from_pretrained(self.model_id)
            else:
                # Default to text models
                self.tokenizer = AutoTokenizer.from_pretrained(self.model_id)
                self.model = AutoModel.from_pretrained(self.model_id)
            
            # Move model to device if not CPU
            if device != "cpu":
                self.model = self.model.to(device)
            
            # Prepare input based on modality
            if 'text' == 'text':
                inputs = self.tokenizer("Test input for bert", return_tensors="pt")
            elif 'text' == 'audio':
                import numpy as np
                sample_rate = 16000
                dummy_audio = np.random.random(sample_rate)
                inputs = self.processor(dummy_audio, sampling_rate=sample_rate, return_tensors="pt")
            elif 'text' == 'vision':
                import numpy as np
                from PIL import Image
                dummy_image = Image.new('RGB', (224, 224), color='white')
                inputs = self.processor(images=dummy_image, return_tensors="pt")
            elif 'text' == 'multimodal' or 'text' == 'video':
                import numpy as np
                from PIL import Image
                dummy_image = Image.new('RGB', (224, 224), color='white')
                inputs = self.processor(images=dummy_image, text="Test input", return_tensors="pt")
            else:
                inputs = self.tokenizer("Test input for bert", return_tensors="pt")
            
            # Move inputs to device if not CPU
            if device != "cpu":
                inputs = {k: v.to(device) for k, v in inputs.items()}
            
            # Run inference
            with torch.no_grad():
                outputs = self.model(**inputs)
            
            # Verify outputs based on model type
            self.assertIsNotNone(outputs)
            # Different models return different output structures
            if 'text' == 'text':
                if hasattr(outputs, 'last_hidden_state'):
                    self.assertIsNotNone(outputs.last_hidden_state)
                else:
                    # Some models might have alternative output structures
                    self.assertTrue(any(key in outputs for key in ['last_hidden_state', 'hidden_states', 'logits']))
            elif 'text' in ['audio', 'vision', 'video']:
                if hasattr(outputs, 'logits'):
                    self.assertIsNotNone(outputs.logits)
                else:
                    # Some models might have alternative output structures
                    self.assertTrue(any(key in outputs for key in ['logits', 'embedding', 'last_hidden_state']))
            elif 'text' == 'multimodal':
                # CLIP, LLAVA, etc. might have different output structures
                self.assertTrue(any(hasattr(outputs, attr) for attr in ['text_embeds', 'image_embeds', 'last_hidden_state', 'logits']))
            
            # Log success
            logger.info(f"Successfully tested {self.model_id} on cpu")

        except Exception as e:
            logger.error(f"Error testing {self.model_id} on cpu: {str(e)}")
            raise

    def test_cuda(self):
        """Test bert with cuda."""
        # Skip if hardware not available
        if not HAS_CUDA: self.skipTest('CUDA not available')
        
        # Set up device
        device = "cuda"

        
        try:
            # Initialize tokenizer and model based on modality
            if 'text' == 'audio':
                from transformers import AutoFeatureExtractor, AutoModelForAudioClassification
                self.processor = AutoFeatureExtractor.from_pretrained(self.model_id)
                self.model = AutoModelForAudioClassification.from_pretrained(self.model_id)
            elif 'text' == 'vision':
                from transformers import AutoImageProcessor, AutoModelForImageClassification
                self.processor = AutoImageProcessor.from_pretrained(self.model_id)
                self.model = AutoModelForImageClassification.from_pretrained(self.model_id)
            elif 'text' == 'multimodal':
                from transformers import AutoProcessor, AutoModel
                self.processor = AutoProcessor.from_pretrained(self.model_id)
                self.model = AutoModel.from_pretrained(self.model_id)
            elif 'text' == 'video':
                from transformers import AutoProcessor, AutoModelForVideoClassification
                self.processor = AutoProcessor.from_pretrained(self.model_id)
                self.model = AutoModelForVideoClassification.from_pretrained(self.model_id)
            else:
                # Default to text models
                self.tokenizer = AutoTokenizer.from_pretrained(self.model_id)
                self.model = AutoModel.from_pretrained(self.model_id)
            
            # Move model to device if not CPU
            if device != "cpu":
                self.model = self.model.to(device)
            
            # Prepare input based on modality
            if 'text' == 'text':
                inputs = self.tokenizer("Test input for bert", return_tensors="pt")
            elif 'text' == 'audio':
                import numpy as np
                sample_rate = 16000
                dummy_audio = np.random.random(sample_rate)
                inputs = self.processor(dummy_audio, sampling_rate=sample_rate, return_tensors="pt")
            elif 'text' == 'vision':
                import numpy as np
                from PIL import Image
                dummy_image = Image.new('RGB', (224, 224), color='white')
                inputs = self.processor(images=dummy_image, return_tensors="pt")
            elif 'text' == 'multimodal' or 'text' == 'video':
                import numpy as np
                from PIL import Image
                dummy_image = Image.new('RGB', (224, 224), color='white')
                inputs = self.processor(images=dummy_image, text="Test input", return_tensors="pt")
            else:
                inputs = self.tokenizer("Test input for bert", return_tensors="pt")
            
            # Move inputs to device if not CPU
            if device != "cpu":
                inputs = {k: v.to(device) for k, v in inputs.items()}
            
            # Run inference
            with torch.no_grad():
                outputs = self.model(**inputs)
            
            # Verify outputs based on model type
            self.assertIsNotNone(outputs)
            # Different models return different output structures
            if 'text' == 'text':
                if hasattr(outputs, 'last_hidden_state'):
                    self.assertIsNotNone(outputs.last_hidden_state)
                else:
                    # Some models might have alternative output structures
                    self.assertTrue(any(key in outputs for key in ['last_hidden_state', 'hidden_states', 'logits']))
            elif 'text' in ['audio', 'vision', 'video']:
                if hasattr(outputs, 'logits'):
                    self.assertIsNotNone(outputs.logits)
                else:
                    # Some models might have alternative output structures
                    self.assertTrue(any(key in outputs for key in ['logits', 'embedding', 'last_hidden_state']))
            elif 'text' == 'multimodal':
                # CLIP, LLAVA, etc. might have different output structures
                self.assertTrue(any(hasattr(outputs, attr) for attr in ['text_embeds', 'image_embeds', 'last_hidden_state', 'logits']))
            
            # Log success
            logger.info(f"Successfully tested {self.model_id} on cuda")

        except Exception as e:
            logger.error(f"Error testing {self.model_id} on cuda: {str(e)}")
            raise

    def test_rocm(self):
        """Test bert with rocm."""
        # Skip if hardware not available
        if not HAS_ROCM: self.skipTest('ROCM not available')
        
        # Set up device
        device = "cuda"

        
        try:
            # Initialize tokenizer and model based on modality
            if 'text' == 'audio':
                from transformers import AutoFeatureExtractor, AutoModelForAudioClassification
                self.processor = AutoFeatureExtractor.from_pretrained(self.model_id)
                self.model = AutoModelForAudioClassification.from_pretrained(self.model_id)
            elif 'text' == 'vision':
                from transformers import AutoImageProcessor, AutoModelForImageClassification
                self.processor = AutoImageProcessor.from_pretrained(self.model_id)
                self.model = AutoModelForImageClassification.from_pretrained(self.model_id)
            elif 'text' == 'multimodal':
                from transformers import AutoProcessor, AutoModel
                self.processor = AutoProcessor.from_pretrained(self.model_id)
                self.model = AutoModel.from_pretrained(self.model_id)
            elif 'text' == 'video':
                from transformers import AutoProcessor, AutoModelForVideoClassification
                self.processor = AutoProcessor.from_pretrained(self.model_id)
                self.model = AutoModelForVideoClassification.from_pretrained(self.model_id)
            else:
                # Default to text models
                self.tokenizer = AutoTokenizer.from_pretrained(self.model_id)
                self.model = AutoModel.from_pretrained(self.model_id)
            
            # Move model to device if not CPU
            if device != "cpu":
                self.model = self.model.to(device)
            
            # Prepare input based on modality
            if 'text' == 'text':
                inputs = self.tokenizer("Test input for bert", return_tensors="pt")
            elif 'text' == 'audio':
                import numpy as np
                sample_rate = 16000
                dummy_audio = np.random.random(sample_rate)
                inputs = self.processor(dummy_audio, sampling_rate=sample_rate, return_tensors="pt")
            elif 'text' == 'vision':
                import numpy as np
                from PIL import Image
                dummy_image = Image.new('RGB', (224, 224), color='white')
                inputs = self.processor(images=dummy_image, return_tensors="pt")
            elif 'text' == 'multimodal' or 'text' == 'video':
                import numpy as np
                from PIL import Image
                dummy_image = Image.new('RGB', (224, 224), color='white')
                inputs = self.processor(images=dummy_image, text="Test input", return_tensors="pt")
            else:
                inputs = self.tokenizer("Test input for bert", return_tensors="pt")
            
            # Move inputs to device if not CPU
            if device != "cpu":
                inputs = {k: v.to(device) for k, v in inputs.items()}
            
            # Run inference
            with torch.no_grad():
                outputs = self.model(**inputs)
            
            # Verify outputs based on model type
            self.assertIsNotNone(outputs)
            # Different models return different output structures
            if 'text' == 'text':
                if hasattr(outputs, 'last_hidden_state'):
                    self.assertIsNotNone(outputs.last_hidden_state)
                else:
                    # Some models might have alternative output structures
                    self.assertTrue(any(key in outputs for key in ['last_hidden_state', 'hidden_states', 'logits']))
            elif 'text' in ['audio', 'vision', 'video']:
                if hasattr(outputs, 'logits'):
                    self.assertIsNotNone(outputs.logits)
                else:
                    # Some models might have alternative output structures
                    self.assertTrue(any(key in outputs for key in ['logits', 'embedding', 'last_hidden_state']))
            elif 'text' == 'multimodal':
                # CLIP, LLAVA, etc. might have different output structures
                self.assertTrue(any(hasattr(outputs, attr) for attr in ['text_embeds', 'image_embeds', 'last_hidden_state', 'logits']))
            
            # Log success
            logger.info(f"Successfully tested {self.model_id} on rocm")

        except Exception as e:
            logger.error(f"Error testing {self.model_id} on rocm: {str(e)}")
            raise

    def test_mps(self):
        """Test bert with mps."""
        # Skip if hardware not available
        if not HAS_MPS: self.skipTest('MPS not available')
        
        # Set up device
        device = "mps"

        
        try:
            # Initialize tokenizer and model based on modality
            if 'text' == 'audio':
                from transformers import AutoFeatureExtractor, AutoModelForAudioClassification
                self.processor = AutoFeatureExtractor.from_pretrained(self.model_id)
                self.model = AutoModelForAudioClassification.from_pretrained(self.model_id)
            elif 'text' == 'vision':
                from transformers import AutoImageProcessor, AutoModelForImageClassification
                self.processor = AutoImageProcessor.from_pretrained(self.model_id)
                self.model = AutoModelForImageClassification.from_pretrained(self.model_id)
            elif 'text' == 'multimodal':
                from transformers import AutoProcessor, AutoModel
                self.processor = AutoProcessor.from_pretrained(self.model_id)
                self.model = AutoModel.from_pretrained(self.model_id)
            elif 'text' == 'video':
                from transformers import AutoProcessor, AutoModelForVideoClassification
                self.processor = AutoProcessor.from_pretrained(self.model_id)
                self.model = AutoModelForVideoClassification.from_pretrained(self.model_id)
            else:
                # Default to text models
                self.tokenizer = AutoTokenizer.from_pretrained(self.model_id)
                self.model = AutoModel.from_pretrained(self.model_id)
            
            # Move model to device if not CPU
            if device != "cpu":
                self.model = self.model.to(device)
            
            # Prepare input based on modality
            if 'text' == 'text':
                inputs = self.tokenizer("Test input for bert", return_tensors="pt")
            elif 'text' == 'audio':
                import numpy as np
                sample_rate = 16000
                dummy_audio = np.random.random(sample_rate)
                inputs = self.processor(dummy_audio, sampling_rate=sample_rate, return_tensors="pt")
            elif 'text' == 'vision':
                import numpy as np
                from PIL import Image
                dummy_image = Image.new('RGB', (224, 224), color='white')
                inputs = self.processor(images=dummy_image, return_tensors="pt")
            elif 'text' == 'multimodal' or 'text' == 'video':
                import numpy as np
                from PIL import Image
                dummy_image = Image.new('RGB', (224, 224), color='white')
                inputs = self.processor(images=dummy_image, text="Test input", return_tensors="pt")
            else:
                inputs = self.tokenizer("Test input for bert", return_tensors="pt")
            
            # Move inputs to device if not CPU
            if device != "cpu":
                inputs = {k: v.to(device) for k, v in inputs.items()}
            
            # Run inference
            with torch.no_grad():
                outputs = self.model(**inputs)
            
            # Verify outputs based on model type
            self.assertIsNotNone(outputs)
            # Different models return different output structures
            if 'text' == 'text':
                if hasattr(outputs, 'last_hidden_state'):
                    self.assertIsNotNone(outputs.last_hidden_state)
                else:
                    # Some models might have alternative output structures
                    self.assertTrue(any(key in outputs for key in ['last_hidden_state', 'hidden_states', 'logits']))
            elif 'text' in ['audio', 'vision', 'video']:
                if hasattr(outputs, 'logits'):
                    self.assertIsNotNone(outputs.logits)
                else:
                    # Some models might have alternative output structures
                    self.assertTrue(any(key in outputs for key in ['logits', 'embedding', 'last_hidden_state']))
            elif 'text' == 'multimodal':
                # CLIP, LLAVA, etc. might have different output structures
                self.assertTrue(any(hasattr(outputs, attr) for attr in ['text_embeds', 'image_embeds', 'last_hidden_state', 'logits']))
            
            # Log success
            logger.info(f"Successfully tested {self.model_id} on mps")

        except Exception as e:
            logger.error(f"Error testing {self.model_id} on mps: {str(e)}")
            raise

    def test_openvino(self):
        """Test bert with openvino."""
        # Skip if hardware not available
        if not HAS_OPENVINO: self.skipTest('OPENVINO not available')
        
        # Set up device
        device = "cpu"
        # Initialize OpenVINO if available
        if HAS_OPENVINO:
            try:
                import openvino as ov
                self.ov_core = ov.Core()
                self.openvino_label = "openvino"
            except Exception as e:
                logger.warning(f"Error initializing OpenVINO: {{e}}")
        
        try:
            # Initialize tokenizer and model based on modality
            if 'text' == 'audio':
                from transformers import AutoFeatureExtractor, AutoModelForAudioClassification
                self.processor = AutoFeatureExtractor.from_pretrained(self.model_id)
                self.model = AutoModelForAudioClassification.from_pretrained(self.model_id)
            elif 'text' == 'vision':
                from transformers import AutoImageProcessor, AutoModelForImageClassification
                self.processor = AutoImageProcessor.from_pretrained(self.model_id)
                self.model = AutoModelForImageClassification.from_pretrained(self.model_id)
            elif 'text' == 'multimodal':
                from transformers import AutoProcessor, AutoModel
                self.processor = AutoProcessor.from_pretrained(self.model_id)
                self.model = AutoModel.from_pretrained(self.model_id)
            elif 'text' == 'video':
                from transformers import AutoProcessor, AutoModelForVideoClassification
                self.processor = AutoProcessor.from_pretrained(self.model_id)
                self.model = AutoModelForVideoClassification.from_pretrained(self.model_id)
            else:
                # Default to text models
                self.tokenizer = AutoTokenizer.from_pretrained(self.model_id)
                self.model = AutoModel.from_pretrained(self.model_id)
            
            # Move model to device if not CPU
            if device != "cpu":
                self.model = self.model.to(device)
            
            # Prepare input based on modality
            if 'text' == 'text':
                inputs = self.tokenizer("Test input for bert", return_tensors="pt")
            elif 'text' == 'audio':
                import numpy as np
                sample_rate = 16000
                dummy_audio = np.random.random(sample_rate)
                inputs = self.processor(dummy_audio, sampling_rate=sample_rate, return_tensors="pt")
            elif 'text' == 'vision':
                import numpy as np
                from PIL import Image
                dummy_image = Image.new('RGB', (224, 224), color='white')
                inputs = self.processor(images=dummy_image, return_tensors="pt")
            elif 'text' == 'multimodal' or 'text' == 'video':
                import numpy as np
                from PIL import Image
                dummy_image = Image.new('RGB', (224, 224), color='white')
                inputs = self.processor(images=dummy_image, text="Test input", return_tensors="pt")
            else:
                inputs = self.tokenizer("Test input for bert", return_tensors="pt")
            
            # Move inputs to device if not CPU
            if device != "cpu":
                inputs = {k: v.to(device) for k, v in inputs.items()}
            
            # Run inference
            with torch.no_grad():
                outputs = self.model(**inputs)
            
            # Verify outputs based on model type
            self.assertIsNotNone(outputs)
            # Different models return different output structures
            if 'text' == 'text':
                if hasattr(outputs, 'last_hidden_state'):
                    self.assertIsNotNone(outputs.last_hidden_state)
                else:
                    # Some models might have alternative output structures
                    self.assertTrue(any(key in outputs for key in ['last_hidden_state', 'hidden_states', 'logits']))
            elif 'text' in ['audio', 'vision', 'video']:
                if hasattr(outputs, 'logits'):
                    self.assertIsNotNone(outputs.logits)
                else:
                    # Some models might have alternative output structures
                    self.assertTrue(any(key in outputs for key in ['logits', 'embedding', 'last_hidden_state']))
            elif 'text' == 'multimodal':
                # CLIP, LLAVA, etc. might have different output structures
                self.assertTrue(any(hasattr(outputs, attr) for attr in ['text_embeds', 'image_embeds', 'last_hidden_state', 'logits']))
            
            # Log success
            logger.info(f"Successfully tested {self.model_id} on openvino")

        except Exception as e:
            logger.error(f"Error testing {self.model_id} on openvino: {str(e)}")
            raise

    def test_qualcomm(self):
        """Test bert with qualcomm."""
        # Skip if hardware not available
        if not HAS_QUALCOMM: self.skipTest('QUALCOMM not available')
        
        # Set up device
        device = "cpu"

        
        try:
            # Initialize tokenizer and model based on modality
            if 'text' == 'audio':
                from transformers import AutoFeatureExtractor, AutoModelForAudioClassification
                self.processor = AutoFeatureExtractor.from_pretrained(self.model_id)
                self.model = AutoModelForAudioClassification.from_pretrained(self.model_id)
            elif 'text' == 'vision':
                from transformers import AutoImageProcessor, AutoModelForImageClassification
                self.processor = AutoImageProcessor.from_pretrained(self.model_id)
                self.model = AutoModelForImageClassification.from_pretrained(self.model_id)
            elif 'text' == 'multimodal':
                from transformers import AutoProcessor, AutoModel
                self.processor = AutoProcessor.from_pretrained(self.model_id)
                self.model = AutoModel.from_pretrained(self.model_id)
            elif 'text' == 'video':
                from transformers import AutoProcessor, AutoModelForVideoClassification
                self.processor = AutoProcessor.from_pretrained(self.model_id)
                self.model = AutoModelForVideoClassification.from_pretrained(self.model_id)
            else:
                # Default to text models
                self.tokenizer = AutoTokenizer.from_pretrained(self.model_id)
                self.model = AutoModel.from_pretrained(self.model_id)
            
            # Move model to device if not CPU
            if device != "cpu":
                self.model = self.model.to(device)
            
            # Prepare input based on modality
            if 'text' == 'text':
                inputs = self.tokenizer("Test input for bert", return_tensors="pt")
            elif 'text' == 'audio':
                import numpy as np
                sample_rate = 16000
                dummy_audio = np.random.random(sample_rate)
                inputs = self.processor(dummy_audio, sampling_rate=sample_rate, return_tensors="pt")
            elif 'text' == 'vision':
                import numpy as np
                from PIL import Image
                dummy_image = Image.new('RGB', (224, 224), color='white')
                inputs = self.processor(images=dummy_image, return_tensors="pt")
            elif 'text' == 'multimodal' or 'text' == 'video':
                import numpy as np
                from PIL import Image
                dummy_image = Image.new('RGB', (224, 224), color='white')
                inputs = self.processor(images=dummy_image, text="Test input", return_tensors="pt")
            else:
                inputs = self.tokenizer("Test input for bert", return_tensors="pt")
            
            # Move inputs to device if not CPU
            if device != "cpu":
                inputs = {k: v.to(device) for k, v in inputs.items()}
            
            # Run inference
            with torch.no_grad():
                outputs = self.model(**inputs)
            
            # Verify outputs based on model type
            self.assertIsNotNone(outputs)
            # Different models return different output structures
            if 'text' == 'text':
                if hasattr(outputs, 'last_hidden_state'):
                    self.assertIsNotNone(outputs.last_hidden_state)
                else:
                    # Some models might have alternative output structures
                    self.assertTrue(any(key in outputs for key in ['last_hidden_state', 'hidden_states', 'logits']))
            elif 'text' in ['audio', 'vision', 'video']:
                if hasattr(outputs, 'logits'):
                    self.assertIsNotNone(outputs.logits)
                else:
                    # Some models might have alternative output structures
                    self.assertTrue(any(key in outputs for key in ['logits', 'embedding', 'last_hidden_state']))
            elif 'text' == 'multimodal':
                # CLIP, LLAVA, etc. might have different output structures
                self.assertTrue(any(hasattr(outputs, attr) for attr in ['text_embeds', 'image_embeds', 'last_hidden_state', 'logits']))
            
            # Log success
            logger.info(f"Successfully tested {self.model_id} on qualcomm")

        except Exception as e:
            logger.error(f"Error testing {self.model_id} on qualcomm: {str(e)}")
            raise

    def test_webnn(self):
        """Test bert with webnn."""
        # Skip if hardware not available
        if not HAS_WEBNN: self.skipTest('WEBNN not available')
        
        # Set up device
        device = "cpu"

        
        try:
            # Initialize tokenizer and model based on modality
            if 'text' == 'audio':
                from transformers import AutoFeatureExtractor, AutoModelForAudioClassification
                self.processor = AutoFeatureExtractor.from_pretrained(self.model_id)
                self.model = AutoModelForAudioClassification.from_pretrained(self.model_id)
            elif 'text' == 'vision':
                from transformers import AutoImageProcessor, AutoModelForImageClassification
                self.processor = AutoImageProcessor.from_pretrained(self.model_id)
                self.model = AutoModelForImageClassification.from_pretrained(self.model_id)
            elif 'text' == 'multimodal':
                from transformers import AutoProcessor, AutoModel
                self.processor = AutoProcessor.from_pretrained(self.model_id)
                self.model = AutoModel.from_pretrained(self.model_id)
            elif 'text' == 'video':
                from transformers import AutoProcessor, AutoModelForVideoClassification
                self.processor = AutoProcessor.from_pretrained(self.model_id)
                self.model = AutoModelForVideoClassification.from_pretrained(self.model_id)
            else:
                # Default to text models
                self.tokenizer = AutoTokenizer.from_pretrained(self.model_id)
                self.model = AutoModel.from_pretrained(self.model_id)
            
            # Move model to device if not CPU
            if device != "cpu":
                self.model = self.model.to(device)
            
            # Prepare input based on modality
            if 'text' == 'text':
                inputs = self.tokenizer("Test input for bert", return_tensors="pt")
            elif 'text' == 'audio':
                import numpy as np
                sample_rate = 16000
                dummy_audio = np.random.random(sample_rate)
                inputs = self.processor(dummy_audio, sampling_rate=sample_rate, return_tensors="pt")
            elif 'text' == 'vision':
                import numpy as np
                from PIL import Image
                dummy_image = Image.new('RGB', (224, 224), color='white')
                inputs = self.processor(images=dummy_image, return_tensors="pt")
            elif 'text' == 'multimodal' or 'text' == 'video':
                import numpy as np
                from PIL import Image
                dummy_image = Image.new('RGB', (224, 224), color='white')
                inputs = self.processor(images=dummy_image, text="Test input", return_tensors="pt")
            else:
                inputs = self.tokenizer("Test input for bert", return_tensors="pt")
            
            # Move inputs to device if not CPU
            if device != "cpu":
                inputs = {k: v.to(device) for k, v in inputs.items()}
            
            # Run inference
            with torch.no_grad():
                outputs = self.model(**inputs)
            
            # Verify outputs based on model type
            self.assertIsNotNone(outputs)
            # Different models return different output structures
            if 'text' == 'text':
                if hasattr(outputs, 'last_hidden_state'):
                    self.assertIsNotNone(outputs.last_hidden_state)
                else:
                    # Some models might have alternative output structures
                    self.assertTrue(any(key in outputs for key in ['last_hidden_state', 'hidden_states', 'logits']))
            elif 'text' in ['audio', 'vision', 'video']:
                if hasattr(outputs, 'logits'):
                    self.assertIsNotNone(outputs.logits)
                else:
                    # Some models might have alternative output structures
                    self.assertTrue(any(key in outputs for key in ['logits', 'embedding', 'last_hidden_state']))
            elif 'text' == 'multimodal':
                # CLIP, LLAVA, etc. might have different output structures
                self.assertTrue(any(hasattr(outputs, attr) for attr in ['text_embeds', 'image_embeds', 'last_hidden_state', 'logits']))
            
            # Log success
            logger.info(f"Successfully tested {self.model_id} on webnn")

        except Exception as e:
            logger.error(f"Error testing {self.model_id} on webnn: {str(e)}")
            raise

    def test_webgpu(self):
        """Test bert with webgpu."""
        # Skip if hardware not available
        if not HAS_WEBGPU: self.skipTest('WEBGPU not available')
        
        # Set up device
        device = "cpu"

        
        try:
            # Initialize tokenizer and model based on modality
            if 'text' == 'audio':
                from transformers import AutoFeatureExtractor, AutoModelForAudioClassification
                self.processor = AutoFeatureExtractor.from_pretrained(self.model_id)
                self.model = AutoModelForAudioClassification.from_pretrained(self.model_id)
            elif 'text' == 'vision':
                from transformers import AutoImageProcessor, AutoModelForImageClassification
                self.processor = AutoImageProcessor.from_pretrained(self.model_id)
                self.model = AutoModelForImageClassification.from_pretrained(self.model_id)
            elif 'text' == 'multimodal':
                from transformers import AutoProcessor, AutoModel
                self.processor = AutoProcessor.from_pretrained(self.model_id)
                self.model = AutoModel.from_pretrained(self.model_id)
            elif 'text' == 'video':
                from transformers import AutoProcessor, AutoModelForVideoClassification
                self.processor = AutoProcessor.from_pretrained(self.model_id)
                self.model = AutoModelForVideoClassification.from_pretrained(self.model_id)
            else:
                # Default to text models
                self.tokenizer = AutoTokenizer.from_pretrained(self.model_id)
                self.model = AutoModel.from_pretrained(self.model_id)
            
            # Move model to device if not CPU
            if device != "cpu":
                self.model = self.model.to(device)
            
            # Prepare input based on modality
            if 'text' == 'text':
                inputs = self.tokenizer("Test input for bert", return_tensors="pt")
            elif 'text' == 'audio':
                import numpy as np
                sample_rate = 16000
                dummy_audio = np.random.random(sample_rate)
                inputs = self.processor(dummy_audio, sampling_rate=sample_rate, return_tensors="pt")
            elif 'text' == 'vision':
                import numpy as np
                from PIL import Image
                dummy_image = Image.new('RGB', (224, 224), color='white')
                inputs = self.processor(images=dummy_image, return_tensors="pt")
            elif 'text' == 'multimodal' or 'text' == 'video':
                import numpy as np
                from PIL import Image
                dummy_image = Image.new('RGB', (224, 224), color='white')
                inputs = self.processor(images=dummy_image, text="Test input", return_tensors="pt")
            else:
                inputs = self.tokenizer("Test input for bert", return_tensors="pt")
            
            # Move inputs to device if not CPU
            if device != "cpu":
                inputs = {k: v.to(device) for k, v in inputs.items()}
            
            # Run inference
            with torch.no_grad():
                outputs = self.model(**inputs)
            
            # Verify outputs based on model type
            self.assertIsNotNone(outputs)
            # Different models return different output structures
            if 'text' == 'text':
                if hasattr(outputs, 'last_hidden_state'):
                    self.assertIsNotNone(outputs.last_hidden_state)
                else:
                    # Some models might have alternative output structures
                    self.assertTrue(any(key in outputs for key in ['last_hidden_state', 'hidden_states', 'logits']))
            elif 'text' in ['audio', 'vision', 'video']:
                if hasattr(outputs, 'logits'):
                    self.assertIsNotNone(outputs.logits)
                else:
                    # Some models might have alternative output structures
                    self.assertTrue(any(key in outputs for key in ['logits', 'embedding', 'last_hidden_state']))
            elif 'text' == 'multimodal':
                # CLIP, LLAVA, etc. might have different output structures
                self.assertTrue(any(hasattr(outputs, attr) for attr in ['text_embeds', 'image_embeds', 'last_hidden_state', 'logits']))
            
            # Log success
            logger.info(f"Successfully tested {self.model_id} on webgpu")

        except Exception as e:
            logger.error(f"Error testing {self.model_id} on webgpu: {str(e)}")
            raise

if __name__ == "__main__":
    unittest.main()
