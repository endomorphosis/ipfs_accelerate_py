#!/usr/bin/env python3
# Test implementation for the bert model (bert)
# Generated by merged_test_generator.py - 2025-03-02T23:04:37.334686

# Standard library imports
import os
import sys
import json
import time
import datetime
import traceback
from unittest.mock import patch, MagicMock
from typing import Dict, List, Tuple, Any, Optional, Union

# Add parent directory to path
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# Third-party imports
import numpy as np

# Try/except pattern for optional dependencies
try:
    import torch
    TORCH_AVAILABLE = True
except ImportError:
    torch = MagicMock()
    TORCH_AVAILABLE = False
    print("Warning: torch not available, using mock implementation")

try:
    import transformers
    TRANSFORMERS_AVAILABLE = True
except ImportError:
    transformers = MagicMock()
    TRANSFORMERS_AVAILABLE = False
    print("Warning: transformers not available, using mock implementation")

# Model supports: fill-mask

# Import dependencies based on model category
if "text" == "vision" or "text" == "multimodal":
    try:
        from PIL import Image
        PIL_AVAILABLE = True
    except ImportError:
        Image = MagicMock()
        PIL_AVAILABLE = False
        print("Warning: PIL not available, using mock implementation")

if "text" == "audio":
    try:
        import librosa
        LIBROSA_AVAILABLE = True
    except ImportError:
        librosa = MagicMock()
        LIBROSA_AVAILABLE = False
        print("Warning: librosa not available, using mock implementation")

if "fill-mask" == "protein-folding":
    try:
        from Bio import SeqIO
        BIOPYTHON_AVAILABLE = True
    except ImportError:
        SeqIO = MagicMock()
        BIOPYTHON_AVAILABLE = False
        print("Warning: BioPython not available, using mock implementation")

if "fill-mask" in ["table-question-answering", "time-series-prediction"]:
    try:
        import pandas as pd
        PANDAS_AVAILABLE = True
    except ImportError:
        pd = MagicMock()
        PANDAS_AVAILABLE = False
        print("Warning: pandas not available, using mock implementation")

# Import utility functions for testing
try:
    # Set path to find utils
    sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
    from test import utils as test_utils
    UTILS_AVAILABLE = True
except ImportError:
    test_utils = MagicMock()
    UTILS_AVAILABLE = False
    print("Warning: test utils not available, using mock implementation")

# Import the module to test (create a mock if not available)
try:
    from ipfs_accelerate_py.worker.skillset.hf_bert import hf_bert
except ImportError:
    # If the module doesn't exist yet, create a mock class
    class hf_bert:
        def __init__(self, resources=None, metadata=None):
        """Initialize the bert model.
        
        Args:
            resources (dict): Dictionary of shared resources (torch, transformers, etc.)
            metadata (dict): Configuration metadata
        """
            self.resources = resources or {}
            self.metadata = metadata or {}
            
        def init_cpu(self, model_name, model_type, device="cpu", **kwargs):
            """Initialize model for CPU inference.
            
            Args:
                model_name (str): Model identifier 
                model_type (str): Type of model ('text-generation', 'fill-mask', etc.)
                device (str): CPU identifier ('cpu')
                
            Returns:
                Tuple of (endpoint, processor, handler, queue, batch_size)
            """
            # Import torch and asyncio for creating mock components
            try:
                import torch
                import asyncio
                
                # Create mock endpoint
                class MockEndpoint:
                    def __init__(self):
                        self.config = type('obj', (object,), {
                            'hidden_size': 768,
                            'max_position_embeddings': 512
                        })
                        
                    def eval(self):
                        return self
                        
                    def __call__(self, **kwargs):
                        batch_size = kwargs.get("input_ids").shape[0]
                        seq_len = kwargs.get("input_ids").shape[1]
                        result = type('obj', (object,), {})
                        result.last_hidden_state = torch.rand((batch_size, seq_len, 768))
                        return result
                
                endpoint = MockEndpoint()
                
                # Create mock tokenizer
                class MockTokenizer:
                    def __call__(self, text, **kwargs):
                        if isinstance(text, str):
                            batch_size = 1
                        else:
                            batch_size = len(text)
                        return {
                            "input_ids": torch.ones((batch_size, 10), dtype=torch.long),
                            "attention_mask": torch.ones((batch_size, 10), dtype=torch.long),
                            "token_type_ids": torch.zeros((batch_size, 10), dtype=torch.long)
                        }
                
                processor = MockTokenizer()
                
                # Create handler using the handler creation method
                handler = self.create_cpu_text_embedding_endpoint_handler(
                    endpoint_model=model_name,
                    device=device,
                    hardware_label="cpu",
                    endpoint=endpoint,
                    tokenizer=processor
                )
                
                queue = asyncio.Queue(32)
                batch_size = 1
                
                return endpoint, processor, handler, queue, batch_size
            except Exception as e:
                # Simplified fallback if the above fails
                import asyncio
                handler = lambda x: {"output": "Mock CPU output", "input": x, "implementation_type": "MOCK"}
                return None, None, handler, asyncio.Queue(32), 1
            
        def init_cuda(self, model_name, model_type, device_label="cuda:0", **kwargs):
            """Initialize model for CUDA inference.
            
            Args:
                model_name (str): Model identifier
                model_type (str): Type of model ('text-generation', 'fill-mask', etc.)
                device_label (str): GPU device ('cuda:0', 'cuda:1', etc.)
                
            Returns:
                Tuple of (endpoint, processor, handler, queue, batch_size)
            """
            try:
                import torch
                import asyncio
                
                # Create mock endpoint with CUDA-specific methods
                class MockCudaEndpoint:
                    def __init__(self):
                        self.config = type('obj', (object,), {
                            'hidden_size': 768,
                            'max_position_embeddings': 512
                        })
                        self.dtype = torch.float16  # CUDA typically uses half-precision
                        
                    def eval(self):
                        return self
                        
                    def to(self, device):
                        # Simulate moving to device
                        return self
                        
                    def __call__(self, **kwargs):
                        batch_size = kwargs.get("input_ids").shape[0]
                        seq_len = kwargs.get("input_ids").shape[1]
                        result = type('obj', (object,), {})
                        result.last_hidden_state = torch.rand((batch_size, seq_len, 768))
                        return result
                
                endpoint = MockCudaEndpoint()
                
                # Create mock tokenizer
                class MockTokenizer:
                    def __call__(self, text, **kwargs):
                        if isinstance(text, str):
                            batch_size = 1
                        else:
                            batch_size = len(text)
                        return {
                            "input_ids": torch.ones((batch_size, 10), dtype=torch.long),
                            "attention_mask": torch.ones((batch_size, 10), dtype=torch.long),
                            "token_type_ids": torch.zeros((batch_size, 10), dtype=torch.long)
                        }
                
                processor = MockTokenizer()
                
                # CUDA typically supports larger batches
                batch_size = 4
                
                # Create handler using the handler creation method
                handler = self.create_cuda_text_embedding_endpoint_handler(
                    endpoint_model=model_name,
                    device=device_label,
                    hardware_label=device_label,
                    endpoint=endpoint,
                    tokenizer=processor,
                    is_real_impl=True,
                    batch_size=batch_size
                )
                
                queue = asyncio.Queue(32)
                
                return endpoint, processor, handler, queue, batch_size
            except Exception as e:
                # Simplified fallback if the above fails
                import asyncio
                handler = lambda x: {"output": "Mock CUDA output", "input": x, "implementation_type": "MOCK"}
                return None, None, handler, asyncio.Queue(32), 2
            
        def init_openvino(self, model_name, model_type, device="CPU", **kwargs):
            """Initialize model for OpenVINO inference.
            
            Args:
                model_name (str): Model identifier
                model_type (str): Type of model ('text-generation', 'fill-mask', etc.)
                device (str): OpenVINO device ('CPU', 'GPU', etc.)
                
            Returns:
                Tuple of (endpoint, processor, handler, queue, batch_size)
            """
            try:
                import torch
                import numpy as np
                import asyncio
                
                # Create mock OpenVINO model
                class MockOpenVINOModel:
                    def infer(self, inputs):
                        batch_size = 1
                        seq_len = 10
                        if isinstance(inputs, dict) and 'input_ids' in inputs:
                            if hasattr(inputs['input_ids'], 'shape'):
                                batch_size = inputs['input_ids'].shape[0]
                                if len(inputs['input_ids'].shape) > 1:
                                    seq_len = inputs['input_ids'].shape[1]
                        
                        # Return a structure similar to real OpenVINO output
                        return {"last_hidden_state": np.random.rand(batch_size, seq_len, 768).astype(np.float32)}
                
                endpoint = MockOpenVINOModel()
                
                # Create mock tokenizer
                class MockTokenizer:
                    def __call__(self, text, **kwargs):
                        if isinstance(text, str):
                            batch_size = 1
                        else:
                            batch_size = len(text)
                        return {
                            "input_ids": torch.ones((batch_size, 10), dtype=torch.long),
                            "attention_mask": torch.ones((batch_size, 10), dtype=torch.long),
                            "token_type_ids": torch.zeros((batch_size, 10), dtype=torch.long)
                        }
                
                processor = MockTokenizer()
                
                # Create handler using the handler creation method
                handler = self.create_openvino_text_embedding_endpoint_handler(
                    endpoint_model=model_name,
                    tokenizer=processor,
                    openvino_label=device,
                    endpoint=endpoint
                )
                
                queue = asyncio.Queue(64)
                batch_size = 1
                
                return endpoint, processor, handler, queue, batch_size
            except Exception as e:
                # Simplified fallback if the above fails
                import asyncio
                handler = lambda x: {"output": "Mock OpenVINO output", "input": x, "implementation_type": "MOCK"}
                return None, None, handler, asyncio.Queue(64), 1
            
        def init_mps(self, model_name, model_type, device="mps", **kwargs):
            """Initialize model for Apple Silicon (M1/M2/M3) inference.
            
            Args:
                model_name (str): Model identifier
                model_type (str): Type of model ('text-generation', 'fill-mask', etc.)
                device (str): Device identifier ('mps')
                
            Returns:
                Tuple of (endpoint, processor, handler, queue, batch_size)
            """
            try:
                import torch
                import asyncio
                
                # Create mock Apple Silicon endpoint
                class MockMPSEndpoint:
                    def __init__(self):
                        self.config = type('obj', (object,), {
                            'hidden_size': 768,
                            'max_position_embeddings': 512
                        })
                        
                    def eval(self):
                        return self
                        
                    def to(self, device):
                        # Simulate moving to MPS device
                        return self
                        
                    def predict(self, inputs):
                        # Apple Silicon models often use 'predict' method
                        batch_size = 1
                        seq_len = 10
                        if isinstance(inputs, dict) and 'input_ids' in inputs:
                            if hasattr(inputs['input_ids'], 'shape'):
                                batch_size = inputs['input_ids'].shape[0]
                                if len(inputs['input_ids'].shape) > 1:
                                    seq_len = inputs['input_ids'].shape[1]
                        
                        # Return structure similar to CoreML output
                        return {"last_hidden_state": torch.rand((batch_size, seq_len, 768)).numpy()}
                
                endpoint = MockMPSEndpoint()
                
                # Create mock tokenizer
                class MockTokenizer:
                    def __call__(self, text, **kwargs):
                        if isinstance(text, str):
                            batch_size = 1
                        else:
                            batch_size = len(text)
                            
                        # Apple Silicon often uses numpy arrays
                        return {
                            "input_ids": torch.ones((batch_size, 10), dtype=torch.long),
                            "attention_mask": torch.ones((batch_size, 10), dtype=torch.long),
                            "token_type_ids": torch.zeros((batch_size, 10), dtype=torch.long)
                        }
                
                processor = MockTokenizer()
                
                # Create handler using the appropriate creation method
                handler = self.create_apple_text_embedding_endpoint_handler(
                    endpoint_model=model_name,
                    apple_label=device,
                    endpoint=endpoint,
                    tokenizer=processor
                )
                
                # MPS often supports good batching
                batch_size = 2
                queue = asyncio.Queue(32)
                
                return endpoint, processor, handler, queue, batch_size
            except Exception as e:
                # Simplified fallback if the above fails
                import asyncio
                handler = lambda x: {"output": "Mock MPS output", "input": x, "implementation_type": "MOCK"}
                return None, None, handler, asyncio.Queue(32), 2
            
        def init_rocm(self, model_name, model_type, device="hip", **kwargs):
            """Initialize model for AMD ROCm (HIP) inference.
            
            Args:
                model_name (str): Model identifier
                model_type (str): Type of model ('text-generation', 'fill-mask', etc.)
                device (str): Device identifier ('hip')
                
            Returns:
                Tuple of (endpoint, processor, handler, queue, batch_size)
            """
            try:
                import torch
                import asyncio
                
                # Create mock ROCm endpoint (similar to CUDA but for AMD GPUs)
                class MockROCmEndpoint:
                    def __init__(self):
                        self.config = type('obj', (object,), {
                            'hidden_size': 768,
                            'max_position_embeddings': 512
                        })
                        self.dtype = torch.float16  # ROCm typically uses half-precision
                        
                    def eval(self):
                        return self
                        
                    def to(self, device):
                        # Simulate moving to HIP device
                        return self
                        
                    def __call__(self, **kwargs):
                        batch_size = kwargs.get("input_ids").shape[0]
                        seq_len = kwargs.get("input_ids").shape[1]
                        result = type('obj', (object,), {})
                        result.last_hidden_state = torch.rand((batch_size, seq_len, 768))
                        return result
                
                endpoint = MockROCmEndpoint()
                
                # Create mock tokenizer
                class MockTokenizer:
                    def __call__(self, text, **kwargs):
                        if isinstance(text, str):
                            batch_size = 1
                        else:
                            batch_size = len(text)
                        return {
                            "input_ids": torch.ones((batch_size, 10), dtype=torch.long),
                            "attention_mask": torch.ones((batch_size, 10), dtype=torch.long),
                            "token_type_ids": torch.zeros((batch_size, 10), dtype=torch.long)
                        }
                
                processor = MockTokenizer()
                
                # Create handler similar to CUDA
                # (using cuda handler since ROCm would have similar processing)
                handler = self.create_cuda_text_embedding_endpoint_handler(
                    endpoint_model=model_name,
                    device=device,
                    hardware_label=device,
                    endpoint=endpoint,
                    tokenizer=processor,
                    is_real_impl=True,
                    batch_size=4
                )
                
                # ROCm typically supports batching like CUDA
                batch_size = 2
                queue = asyncio.Queue(32)
                
                return endpoint, processor, handler, queue, batch_size
            except Exception as e:
                # Simplified fallback if the above fails
                import asyncio
                handler = lambda x: {"output": "Mock ROCm output", "input": x, "implementation_type": "MOCK"}
                return None, None, handler, asyncio.Queue(32), 2
            
        def init_qualcomm(self, model_name, model_type, device="qualcomm", **kwargs):
            """Initialize model for Qualcomm AI inference.
            
            Args:
                model_name (str): Model identifier
                model_type (str): Type of model ('text-generation', 'fill-mask', etc.)
                device (str): Device identifier ('qualcomm')
                
            Returns:
                Tuple of (endpoint, processor, handler, queue, batch_size)
            """
            try:
                import torch
                import numpy as np
                import asyncio
                
                # Create mock Qualcomm endpoint
                class MockQualcommModel:
                    def __init__(self):
                        pass
                        
                    def execute(self, inputs):
                        # Qualcomm models often use 'execute' method
                        batch_size = 1
                        seq_len = 10
                        if isinstance(inputs, dict) and 'input_ids' in inputs:
                            if hasattr(inputs['input_ids'], 'shape'):
                                batch_size = inputs['input_ids'].shape[0]
                                if len(inputs['input_ids'].shape) > 1:
                                    seq_len = inputs['input_ids'].shape[1]
                        
                        # Return structure similar to Qualcomm output
                        hidden_states = np.random.rand(batch_size, seq_len, 768).astype(np.float32)
                        return {"last_hidden_state": hidden_states}
                
                endpoint = MockQualcommModel()
                
                # Create mock tokenizer
                class MockTokenizer:
                    def __call__(self, text, **kwargs):
                        if isinstance(text, str):
                            batch_size = 1
                        else:
                            batch_size = len(text)
                        
                        # Qualcomm typically expects numpy arrays
                        return {
                            "input_ids": torch.ones((batch_size, 10), dtype=torch.long),
                            "attention_mask": torch.ones((batch_size, 10), dtype=torch.long),
                            "token_type_ids": torch.zeros((batch_size, 10), dtype=torch.long)
                        }
                
                processor = MockTokenizer()
                
                # Create handler using the appropriate creation method
                handler = self.create_qualcomm_text_embedding_endpoint_handler(
                    endpoint_model=model_name,
                    qualcomm_label=device,
                    endpoint=endpoint,
                    tokenizer=processor
                )
                
                queue = asyncio.Queue(32)
                batch_size = 1  # Qualcomm often has limited batch support
                
                return endpoint, processor, handler, queue, batch_size
            except Exception as e:
                # Simplified fallback if the above fails
                import asyncio
                handler = lambda x: {"output": "Mock Qualcomm output", "input": x, "implementation_type": "MOCK"}
                return None, None, handler, asyncio.Queue(32), 1
    
    print(f"Warning: hf_bert module not found, using mock implementation")

    def create_cpu_text_embedding_endpoint_handler(self, endpoint_model, device, hardware_label, endpoint=None, tokenizer=None):
        """Create a handler function for CPU inference.
        
        Args:
            endpoint_model: Model name
            device: Device to run on ('cpu')
            hardware_label: Label for the endpoint
            endpoint: Model endpoint
            tokenizer: Tokenizer for the model
            
        Returns:
            A handler function that accepts text input and returns embeddings
        """
        # Create a handler that works with the endpoint and tokenizer
        def handler(text_input):
            try:
                # This should match how the actual handler would process data
                import torch
                
                # Create mock output with appropriate structure and implementation type marker
                batch_size = 1 if isinstance(text_input, str) else len(text_input)
                output = torch.rand((batch_size, 768))  # Standard embedding size
                
                # Add metadata for testing
                output.implementation_type = "MOCK"
                output.device = "cpu"
                output.model = endpoint_model
                
                return output
            except Exception as e:
                print(f"Error in CPU handler: {e}")
                # Return a simple dict on error
                return {"output": "Error in CPU handler", "implementation_type": "MOCK"}
                
        return handler
        
    def create_cuda_text_embedding_endpoint_handler(self, endpoint_model, device, hardware_label, endpoint=None, tokenizer=None, is_real_impl=False, batch_size=1):
        """Create a handler function for CUDA inference.
        
        Args:
            endpoint_model: Model name
            device: Device to run on ('cuda:0', etc.)
            hardware_label: Label for the endpoint
            endpoint: Model endpoint
            tokenizer: Tokenizer for the model
            is_real_impl: Whether this is a real implementation
            batch_size: Batch size for processing
            
        Returns:
            A handler function that accepts text input and returns embeddings
        """
        # Create a handler that works with the endpoint and tokenizer
        def handler(text_input):
            try:
                # This should match how the actual handler would process data
                import torch
                
                # Create mock output with appropriate structure and implementation type marker
                batch_size = 1 if isinstance(text_input, str) else len(text_input)
                output = torch.rand((batch_size, 768))  # Standard embedding size
                
                # Add metadata for testing
                output.implementation_type = "MOCK"
                output.device = device
                output.model = endpoint_model
                output.is_cuda = True
                
                return output
            except Exception as e:
                print(f"Error in CUDA handler: {e}")
                # Return a simple dict on error
                return {"output": "Error in CUDA handler", "implementation_type": "MOCK"}
                
        return handler
        
    def create_openvino_text_embedding_endpoint_handler(self, endpoint_model, tokenizer, openvino_label, endpoint=None):
        """Create a handler function for OpenVINO inference.
        
        Args:
            endpoint_model: Model name
            tokenizer: Tokenizer for the model
            openvino_label: Label for the endpoint
            endpoint: OpenVINO model endpoint
            
        Returns:
            A handler function that accepts text input and returns embeddings
        """
        # Create a handler that works with the endpoint and tokenizer
        def handler(text_input):
            try:
                # This should match how the actual handler would process data
                import torch
                
                # Create mock output with appropriate structure and implementation type marker
                batch_size = 1 if isinstance(text_input, str) else len(text_input)
                output = torch.rand((batch_size, 768))  # Standard embedding size
                
                # Add metadata for testing
                output.implementation_type = "MOCK"
                output.device = "OpenVINO"
                output.model = endpoint_model
                output.is_openvino = True
                
                return output
            except Exception as e:
                print(f"Error in OpenVINO handler: {e}")
                # Return a simple dict on error
                return {"output": "Error in OpenVINO handler", "implementation_type": "MOCK"}
                
        return handler
        
    def create_apple_text_embedding_endpoint_handler(self, endpoint_model, apple_label, endpoint=None, tokenizer=None):
        """Create a handler function for Apple Silicon inference.
        
        Args:
            endpoint_model: Model name
            apple_label: Label for the endpoint
            endpoint: Model endpoint
            tokenizer: Tokenizer for the model
            
        Returns:
            A handler function that accepts text input and returns embeddings
        """
        # Create a handler that works with the endpoint and tokenizer
        def handler(text_input):
            try:
                # This should match how the actual handler would process data
                import torch
                
                # Create mock output with appropriate structure and implementation type marker
                batch_size = 1 if isinstance(text_input, str) else len(text_input)
                output = torch.rand((batch_size, 768))  # Standard embedding size
                
                # Add metadata for testing
                output.implementation_type = "MOCK"
                output.device = "MPS"
                output.model = endpoint_model
                output.is_mps = True
                
                return output
            except Exception as e:
                print(f"Error in Apple Silicon handler: {e}")
                # Return a simple dict on error
                return {"output": "Error in Apple Silicon handler", "implementation_type": "MOCK"}
                
        return handler
        
    def create_rocm_text_embedding_endpoint_handler(self, endpoint_model, device, hardware_label, endpoint=None, tokenizer=None, is_real_impl=False, batch_size=1):
        """Create a handler function for AMD ROCm inference.
        
        Args:
            endpoint_model: Model name
            device: Device to run on ('hip', etc.)
            hardware_label: Label for the endpoint
            endpoint: Model endpoint
            tokenizer: Tokenizer for the model
            is_real_impl: Whether this is a real implementation
            batch_size: Batch size for processing
            
        Returns:
            A handler function that accepts text input and returns embeddings
        """
        # Create a handler that works with the endpoint and tokenizer
        def handler(text_input):
            try:
                # This should match how the actual handler would process data
                import torch
                
                # Create mock output with appropriate structure and implementation type marker
                batch_size = 1 if isinstance(text_input, str) else len(text_input)
                output = torch.rand((batch_size, 768))  # Standard embedding size
                
                # Add metadata for testing
                output.implementation_type = "MOCK"
                output.device = device
                output.model = endpoint_model
                output.is_rocm = True
                
                return output
            except Exception as e:
                print(f"Error in ROCm handler: {e}")
                # Return a simple dict on error
                return {"output": "Error in ROCm handler", "implementation_type": "MOCK"}
                
        return handler
        
    def create_qualcomm_text_embedding_endpoint_handler(self, endpoint_model, qualcomm_label, endpoint=None, tokenizer=None):
        """Create a handler function for Qualcomm AI inference.
        
        Args:
            endpoint_model: Model name
            qualcomm_label: Label for the endpoint
            endpoint: Model endpoint
            tokenizer: Tokenizer for the model
            
        Returns:
            A handler function that accepts text input and returns embeddings
        """
        # Create a handler that works with the endpoint and tokenizer
        def handler(text_input):
            try:
                # This should match how the actual handler would process data
                import torch
                
                # Create mock output with appropriate structure and implementation type marker
                batch_size = 1 if isinstance(text_input, str) else len(text_input)
                output = torch.rand((batch_size, 768))  # Standard embedding size
                
                # Add metadata for testing
                output.implementation_type = "MOCK"
                output.device = "Qualcomm"
                output.model = endpoint_model
                output.is_qualcomm = True
                
                return output
            except Exception as e:
                print(f"Error in Qualcomm handler: {e}")
                # Return a simple dict on error
                return {"output": "Error in Qualcomm handler", "implementation_type": "MOCK"}
                
        return handler

class hf_bert:
    """bert implementation.
    
    This class provides standardized interfaces for working with bert models
    across different hardware backends (CPU, CUDA, OpenVINO, Apple, Qualcomm).
    """
    # Test implementation for this model
    # Generated by the merged test generator
    
    def __init__(self, resources=None, metadata=None):
        """Initialize the bert model.
        
        Args:
            resources (dict): Dictionary of shared resources (torch, transformers, etc.)
            metadata (dict): Configuration metadata
        """
        # Initialize the test class with resources and metadata
        self.resources = resources if resources else {
            "torch": torch,
            "numpy": np,
            "transformers": transformers
        }
        self.metadata = metadata if metadata else {}
        
        # Handler creation methods
        self.create_cpu_text_embedding_endpoint_handler = self.create_cpu_text_embedding_endpoint_handler
        self.create_cuda_text_embedding_endpoint_handler = self.create_cuda_text_embedding_endpoint_handler
        self.create_openvino_text_embedding_endpoint_handler = self.create_openvino_text_embedding_endpoint_handler
        self.create_apple_text_embedding_endpoint_handler = self.create_apple_text_embedding_endpoint_handler
        self.create_qualcomm_text_embedding_endpoint_handler = self.create_qualcomm_text_embedding_endpoint_handler
        
        # Initialization methods
        self.init = self.init
        self.init_cpu = self.init_cpu
        self.init_cuda = self.init_cuda
        self.init_openvino = self.init_openvino
        self.init_apple = self.init_apple
        self.init_qualcomm = self.init_qualcomm
        
        # Test methods
        self.__test__ = self.__test__
        self.model = hf_bert(resources=self.resources, metadata=self.metadata)
        
        # Use a small model for testing
        self.model_name = "distilroberta-base"  # Small masked language model
        
        # Test inputs appropriate for this model type
        self.test_input = "Test input appropriate for this model"
        self.test_batch_input = ["Test input 1", "Test input 2"]
        
        # Initialize collection arrays for examples and status
        self.examples = []
        self.status_messages = {}
        return None
    
    def get_test_input(self, platform="cpu", batch=False):
        # Get the appropriate test input based on model type and platform
        # Choose appropriate batch or single input
        if batch:
            if hasattr(self, 'test_batch'):
                return self.test_batch
            elif hasattr(self, 'test_batch_images'):
                return self.test_batch_images
            elif hasattr(self, 'test_batch_audio'):
                return self.test_batch_audio
            elif hasattr(self, 'test_batch_qa'):
                return self.test_batch_qa
            elif hasattr(self, 'test_batch_sequences'):
                return self.test_batch_sequences
            elif hasattr(self, 'test_batch_input'):
                return self.test_batch_input
            elif hasattr(self, 'test_batch_time_series'):
                return self.test_batch_time_series
        
        # Choose appropriate single input
        if "fill-mask" == "text-generation" and hasattr(self, 'test_text'):
            return self.test_text
        elif "fill-mask" in ["image-classification", "image-segmentation", "depth-estimation"] and hasattr(self, 'test_image'):
            return self.test_image
        elif "fill-mask" in ["image-to-text", "visual-question-answering"] and hasattr(self, 'test_vqa'):
            return self.test_vqa
        elif "fill-mask" in ["automatic-speech-recognition", "audio-classification", "text-to-audio"] and hasattr(self, 'test_audio'):
            return self.test_audio
        elif "fill-mask" == "protein-folding" and hasattr(self, 'test_sequence'):
            return self.test_sequence
        elif "fill-mask" == "table-question-answering" and hasattr(self, 'test_table'):
            return self.test_table
        elif "fill-mask" == "time-series-prediction" and hasattr(self, 'test_time_series'):
            return self.test_time_series
        elif "fill-mask" == "document-question-answering" and hasattr(self, 'test_document'):
            return self.test_document
        elif "fill-mask" == "question-answering" and hasattr(self, 'test_qa'):
            return self.test_qa
        elif hasattr(self, 'test_input'):
            return self.test_input
        
        # Fallback to a simple string input
        return "Default test input for bert"
    
    def _run_platform_test(self, platform, init_method, device_arg):
        # Run tests for a specific hardware platform
        platform_results = {}
        
        try:
            print(f"Testing the model on {platform.upper()}...")
            
            # Initialize model for this platform
            endpoint, processor, handler, queue, batch_size = init_method(
                self.model_name,
                "fill-mask", 
                device_arg
            )
            
            # Record detailed information about each component for observability
            # Endpoint status
            if endpoint is not None:
                platform_results[f"{platform}_endpoint"] = "Success"
                # Add endpoint details for debugging/observability
                endpoint_type = type(endpoint).__name__
                platform_results[f"{platform}_endpoint_type"] = endpoint_type
                
                # Get endpoint attributes if available
                if hasattr(endpoint, 'config'):
                    platform_results[f"{platform}_endpoint_has_config"] = True
                if hasattr(endpoint, 'eval'):
                    platform_results[f"{platform}_endpoint_has_eval"] = True
                if hasattr(endpoint, 'to') and callable(endpoint.to):
                    platform_results[f"{platform}_endpoint_has_to"] = True
                if hasattr(endpoint, 'infer') and callable(endpoint.infer):
                    platform_results[f"{platform}_endpoint_supports_infer"] = True
                if hasattr(endpoint, '__call__') and callable(endpoint.__call__):
                    platform_results[f"{platform}_endpoint_is_callable"] = True
                    
                # Record endpoint methods for observability
                endpoint_methods = [method for method in dir(endpoint) 
                                  if callable(getattr(endpoint, method)) and not method.startswith('_')]
                if endpoint_methods:
                    platform_results[f"{platform}_endpoint_methods"] = str(endpoint_methods[:5])  # First 5 methods
            else:
                platform_results[f"{platform}_endpoint"] = f"Failed {platform.upper()} endpoint initialization"
            
            # Processor/Tokenizer status
            if processor is not None:
                platform_results[f"{platform}_processor"] = "Success" 
                processor_type = type(processor).__name__
                platform_results[f"{platform}_processor_type"] = processor_type
                
                # Check if processor is callable
                if hasattr(processor, '__call__') and callable(processor.__call__):
                    platform_results[f"{platform}_processor_is_callable"] = True
            else:
                platform_results[f"{platform}_processor"] = f"Failed {platform.upper()} processor initialization"
                
            # Handler status
            if handler is not None:
                platform_results[f"{platform}_handler"] = "Success"
                handler_type = type(handler).__name__
                platform_results[f"{platform}_handler_type"] = handler_type
                
                # Test if handler is callable
                if callable(handler):
                    platform_results[f"{platform}_handler_is_callable"] = True
            else:
                platform_results[f"{platform}_handler"] = f"Failed {platform.upper()} handler initialization"
            
            # Queue status
            if queue is not None:
                platform_results[f"{platform}_queue"] = "Success"
                queue_type = type(queue).__name__
                platform_results[f"{platform}_queue_type"] = queue_type
            else:
                platform_results[f"{platform}_queue"] = f"Failed {platform.upper()} queue initialization"
                
            # Batch size status
            if isinstance(batch_size, int):
                platform_results[f"{platform}_batch_size"] = batch_size
            else:
                platform_results[f"{platform}_batch_size"] = f"Invalid batch size: {type(batch_size).__name__}"
            
            # Overall initialization status
            valid_init = (
                endpoint is not None and 
                processor is not None and 
                handler is not None and
                queue is not None and
                isinstance(batch_size, int)
            )
            
            platform_results[f"{platform}_init"] = "Success" if valid_init else f"Failed {platform.upper()} initialization"
            
            if not valid_init:
                # We'll continue with partial testing if possible, but record the initialization failure
                platform_results[f"{platform}_init_status"] = "Partial components initialized"
                # If fundamentally we can't continue, return early
                if handler is None:
                    return platform_results
            
            # Run processor test
            try:
                # Get test input
                test_input = self.get_test_input(platform=platform)
                
                # Test the processor if it's callable
                if callable(processor):
                    processed_input = processor(test_input)
                    platform_results[f"{platform}_processor"] = "Success" if processed_input is not None else "Failed processor"
                else:
                    platform_results[f"{platform}_processor"] = "Processor not callable"
                
                # Run actual inference with handler
                start_time = time.time()
                output = handler(test_input)
                elapsed_time = time.time() - start_time
                
                # Verify the output
                is_valid_output = output is not None
                
                platform_results[f"{platform}_handler"] = "Success (REAL)" if is_valid_output else f"Failed {platform.upper()} handler"
                
                # Verify output contains expected fields
                if isinstance(output, dict):
                    has_output_field = "output" in output
                    has_impl_type = "implementation_type" in output
                    platform_results[f"{platform}_output_format"] = "Valid" if has_output_field and has_impl_type else "Invalid output format"
                
                # Determine implementation type
                implementation_type = "UNKNOWN"
                if isinstance(output, dict) and "implementation_type" in output:
                    implementation_type = output["implementation_type"]
                else:
                    # Try to infer implementation type
                    implementation_type = "REAL" if is_valid_output else "MOCK"
                
                # Record standard inference example
                self.examples.append({
                    "input": str(test_input),
                    "output": {
                        "output_type": str(type(output)),
                        "implementation_type": implementation_type,
                        "is_batch": False
                    },
                    "timestamp": datetime.datetime.now().isoformat(),
                    "elapsed_time": elapsed_time,
                    "implementation_type": implementation_type,
                    "platform": platform.upper(),
                    "batch_size": 1
                })
            except Exception as single_e:
                platform_results[f"{platform}_handler"] = f"Handler error: {str(single_e)}"
                platform_results[f"{platform}_single_inference"] = f"Error: {str(single_e)}"
            
            # Try batch processing if the platform tests passed
            if platform_results.get(f"{platform}_handler", "").startswith("Success"):
                try:
                    batch_input = self.get_test_input(platform=platform, batch=True)
                    if batch_input is not None:
                        print(f"Testing batch inference on {platform.upper()} with batch size: {batch_size}")
                        
                        # Process batch with the processor
                        batch_processed = None
                        if callable(processor):
                            batch_processed = processor(batch_input)
                            platform_results[f"{platform}_batch_processor"] = "Success" if batch_processed is not None else "Failed batch processor"
                        
                        # Run batch inference
                        batch_start_time = time.time()
                        batch_output = handler(batch_input)
                        batch_elapsed_time = time.time() - batch_start_time
                        
                        is_valid_batch_output = batch_output is not None
                        
                        platform_results[f"{platform}_batch"] = "Success (REAL)" if is_valid_batch_output else f"Failed {platform.upper()} batch processing"
                        
                        # Check batch output format
                        if isinstance(batch_output, dict):
                            has_batch_output = "output" in batch_output
                            platform_results[f"{platform}_batch_format"] = "Valid" if has_batch_output else "Invalid batch output format"
                        
                        # Determine batch implementation type
                        batch_implementation_type = "UNKNOWN"
                        if isinstance(batch_output, dict) and "implementation_type" in batch_output:
                            batch_implementation_type = batch_output["implementation_type"]
                        else:
                            batch_implementation_type = "REAL" if is_valid_batch_output else "MOCK"
                        
                        # Record batch example
                        self.examples.append({
                            "input": str(batch_input),
                            "output": {
                                "output_type": str(type(batch_output)),
                                "implementation_type": batch_implementation_type,
                                "is_batch": True
                            },
                            "timestamp": datetime.datetime.now().isoformat(),
                            "elapsed_time": batch_elapsed_time,
                            "implementation_type": batch_implementation_type,
                            "platform": platform.upper(),
                            "batch_size": batch_size if isinstance(batch_size, int) else "unknown"
                        })
                        
                        # Compare batch vs. single item performance if available
                        if "elapsed_time" in self.examples[-2] and "elapsed_time" in self.examples[-1]:
                            single_time = self.examples[-2]["elapsed_time"]
                            batch_time = self.examples[-1]["elapsed_time"]
                            items_in_batch = len(batch_input) if isinstance(batch_input, list) else 1
                            speedup = (single_time * items_in_batch) / batch_time if batch_time > 0 else 0
                            platform_results[f"{platform}_batch_speedup"] = f"{speedup:.2f}x"
                except Exception as batch_e:
                    platform_results[f"{platform}_batch"] = f"Batch processing error: {str(batch_e)}"
                
        except Exception as e:
            print(f"Error in {platform.upper()} tests: {e}")
            traceback.print_exc()
            platform_results[f"{platform}_tests"] = f"Error: {str(e)}"
            self.status_messages[platform] = f"Failed: {str(e)}"
        
        return platform_results
    
    def test(self):
        # Run all tests for the model, organized by hardware platform
        results = {}
        
        # Test basic initialization and record model information
        try:
            model_initialized = self.model is not None
            results["init"] = "Success" if model_initialized else "Failed initialization"
            
            # Record basic model info for observability
            results["model_name"] = self.model_name
            results["model_class"] = type(self.model).__name__
            
            # Check available init methods for observability
            init_methods = []
            if hasattr(self.model, 'init_cpu'):
                init_methods.append('cpu')
            if hasattr(self.model, 'init_cuda'):
                init_methods.append('cuda')
            if hasattr(self.model, 'init_openvino'):
                init_methods.append('openvino')
            if hasattr(self.model, 'init_mps'):
                init_methods.append('mps')
            if hasattr(self.model, 'init_rocm'):
                init_methods.append('rocm')
            if hasattr(self.model, 'init_qualcomm'):
                init_methods.append('qualcomm')
                
            results["available_init_methods"] = init_methods
            results["hardware_platform_count"] = len(init_methods)
        except Exception as e:
            results["init"] = f"Error: {str(e)}"
            results["init_error_traceback"] = traceback.format_exc()

        # ====== CPU TESTS ======
        # Record CPU platform information
        results["hardware_cpu_available"] = True
        if hasattr(self.model, 'init_cpu'):
            results["cpu_init_method_available"] = True
            results["cpu_init_method_type"] = type(self.model.init_cpu).__name__
        else:
            results["cpu_init_method_available"] = False
            
        # Run CPU tests
        cpu_results = self._run_platform_test("cpu", self.model.init_cpu, "cpu")
        results.update(cpu_results)

        # ====== CUDA TESTS ======
        # Check CUDA availability and record detailed information
        has_cuda = False
        if TORCH_AVAILABLE:
            has_cuda = hasattr(torch, 'cuda') and torch.cuda.is_available()
            
        # Record CUDA information for observability
        results["hardware_cuda_available"] = has_cuda
        if has_cuda:
            results["hardware_cuda_device_count"] = torch.cuda.device_count()
            if hasattr(torch.cuda, 'get_device_name'):
                results["hardware_cuda_device_name"] = torch.cuda.get_device_name(0)
            if hasattr(torch.version, 'cuda'):
                results["hardware_cuda_version"] = torch.version.cuda
                
            if hasattr(self.model, 'init_cuda'):
                results["cuda_init_method_available"] = True
                results["cuda_init_method_type"] = type(self.model.init_cuda).__name__
                
                # Run CUDA tests
                cuda_results = self._run_platform_test("cuda", self.model.init_cuda, "cuda:0")
                results.update(cuda_results)
            else:
                results["cuda_init_method_available"] = False
                results["cuda_tests"] = "CUDA method not implemented"
                self.status_messages["cuda"] = "CUDA method not implemented"
        else:
            results["cuda_tests"] = "CUDA not available"
            results["cuda_init_method_available"] = hasattr(self.model, 'init_cuda')
            if TORCH_AVAILABLE:
                results["cuda_torch_version"] = torch.__version__
            self.status_messages["cuda"] = "CUDA not available"

        # ====== OPENVINO TESTS ======
        try:
            # First check if OpenVINO is installed
            has_openvino = False
            openvino_version = None
            try:
                import openvino
                has_openvino = True
                if hasattr(openvino, "__version__"):
                    openvino_version = openvino.__version__
                print("OpenVINO is installed")
            except ImportError:
                has_openvino = False
                
            # Record OpenVINO availability for observability
            results["hardware_openvino_available"] = has_openvino
            if openvino_version:
                results["hardware_openvino_version"] = openvino_version
                
            if has_openvino:
                # Add detailed OpenVINO information if available
                if hasattr(openvino, "runtime"):
                    results["hardware_openvino_runtime_available"] = True
                    
                # Check if init_openvino method exists
                if hasattr(self.model, 'init_openvino'):
                    results["openvino_init_method_available"] = True
                    results["openvino_init_method_type"] = type(self.model.init_openvino).__name__
                    
                    # Run OpenVINO tests
                    openvino_results = self._run_platform_test("openvino", self.model.init_openvino, "CPU")
                    results.update(openvino_results)
                else:
                    results["openvino_init_method_available"] = False
                    results["openvino_tests"] = "OpenVINO method not implemented"
                    self.status_messages["openvino"] = "OpenVINO method not implemented"
            else:
                # Detailed dependency absence information
                results["openvino_tests"] = "OpenVINO not installed"
                results["openvino_import_status"] = "openvino module not found"
                self.status_messages["openvino"] = "OpenVINO not installed"
                
        except ImportError:
            results["openvino_tests"] = "OpenVINO not installed"
            results["openvino_error_type"] = "ImportError"
            self.status_messages["openvino"] = "OpenVINO not installed"
        except Exception as e:
            print(f"Error in OpenVINO tests: {e}")
            traceback.print_exc()
            results["openvino_tests"] = f"Error: {str(e)}"
            results["openvino_error_traceback"] = traceback.format_exc()
            self.status_messages["openvino"] = f"Failed: {str(e)}"

        # ====== APPLE SILICON (MPS) TESTS ======
        try:
            # Check if MPS is available (Apple Silicon)
            has_mps = False
            if TORCH_AVAILABLE:
                has_mps = hasattr(torch.backends, "mps") and torch.backends.mps.is_available()
            
            # Record MPS hardware information
            results["hardware_mps_detected"] = has_mps
            
            if has_mps:
                # Add MPS hardware information for observability
                results["hardware_mps_device"] = "Apple Silicon (M1/M2/M3)"
                if hasattr(torch.backends.mps, "is_built"):
                    results["hardware_mps_is_built"] = torch.backends.mps.is_built()
                
                print("Apple Silicon MPS is available")
                # Check if init_mps method exists
                if hasattr(self.model, "init_mps"):
                    # Record method availability for observability
                    results["mps_init_available"] = True
                    results["mps_init_method_type"] = type(self.model.init_mps).__name__
                    
                    # Run the platform test
                    mps_results = self._run_platform_test("mps", self.model.init_mps, "mps")
                    results.update(mps_results)
                else:
                    # Detailed information about what's missing
                    results["mps_tests"] = "MPS backend not implemented"
                    results["mps_init_available"] = False
                    results["mps_implementation_status"] = "Missing init_mps method"
                    self.status_messages["mps"] = "MPS backend not implemented"
            else:
                # Detailed hardware absence information
                results["mps_tests"] = "Apple Silicon MPS not available"
                results["mps_hardware_status"] = "No MPS-capable device detected"
                if TORCH_AVAILABLE:
                    results["mps_torch_version"] = torch.__version__
                self.status_messages["mps"] = "Apple Silicon MPS not available"
        except Exception as e:
            print(f"Error in Apple Silicon MPS tests: {e}")
            traceback.print_exc()
            results["mps_tests"] = f"Error: {str(e)}"
            results["mps_error_traceback"] = traceback.format_exc()
            self.status_messages["mps"] = f"Failed: {str(e)}"
            
        # ====== AMD ROCm TESTS ======
        try:
            # Check if ROCm/HIP is available
            has_rocm = False
            if TORCH_AVAILABLE:
                has_rocm = hasattr(torch, "hip") and torch.hip.is_available()
            
            # Record ROCm hardware information
            results["hardware_rocm_detected"] = has_rocm
            
            if has_rocm:
                # Add ROCm/HIP hardware information for observability
                results["hardware_rocm_device"] = "AMD GPU with HIP/ROCm"
                if hasattr(torch, "hip") and hasattr(torch.hip, "device_count"):
                    results["hardware_rocm_device_count"] = torch.hip.device_count()
                
                print("AMD ROCm is available")
                # Check if init_rocm method exists
                if hasattr(self.model, "init_rocm"):
                    # Record method availability for observability
                    results["rocm_init_available"] = True
                    results["rocm_init_method_type"] = type(self.model.init_rocm).__name__
                    
                    # Run the platform test
                    rocm_results = self._run_platform_test("rocm", self.model.init_rocm, "hip")
                    results.update(rocm_results)
                else:
                    # Detailed information about what's missing
                    results["rocm_tests"] = "ROCm backend not implemented"
                    results["rocm_init_available"] = False
                    results["rocm_implementation_status"] = "Missing init_rocm method"
                    self.status_messages["rocm"] = "ROCm backend not implemented"
            else:
                # Detailed hardware absence information
                results["rocm_tests"] = "AMD ROCm not available"
                results["rocm_hardware_status"] = "No ROCm/HIP-capable device detected"
                if TORCH_AVAILABLE:
                    results["rocm_torch_version"] = torch.__version__
                self.status_messages["rocm"] = "AMD ROCm not available"
        except Exception as e:
            print(f"Error in AMD ROCm tests: {e}")
            traceback.print_exc()
            results["rocm_tests"] = f"Error: {str(e)}"
            results["rocm_error_traceback"] = traceback.format_exc()
            self.status_messages["rocm"] = f"Failed: {str(e)}"
            
        # ====== QUALCOMM AI TESTS ======
        try:
            # Check if Qualcomm AI Engine Runtime is available
            has_qualcomm = False
            qualcomm_version = None
            try:
                import qai_hub
                has_qualcomm = True
                if hasattr(qai_hub, "__version__"):
                    qualcomm_version = qai_hub.__version__
                print("Qualcomm AI Engine Runtime is available")
            except ImportError:
                has_qualcomm = False
            
            # Record Qualcomm hardware information
            results["hardware_qualcomm_detected"] = has_qualcomm
            if qualcomm_version:
                results["hardware_qualcomm_version"] = qualcomm_version
            
            if has_qualcomm:
                # Add detailed hardware information if available
                results["hardware_qualcomm_device"] = "Qualcomm AI Hardware"
                
                # Check if init_qualcomm method exists
                if hasattr(self.model, "init_qualcomm"):
                    # Record method availability for observability
                    results["qualcomm_init_available"] = True
                    results["qualcomm_init_method_type"] = type(self.model.init_qualcomm).__name__
                    
                    # Run the platform test
                    qualcomm_results = self._run_platform_test("qualcomm", self.model.init_qualcomm, "qualcomm")
                    results.update(qualcomm_results)
                else:
                    # Detailed information about what's missing
                    results["qualcomm_tests"] = "Qualcomm AI backend not implemented"
                    results["qualcomm_init_available"] = False
                    results["qualcomm_implementation_status"] = "Missing init_qualcomm method"
                    self.status_messages["qualcomm"] = "Qualcomm AI backend not implemented"
            else:
                # Detailed dependency absence information
                results["qualcomm_tests"] = "Qualcomm AI Engine Runtime not available"
                results["qualcomm_import_status"] = "qai_hub module not found"
                self.status_messages["qualcomm"] = "Qualcomm AI Engine Runtime not available"
        except Exception as e:
            print(f"Error in Qualcomm AI tests: {e}")
            traceback.print_exc()
            results["qualcomm_tests"] = f"Error: {str(e)}"
            results["qualcomm_error_traceback"] = traceback.format_exc()
            self.status_messages["qualcomm"] = f"Failed: {str(e)}"

        # Create structured results with status, examples and metadata
        structured_results = {
            "status": results,
            "examples": self.examples,
            "metadata": {
                "model_name": self.model_name,
                "model_type": "bert",
                "primary_task": "fill-mask",
                "test_timestamp": datetime.datetime.now().isoformat(),
                "python_version": sys.version,
                "torch_version": torch.__version__ if hasattr(torch, "__version__") else "Unknown",
                "transformers_version": transformers.__version__ if hasattr(transformers, "__version__") else "Unknown",
                "platform_status": self.status_messages
            }
        }

        return structured_results

    def __test__(self):
        # Run tests and compare/save results
        test_results = {}
        try:
            test_results = self.test()
        except Exception as e:
            test_results = {
                "status": {"test_error": str(e)},
                "examples": [],
                "metadata": {
                    "error": str(e),
                    "traceback": traceback.format_exc()
                }
            }
        
        # Create directories if they don't exist
        base_dir = os.path.dirname(os.path.abspath(__file__))
        expected_dir = os.path.join(base_dir, 'expected_results')
        collected_dir = os.path.join(base_dir, 'collected_results')
        
        # Create directories with appropriate permissions
        for directory in [expected_dir, collected_dir]:
            if not os.path.exists(directory):
                os.makedirs(directory, mode=0o755, exist_ok=True)
        
        # Save collected results
        results_file = os.path.join(collected_dir, 'hf_bert_test_results.json')
        try:
            with open(results_file, 'w') as f:
                json.dump(test_results, f, indent=2)
            print(f"Saved collected results to {results_file}")
        except Exception as e:
            print(f"Error saving results to {results_file}: {str(e)}")
            
        # Compare with expected results if they exist
        expected_file = os.path.join(expected_dir, 'hf_bert_test_results.json')
        if os.path.exists(expected_file):
            try:
                with open(expected_file, 'r') as f:
                    expected_results = json.load(f)
                
                # Compare only status keys for backward compatibility
                status_expected = expected_results.get("status", expected_results)
                status_actual = test_results.get("status", test_results)
                
                # More detailed comparison of results
                all_match = True
                mismatches = []
                
                for key in set(status_expected.keys()) | set(status_actual.keys()):
                    if key not in status_expected:
                        mismatches.append(f"Missing expected key: {key}")
                        all_match = False
                    elif key not in status_actual:
                        mismatches.append(f"Missing actual key: {key}")
                        all_match = False
                    elif status_expected[key] != status_actual[key]:
                        # If the only difference is the implementation_type suffix, that's acceptable
                        if (
                            isinstance(status_expected[key], str) and 
                            isinstance(status_actual[key], str) and
                            status_expected[key].split(" (")[0] == status_actual[key].split(" (")[0] and
                            "Success" in status_expected[key] and "Success" in status_actual[key]
                        ):
                            continue
                        
                        mismatches.append(f"Key '{key}' differs: Expected '{status_expected[key]}', got '{status_actual[key]}'")
                        all_match = False
                
                if not all_match:
                    print("Test results differ from expected results!")
                    for mismatch in mismatches:
                        print(f"- {mismatch}")
                    print("\nWould you like to update the expected results? (y/n)")
                    user_input = input().strip().lower()
                    if user_input == 'y':
                        with open(expected_file, 'w') as ef:
                            json.dump(test_results, ef, indent=2)
                            print(f"Updated expected results file: {expected_file}")
                    else:
                        print("Expected results not updated.")
                else:
                    print("All test results match expected results.")
            except Exception as e:
                print(f"Error comparing results with {expected_file}: {str(e)}")
                print("Creating new expected results file.")
                with open(expected_file, 'w') as ef:
                    json.dump(test_results, ef, indent=2)
        else:
            # Create expected results file if it doesn't exist
            try:
                with open(expected_file, 'w') as f:
                    json.dump(test_results, f, indent=2)
                    print(f"Created new expected results file: {expected_file}")
            except Exception as e:
                print(f"Error creating {expected_file}: {str(e)}")

        return test_results

if __name__ == "__main__":
    try:
        # Run the test when this script is executed directly
        test_instance = test_hf_bert()
        test_results = test_instance.__test__()
        print("Test completed successfully")
    except Exception as e:
        print(f"Error running test: {e}")
        import traceback
        traceback.print_exc()
