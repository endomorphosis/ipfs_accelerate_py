#!/usr/bin/env python3
"""
Test script for the enhanced artifact discovery and retrieval system.

This script demonstrates and tests the artifact metadata extraction, discovery, 
retrieval, trend analysis, and comparison capabilities.
"""

import asyncio
import json
import logging
import os
import tempfile
import shutil
import time
from datetime import datetime
from typing import Dict, List, Any, Optional

import sys
# Add the parent directory to the path
sys.path.append('/home/barberb/ipfs_accelerate_py/test')

from distributed_testing.ci.api_interface import CIProviderInterface, CIProviderFactory
from distributed_testing.ci.artifact_metadata import ArtifactMetadata, ArtifactDiscovery
from distributed_testing.ci.artifact_retriever import ArtifactRetriever
from distributed_testing.ci.artifact_handler import ArtifactHandler, get_artifact_handler
from distributed_testing.ci.github_client import GitHubClient

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Mock CI provider for testing
class MockCIProvider(CIProviderInterface):
    """Mock CI provider for testing."""
    
    def __init__(self, provider_name):
        """Initialize the mock provider."""
        self.provider_name = provider_name
        self.artifacts = {}
        self.test_runs = {}
        self.artifact_urls = {}
        self.pr_comments = {}
        self.build_status = {}
    
    async def initialize(self, config: Dict[str, Any]) -> bool:
        """Initialize the CI provider with configuration."""
        logger.info(f"Initializing {self.provider_name} provider with config: {config}")
        return True
    
    async def create_test_run(self, test_run_data: Dict[str, Any]) -> Dict[str, Any]:
        """Create a new test run."""
        test_run_id = f"{self.provider_name}-test-run-{len(self.test_runs) + 1}"
        self.test_runs[test_run_id] = {
            "id": test_run_id,
            "name": test_run_data.get("name", f"Test Run {test_run_id}"),
            "status": "running",
            "start_time": datetime.now().isoformat(),
            "data": test_run_data
        }
        logger.info(f"Created test run {test_run_id} for {self.provider_name}")
        return self.test_runs[test_run_id]
    
    async def update_test_run(self, test_run_id: str, update_data: Dict[str, Any]) -> bool:
        """Update a test run."""
        if test_run_id not in self.test_runs:
            logger.error(f"Test run {test_run_id} not found for {self.provider_name}")
            return False
        
        self.test_runs[test_run_id].update(update_data)
        logger.info(f"Updated test run {test_run_id} for {self.provider_name}")
        return True
    
    async def add_pr_comment(self, pr_number: str, comment: str) -> bool:
        """Add a comment to a pull request."""
        if pr_number not in self.pr_comments:
            self.pr_comments[pr_number] = []
        
        self.pr_comments[pr_number].append(comment)
        logger.info(f"Added comment to PR {pr_number} for {self.provider_name}")
        return True
    
    async def upload_artifact(self, test_run_id: str, artifact_path: str, artifact_name: str) -> bool:
        """Upload an artifact for a test run."""
        if test_run_id not in self.artifacts:
            self.artifacts[test_run_id] = []
        
        self.artifacts[test_run_id].append({
            "path": artifact_path,
            "name": artifact_name,
            "upload_time": datetime.now().isoformat()
        })
        
        # Mock artifact URL
        if test_run_id not in self.artifact_urls:
            self.artifact_urls[test_run_id] = {}
        
        self.artifact_urls[test_run_id][artifact_name] = f"https://{self.provider_name}.example.com/artifacts/{test_run_id}/{artifact_name}"
        
        logger.info(f"Uploaded artifact {artifact_name} for test run {test_run_id} with {self.provider_name}")
        return True
    
    async def get_artifact_url(self, test_run_id: str, artifact_name: str) -> Optional[str]:
        """Get the URL for a test run artifact."""
        if test_run_id in self.artifact_urls and artifact_name in self.artifact_urls[test_run_id]:
            return self.artifact_urls[test_run_id][artifact_name]
        return None
    
    async def get_test_run_status(self, test_run_id: str) -> Dict[str, Any]:
        """Get the status of a test run."""
        if test_run_id not in self.test_runs:
            logger.error(f"Test run {test_run_id} not found for {self.provider_name}")
            return {"status": "unknown", "error": "Test run not found"}
        
        return self.test_runs[test_run_id]
    
    async def set_build_status(self, status: str, description: str) -> bool:
        """Set the build status in the CI system."""
        self.build_status = {
            "status": status,
            "description": description,
            "time": datetime.now().isoformat()
        }
        
        logger.info(f"Set build status to {status} for {self.provider_name}")
        return True
    
    async def close(self) -> None:
        """Close the CI provider and clean up resources."""
        logger.info(f"Closed {self.provider_name} provider")

async def test_artifact_metadata_extraction():
    """Test extraction of metadata from various artifact types."""
    logger.info("Testing artifact metadata extraction...")
    
    # Create a temporary directory for test files
    temp_dir = tempfile.mkdtemp()
    
    try:
        # Create test artifact files
        artifacts = []
        
        # Create a log file
        log_path = os.path.join(temp_dir, "test.log")
        with open(log_path, "w") as f:
            f.write("Sample log content\nLine 2\nLine 3\nError: Something went wrong\nInfo: Test completed\n")
        
        artifacts.append(
            ArtifactMetadata(
                artifact_name="test.log",
                artifact_path=log_path
            )
        )
        
        # Create a JSON report file
        report_path = os.path.join(temp_dir, "test_report.json")
        with open(report_path, "w") as f:
            json.dump({
                "tests": 10,
                "passed": 8,
                "failed": 2,
                "skipped": 0,
                "duration": 15.3,
                "test_cases": [
                    {"name": "test_1", "status": "passed", "duration": 1.2},
                    {"name": "test_2", "status": "passed", "duration": 0.8},
                    {"name": "test_3", "status": "failed", "duration": 2.1}
                ]
            }, f, indent=2)
        
        artifacts.append(
            ArtifactMetadata(
                artifact_name="test_report.json",
                artifact_path=report_path
            )
        )
        
        # Create a performance report
        perf_path = os.path.join(temp_dir, "performance_benchmark.json")
        with open(perf_path, "w") as f:
            json.dump({
                "throughput": 1250.5,
                "latency": 8.2,
                "memory_usage": 1024.3,
                "cpu_usage": 76.5,
                "duration": 60.0,
                "benchmark_details": {
                    "hardware": "cpu",
                    "model": "bert-base-uncased",
                    "batch_size": 4
                }
            }, f, indent=2)
        
        artifacts.append(
            ArtifactMetadata(
                artifact_name="performance_benchmark.json",
                artifact_path=perf_path,
                artifact_type="performance_report"
            )
        )
        
        # Verify metadata extraction
        for artifact in artifacts:
            logger.info(f"Artifact: {artifact.artifact_name}")
            logger.info(f"  Type: {artifact.artifact_type}")
            logger.info(f"  MIME Type: {artifact.mimetype}")
            logger.info(f"  Binary: {artifact.is_binary}")
            logger.info(f"  Size: {artifact.file_size} bytes")
            logger.info(f"  Content Hash: {artifact.content_hash}")
            
            # Check content metadata
            if artifact.content_metadata:
                logger.info(f"  Content Metadata: {json.dumps(artifact.content_metadata, indent=2)}")
                
                # Verify metrics extraction for reports
                if artifact.artifact_type in ["test_report", "performance_report"] and "metrics" in artifact.content_metadata:
                    logger.info(f"  Metrics: {json.dumps(artifact.content_metadata['metrics'], indent=2)}")
                    
                    # Verify we extracted appropriate metrics
                    if artifact.artifact_name == "test_report.json":
                        assert "tests" in artifact.content_metadata["metrics"]
                        assert "passed" in artifact.content_metadata["metrics"]
                        assert "failed" in artifact.content_metadata["metrics"]
                        assert artifact.content_metadata["metrics"]["tests"] == 10
                        assert artifact.content_metadata["metrics"]["passed"] == 8
                        assert artifact.content_metadata["metrics"]["failed"] == 2
                    
                    if artifact.artifact_name == "performance_benchmark.json":
                        assert "throughput" in artifact.content_metadata["metrics"]
                        assert "latency" in artifact.content_metadata["metrics"]
                        assert "memory_usage" in artifact.content_metadata["metrics"]
                        assert artifact.content_metadata["metrics"]["throughput"] == 1250.5
                        assert artifact.content_metadata["metrics"]["latency"] == 8.2
        
        logger.info("Artifact metadata extraction tests passed!")
    
    finally:
        # Clean up
        shutil.rmtree(temp_dir)

async def test_artifact_discovery():
    """Test discovery of artifacts based on various criteria."""
    logger.info("Testing artifact discovery...")
    
    # Create a temporary directory for test files
    temp_dir = tempfile.mkdtemp()
    
    try:
        # Create test artifacts with different characteristics
        artifacts = []
        
        # Test logs with different labels
        for i in range(5):
            log_path = os.path.join(temp_dir, f"test_{i}.log")
            with open(log_path, "w") as f:
                f.write(f"Sample log content {i}\nLine 2\nLine 3\n")
            
            artifact = ArtifactMetadata(
                artifact_name=f"test_{i}.log",
                artifact_path=log_path,
                test_run_id=f"test-run-{i % 3}",  # Distribute across 3 test runs
                provider_name="provider1" if i % 2 == 0 else "provider2",  # Alternate providers
                labels=["log", f"priority-{i % 3 + 1}"]  # Add priority labels
            )
            
            # Add additional labels to some artifacts
            if i % 2 == 0:
                artifact.add_label("daily-test")
            if i % 3 == 0:
                artifact.add_label("regression-test")
            
            # Add custom metadata
            artifact.add_metadata("version", f"1.{i}")
            artifact.add_metadata("platform", "linux" if i % 2 == 0 else "windows")
            
            artifacts.append(artifact)
        
        # Performance reports with metrics
        for i in range(3):
            perf_path = os.path.join(temp_dir, f"perf_{i}.json")
            with open(perf_path, "w") as f:
                json.dump({
                    "throughput": 1000 + i * 100,
                    "latency": 10 - i,
                    "memory_usage": 1000 + i * 200,
                    "cpu_usage": 70 + i * 5,
                    "duration": 60.0,
                    "benchmark_details": {
                        "hardware": "cuda" if i % 2 == 0 else "cpu",
                        "model": "bert-base-uncased",
                        "batch_size": 4 * (i + 1)
                    }
                }, f, indent=2)
            
            artifact = ArtifactMetadata(
                artifact_name=f"perf_{i}.json",
                artifact_path=perf_path,
                artifact_type="performance_report",
                test_run_id=f"perf-run-{i}",
                provider_name="provider1",
                labels=["performance", "benchmark"]
            )
            
            # Add custom metadata
            artifact.add_metadata("hardware", "cuda" if i % 2 == 0 else "cpu")
            artifact.add_metadata("batch_size", 4 * (i + 1))
            
            artifacts.append(artifact)
        
        # Test discovery by type
        log_artifacts = ArtifactDiscovery.discover_artifacts(
            artifacts=artifacts,
            artifact_type="log"
        )
        
        logger.info(f"Found {len(log_artifacts)} log artifacts")
        # The log files are created with auto-detected type "test_log", not "log"
        logs_and_test_logs = ArtifactDiscovery.discover_artifacts(
            artifacts=artifacts,
            name_pattern=r"test_.*\.log"
        )
        logger.info(f"Found {len(logs_and_test_logs)} log artifacts by name pattern")
        assert len(logs_and_test_logs) == 5, "Expected 5 log artifacts"
        
        # Test discovery by provider
        provider1_artifacts = ArtifactDiscovery.discover_artifacts(
            artifacts=artifacts,
            provider_name="provider1"
        )
        
        logger.info(f"Found {len(provider1_artifacts)} provider1 artifacts")
        assert len(provider1_artifacts) >= 3, "Expected at least 3 provider1 artifacts"
        
        # Test discovery by test run
        test_run_0_artifacts = ArtifactDiscovery.discover_artifacts(
            artifacts=artifacts,
            test_run_id="test-run-0"
        )
        
        logger.info(f"Found {len(test_run_0_artifacts)} artifacts for test-run-0")
        assert len(test_run_0_artifacts) > 0, "Expected artifacts for test-run-0"
        
        # Test discovery by labels
        regression_test_artifacts = ArtifactDiscovery.discover_artifacts(
            artifacts=artifacts,
            labels=["regression-test"]
        )
        
        logger.info(f"Found {len(regression_test_artifacts)} regression-test artifacts")
        assert len(regression_test_artifacts) > 0, "Expected regression-test artifacts"
        
        # Test discovery by metadata
        cuda_artifacts = ArtifactDiscovery.discover_artifacts(
            artifacts=artifacts,
            metadata_query={"hardware": "cuda"}
        )
        
        logger.info(f"Found {len(cuda_artifacts)} CUDA artifacts")
        assert len(cuda_artifacts) > 0, "Expected CUDA artifacts"
        
        # Test discovery by name pattern
        perf_artifacts = ArtifactDiscovery.discover_artifacts(
            artifacts=artifacts,
            name_pattern="perf_.*"
        )
        
        logger.info(f"Found {len(perf_artifacts)} perf artifacts by name pattern")
        assert len(perf_artifacts) == 3, "Expected 3 perf artifacts"
        
        # Test combined criteria
        combined_artifacts = ArtifactDiscovery.discover_artifacts(
            artifacts=artifacts,
            provider_name="provider1",
            labels=["daily-test"]
        )
        
        logger.info(f"Found {len(combined_artifacts)} provider1 daily-test artifacts")
        assert len(combined_artifacts) > 0, "Expected provider1 daily-test artifacts"
        
        # Test grouping by type
        grouped_artifacts = ArtifactDiscovery.group_artifacts_by_type(artifacts)
        
        logger.info(f"Grouped artifacts by type: {list(grouped_artifacts.keys())}")
        assert "test_log" in grouped_artifacts, "Expected test_log group"
        assert "performance_report" in grouped_artifacts, "Expected performance_report group"
        
        # Test finding latest artifact
        latest_perf = ArtifactDiscovery.find_latest_artifact(
            artifacts=artifacts,
            artifact_type="performance_report"
        )
        
        logger.info(f"Latest performance artifact: {latest_perf.artifact_name}")
        assert latest_perf is not None, "Expected to find latest performance artifact"
        
        # Test extracting metrics
        perf_metrics = ArtifactDiscovery.extract_metrics_from_artifacts(
            artifacts=[a for a in artifacts if a.artifact_type == "performance_report"],
            metric_names=["throughput", "latency", "memory_usage"]
        )
        
        logger.info(f"Extracted performance metrics: {perf_metrics}")
        assert "throughput" in perf_metrics, "Expected throughput metrics"
        assert "latency" in perf_metrics, "Expected latency metrics"
        assert "memory_usage" in perf_metrics, "Expected memory_usage metrics"
        assert len(perf_metrics["throughput"]) == 3, "Expected 3 throughput values"
        
        logger.info("Artifact discovery tests passed!")
    
    finally:
        # Clean up
        shutil.rmtree(temp_dir)

async def test_artifact_retrieval():
    """Test retrieval and caching of artifacts from providers."""
    logger.info("Testing artifact retrieval...")
    
    # Create a temporary directory for test files and cache
    temp_dir = tempfile.mkdtemp()
    cache_dir = os.path.join(temp_dir, "cache")
    artifacts_dir = os.path.join(temp_dir, "artifacts")
    
    os.makedirs(cache_dir, exist_ok=True)
    os.makedirs(artifacts_dir, exist_ok=True)
    
    try:
        # Create test artifacts
        log_path = os.path.join(artifacts_dir, "test.log")
        with open(log_path, "w") as f:
            f.write("Sample log content\nLine 2\nLine 3\n")
        
        report_path = os.path.join(artifacts_dir, "test_report.json")
        with open(report_path, "w") as f:
            json.dump({
                "tests": 10,
                "passed": 8,
                "failed": 2,
                "duration": 15.3
            }, f, indent=2)
        
        # Create providers
        providers = {
            "github": MockCIProvider("github"),
            "gitlab": MockCIProvider("gitlab")
        }
        
        # Initialize providers
        for name, provider in providers.items():
            await provider.initialize({})
        
        # Create test runs
        test_runs = {}
        for name, provider in providers.items():
            test_run = await provider.create_test_run({
                "name": f"{name} Test Run"
            })
            test_runs[name] = test_run
        
        # Upload artifacts to providers
        for name, provider in providers.items():
            await provider.upload_artifact(
                test_run_id=test_runs[name]["id"],
                artifact_path=log_path,
                artifact_name="test.log"
            )
            
            await provider.upload_artifact(
                test_run_id=test_runs[name]["id"],
                artifact_path=report_path,
                artifact_name="test_report.json"
            )
        
        # Create artifact retriever
        retriever = ArtifactRetriever(
            cache_dir=cache_dir,
            max_cache_size_mb=100,
            providers=providers
        )
        
        # The MockCIProvider doesn't actually serve files via HTTP, so we'll
        # create a local HTTP file server to simulate artifact downloads.
        
        # Instead of setting up a real HTTP server, we'll just mock the
        # session.get method by manipulating the response
        
        # Method to simulate artifact download
        async def test_retrieval_without_real_http():
            # Test retrieval by manipulating providers
            for name, provider in providers.items():
                # For each mock provider, create a retrieval method that
                # returns the actual file content instead of downloading from URL
                
                original_get_artifact_url = provider.get_artifact_url
                
                # Override get_artifact_url for testing
                async def get_artifact_content(artifact_path):
                    # Instead of HTTP GET, directly return file content
                    # This is a mock implementation
                    if os.path.exists(artifact_path):
                        with open(artifact_path, "rb") as f:
                            return f.read()
                    return None
                
                # Test retrieving log artifact
                test_run_id = test_runs[name]["id"]
                
                # Set up response using mocked file content
                artifact_name = "test.log"
                test_log_content = get_artifact_content(log_path)
                
                # Test retrieving report artifact
                artifact_name = "test_report.json"
                test_report_content = get_artifact_content(report_path)
                
                # Instead of actually retrieving via HTTP, we'll directly add
                # the artifacts to the cache to simulate retrieval
                
                # For log artifact
                log_cache_path, log_cache_key = retriever._get_cache_path_for_artifact(
                    test_run_id=test_run_id,
                    artifact_name="test.log",
                    provider_name=name
                )
                
                # Create directory if it doesn't exist
                os.makedirs(os.path.dirname(log_cache_path), exist_ok=True)
                
                # Write content to cache file
                with open(log_cache_path, "wb") as f:
                    f.write(test_log_content)
                
                # Create log artifact metadata
                log_metadata = ArtifactMetadata(
                    artifact_name="test.log",
                    artifact_path=log_cache_path,
                    test_run_id=test_run_id,
                    provider_name=name
                )
                
                # Update cache metadata
                retriever.cache_metadata[log_cache_key] = {
                    "cache_path": log_cache_path,
                    "download_time": time.time(),
                    "last_access_time": time.time(),
                    "url": f"https://{name}.example.com/artifacts/{test_run_id}/test.log",
                    "size": os.path.getsize(log_cache_path),
                    "content_hash": log_metadata.content_hash,
                    "metadata": {
                        "artifact_type": log_metadata.artifact_type,
                        "provider_specific_id": log_metadata.provider_specific_id,
                        "labels": log_metadata.labels,
                        "additional_metadata": log_metadata.metadata
                    }
                }
                
                # For report artifact
                report_cache_path, report_cache_key = retriever._get_cache_path_for_artifact(
                    test_run_id=test_run_id,
                    artifact_name="test_report.json",
                    provider_name=name
                )
                
                # Create directory if it doesn't exist
                os.makedirs(os.path.dirname(report_cache_path), exist_ok=True)
                
                # Write content to cache file
                with open(report_cache_path, "wb") as f:
                    f.write(test_report_content)
                
                # Create report artifact metadata
                report_metadata = ArtifactMetadata(
                    artifact_name="test_report.json",
                    artifact_path=report_cache_path,
                    test_run_id=test_run_id,
                    provider_name=name
                )
                
                # Update cache metadata
                retriever.cache_metadata[report_cache_key] = {
                    "cache_path": report_cache_path,
                    "download_time": time.time(),
                    "last_access_time": time.time(),
                    "url": f"https://{name}.example.com/artifacts/{test_run_id}/test_report.json",
                    "size": os.path.getsize(report_cache_path),
                    "content_hash": report_metadata.content_hash,
                    "metadata": {
                        "artifact_type": report_metadata.artifact_type,
                        "provider_specific_id": report_metadata.provider_specific_id,
                        "labels": report_metadata.labels,
                        "additional_metadata": report_metadata.metadata
                    }
                }
                
                # Save cache metadata
                retriever._save_cache_metadata()
                
                # Test retrieving cached artifacts
                cached_log = await retriever.retrieve_artifact(
                    test_run_id=test_run_id,
                    artifact_name="test.log",
                    provider_name=name,
                    use_cache=True
                )
                
                assert cached_log is not None, f"Failed to retrieve cached log for {name}"
                cached_log_path, cached_log_metadata = cached_log
                
                logger.info(f"Retrieved cached log for {name}: {cached_log_path}")
                logger.info(f"Log metadata: {cached_log_metadata.to_dict()}")
                
                assert os.path.exists(cached_log_path), f"Cached log file doesn't exist: {cached_log_path}"
                assert cached_log_metadata.artifact_type == "log", f"Unexpected artifact type: {cached_log_metadata.artifact_type}"
                
                cached_report = await retriever.retrieve_artifact(
                    test_run_id=test_run_id,
                    artifact_name="test_report.json",
                    provider_name=name,
                    use_cache=True
                )
                
                assert cached_report is not None, f"Failed to retrieve cached report for {name}"
                cached_report_path, cached_report_metadata = cached_report
                
                logger.info(f"Retrieved cached report for {name}: {cached_report_path}")
                logger.info(f"Report metadata: {cached_report_metadata.to_dict()}")
                
                assert os.path.exists(cached_report_path), f"Cached report file doesn't exist: {cached_report_path}"
                assert cached_report_metadata.artifact_type == "json", f"Unexpected artifact type: {cached_report_metadata.artifact_type}"
                
                # Test batch retrieval
                batch_artifacts = [
                    {
                        "test_run_id": test_run_id,
                        "artifact_name": "test.log",
                        "provider_name": name
                    },
                    {
                        "test_run_id": test_run_id,
                        "artifact_name": "test_report.json",
                        "provider_name": name
                    }
                ]
                
                batch_results = await retriever.retrieve_artifacts_batch(
                    artifacts=batch_artifacts,
                    use_cache=True
                )
                
                assert "test.log" in batch_results, f"Missing log in batch results for {name}"
                assert "test_report.json" in batch_results, f"Missing report in batch results for {name}"
                
                logger.info(f"Batch retrieval successful for {name}")
                
                # Simulate performance metric artifacts for trend analysis
                for i in range(3):
                    perf_path = os.path.join(artifacts_dir, f"perf_{i}.json")
                    with open(perf_path, "w") as f:
                        json.dump({
                            "throughput": 1000 + i * 100,
                            "latency": 10 - i,
                            "memory_usage": 1000 + i * 200,
                            "cpu_usage": 70 + i * 5,
                            "duration": 60.0,
                        }, f, indent=2)
                    
                    # Add to cache to simulate retrieval
                    perf_cache_path, perf_cache_key = retriever._get_cache_path_for_artifact(
                        test_run_id=test_run_id,
                        artifact_name=f"perf_{i}.json",
                        provider_name=name
                    )
                    
                    # Create directory if it doesn't exist
                    os.makedirs(os.path.dirname(perf_cache_path), exist_ok=True)
                    
                    # Write content to cache file
                    with open(perf_cache_path, "wb") as f:
                        with open(perf_path, "rb") as src:
                            f.write(src.read())
                    
                    # Create perf artifact metadata
                    perf_metadata = ArtifactMetadata(
                        artifact_name=f"perf_{i}.json",
                        artifact_path=perf_cache_path,
                        artifact_type="performance_report",
                        test_run_id=test_run_id,
                        provider_name=name
                    )
                    
                    # Update cache metadata
                    retriever.cache_metadata[perf_cache_key] = {
                        "cache_path": perf_cache_path,
                        "download_time": time.time() - (i * 86400),  # Simulate different days
                        "last_access_time": time.time(),
                        "url": f"https://{name}.example.com/artifacts/{test_run_id}/perf_{i}.json",
                        "size": os.path.getsize(perf_cache_path),
                        "content_hash": perf_metadata.content_hash,
                        "metadata": {
                            "artifact_type": "performance_report",
                            "provider_specific_id": None,
                            "labels": [],
                            "additional_metadata": {}
                        }
                    }
                
                # Save cache metadata
                retriever._save_cache_metadata()
            
            return True
        
        # Run the test
        test_result = await test_retrieval_without_real_http()
        assert test_result, "Retrieval test failed"
        
        # Test trend analysis
        trend_result = await retriever.analyze_metrics_trend(
            provider_name="github",
            artifact_type="performance_report",
            metric_name="throughput",
            days=30,
            max_artifacts=10
        )
        
        logger.info(f"Trend analysis result: {trend_result}")
        if "error" not in trend_result:
            assert "metric" in trend_result, "Missing metric field in trend result"
            assert "values" in trend_result, "Missing values field in trend result"
            if "trend" in trend_result and trend_result["trend"] is not None:
                assert trend_result["trend"] in ["increasing", "decreasing", "stable"], f"Invalid trend: {trend_result['trend']}"
        
        logger.info("Artifact retrieval tests passed!")
    
    finally:
        # Clean up
        shutil.rmtree(temp_dir)

async def test_provider_artifact_url_retrieval():
    """Test the artifact URL retrieval functionality from various CI providers."""
    logger.info("Testing CI provider artifact URL retrieval...")
    
    # Import the CI provider clients
    from distributed_testing.ci.jenkins_client import JenkinsClient
    from distributed_testing.ci.circleci_client import CircleCIClient
    from distributed_testing.ci.azure_client import AzureDevOpsClient
    
    # Create a temporary directory for test files
    temp_dir = tempfile.mkdtemp()
    
    try:
        # Create a test artifact file
        artifact_path = os.path.join(temp_dir, "test_artifact.json")
        with open(artifact_path, "w") as f:
            json.dump({"test": "data", "timestamp": "2023-01-01T00:00:00Z"}, f)
        
        # Test Jenkins client
        logger.info("Testing Jenkins client artifact URL retrieval...")
        jenkins_client = JenkinsClient()
        await jenkins_client.initialize({
            "url": "https://jenkins.example.com/",
            "user": "test_user",
            "token": "test_token"
        })
        
        jenkins_test_run = await jenkins_client.create_test_run({
            "name": "Jenkins Test Run",
            "job_name": "test-job",
            "build_id": "123"
        })
        
        jenkins_test_run_id = jenkins_test_run.get("id", "jenkins-test-job-123")
        artifact_name = "test_artifact.json"
        
        # Upload the artifact
        await jenkins_client.upload_artifact(jenkins_test_run_id, artifact_path, artifact_name)
        
        # Get the artifact URL
        jenkins_url = await jenkins_client.get_artifact_url(jenkins_test_run_id, artifact_name)
        logger.info(f"Jenkins artifact URL: {jenkins_url}")
        
        # Test CircleCI client
        logger.info("Testing CircleCI client artifact URL retrieval...")
        circleci_client = CircleCIClient()
        await circleci_client.initialize({
            "token": "test_token",
            "project_slug": "github/test-owner/test-repo"
        })
        
        circleci_test_run = await circleci_client.create_test_run({
            "name": "CircleCI Test Run",
            "pipeline_id": "test-pipeline",
            "workflow_id": "test-workflow",
            "job_number": "123"
        })
        
        circleci_test_run_id = circleci_test_run.get("id", "circleci-github/test-owner/test-repo-123")
        
        # Upload the artifact
        await circleci_client.upload_artifact(circleci_test_run_id, artifact_path, artifact_name)
        
        # Get the artifact URL
        circleci_url = await circleci_client.get_artifact_url(circleci_test_run_id, artifact_name)
        logger.info(f"CircleCI artifact URL: {circleci_url}")
        
        # Test Azure DevOps client
        logger.info("Testing Azure DevOps client artifact URL retrieval...")
        azure_client = AzureDevOpsClient()
        await azure_client.initialize({
            "token": "test_token",
            "organization": "test-org",
            "project": "test-project"
        })
        
        azure_test_run = await azure_client.create_test_run({
            "name": "Azure Test Run",
            "build_id": "123"
        })
        
        azure_test_run_id = azure_test_run.get("id", "123")
        
        # Upload the artifact
        await azure_client.upload_artifact(azure_test_run_id, artifact_path, artifact_name)
        
        # Get the artifact URL
        azure_url = await azure_client.get_artifact_url(azure_test_run_id, artifact_name)
        logger.info(f"Azure DevOps artifact URL: {azure_url}")
        
        # Close the clients
        await jenkins_client.close()
        await circleci_client.close()
        await azure_client.close()
        
        logger.info("CI provider artifact URL retrieval tests passed!")
    
    except Exception as e:
        logger.error(f"Error in CI provider artifact URL retrieval test: {str(e)}")
        raise
    
    finally:
        # Clean up
        shutil.rmtree(temp_dir)

async def run_all_tests():
    """Run all artifact discovery and retrieval tests."""
    logger.info("Running all artifact discovery and retrieval tests...")
    
    # Test artifact metadata extraction
    await test_artifact_metadata_extraction()
    
    # Test artifact discovery
    await test_artifact_discovery()
    
    # Test artifact retrieval
    await test_artifact_retrieval()
    
    # Test CI provider artifact URL retrieval
    await test_provider_artifact_url_retrieval()
    
    logger.info("All tests complete!")

if __name__ == "__main__":
    asyncio.run(run_all_tests())