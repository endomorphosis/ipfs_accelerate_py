from transformers import AutoConfig
import time
import json
import os
# The curated list of 256 unique Hugging Face model IDs (with "opus-mt" models removed).

        
# with open(os.path.join(this_dir, "found_models.json"), "w") as f:
#     json.dump(found_models, f)

# with open(os.path.join(this_dir, "model_mapping.json"), "w") as f:
#     json.dump(model_mappping, f)
class random_models:
    def __init__(self, resources=None, metadata=None):
        self.resources = resources
        self.metadata = metadata
    return None

    def __call__(self, *args, **kwds):
            
        model_list = [],
        "huggingface/bert-base-uncased",
        "huggingface/bert-large-uncased",
        "huggingface/bert-base-cased",
        "huggingface/bert-large-cased",
        "dbmdz/bert-base-german-cased",
        "dbmdz/bert-base-german-dbmdz-cased",
        "dbmdz/bert-base-german-uncased",
        "TurkuNLP/bert-base-finnish-cased-v1",
        "wietsedv/bert-base-dutch-cased",
        "distilbert/distilbert-base-uncased",
        "distilbert/distilbert-base-cased",
        "distilbert/distilbert-base-uncased-distilled-squad",
        "distilbert/distilbert-base-cased-distilled-squad",
        "distilbert/distilroberta-base",
        "facebook/roberta-base",
        "facebook/roberta-large",
        "ynie/roberta-large-mnli",
        "openai/roberta-base-openai-detector",
        "cardiffnlp/twitter-roberta-base",
        "cardiffnlp/twitter-roberta-base-sentiment",
        "openai/gpt2",
        "openai/gpt2-medium",
        "openai/gpt2-large",
        "openai/gpt2-xl",
        "EleutherAI/gpt-neo-125M",
        "EleutherAI/gpt-neo-1.3B",
        "EleutherAI/gpt-neo-2.7B",
        "EleutherAI/gpt-j-6B",
        "facebook/bart-base",
        "facebook/bart-large",
        "facebook/bart-large-mnli",
        "sshleifer/bart-tiny-random",
        "sshleifer/bart-base",
        "huggingface/t5-small",
        "huggingface/t5-base",
        "huggingface/t5-large",
        "huggingface/t5-3b",
        "google/mt5-small",
        "google/mt5-base",
        "google/mt5-large",
        "google/mt5-xl",
        "facebook/mbart-large-50",
        "google/electra-small-discriminator",
        "google/electra-base-discriminator",
        "google/electra-large-discriminator",
        "huggingface/xlnet-base-cased",
        "huggingface/xlnet-large-cased",
        "allenai/longformer-base-4096",
        "allenai/longformer-large-4096",
        "microsoft/deberta-v3-base",
        "microsoft/deberta-v3-large",
        "microsoft/DialoGPT-small",
        "microsoft/DialoGPT-medium",
        "microsoft/DialoGPT-large",
        "facebook/dpr-question_encoder-single-nq-base",
        "facebook/dpr-ctx_encoder-single-nq-base",
        "facebook/dpr-reader-single-nq-base",
        "nlpaueb/legal-bert-base-uncased",
        "microsoft/codebert-base",
        "deepset/roberta-base-squad2",
        "deepset/roberta-large-squad2",
        "google/bert_uncased_L-12_H-768_A-12",
        "google/bert_uncased_L-24_H-1024_A-16",
        "google/mobilebert-uncased",
        "microsoft/Multilingual-MiniLM-L12-H384",
        "sentence-transformers/all-mpnet-base-v2",
        "sentence-transformers/all-MiniLM-L6-v2",
        "sentence-transformers/all-distilroberta-v1",
        "sentence-transformers/bert-base-nli-mean-tokens",
        "sentence-transformers/paraphrase-MiniLM-L6-v2",
        "jplu/xlm-roberta-base",
        "jplu/xlm-roberta-large",
        "camembert/camembert-base",
        "flaubert/flaubert_small_cased",
        "flaubert/flaubert_base_uncased",
        "flaubert/flaubert_large_uncased",
        "distilbert/distilbert-base-multilingual-cased",
        "facebook/xlm-mlm-100-1280",
        "facebook/mbart-large-50-many-to-many-mmt",
        "Salesforce/codegen-350M-mono",
        "Salesforce/codegen-2B-mono",
        "sentence-transformers/paraphrase-distilroberta-base-v1",
        "sentence-transformers/paraphrase-xlm-r-multilingual-v1",
        "sentence-transformers/all-MiniLM-L12-v2",
        "sentence-transformers/multi-qa-mpnet-base-cos-v1",
        "albert/albert-base-v2",
        "albert/albert-large-v2",
        "albert/albert-xlarge-v2",
        "albert/albert-xxlarge-v2",
        "facebook/opt-125m",
        "facebook/opt-350m",
        "facebook/opt-1.3b",
        "facebook/opt-2.7b",
        "facebook/opt-6.7b",
        "facebook/wav2vec2-base-960h",
        "facebook/wav2vec2-large-960h-lv60",
        "openai/clip-vit-base-patch32",
        "openai/clip-vit-large-patch14",
        "sshleifer/distilbart-cnn-12-6",
        "sshleifer/bart-large-cnn",
        "google/flan-t5-small",
        "google/flan-t5-base",
        "google/flan-t5-large",
        "google/flan-t5-xl",
        "facebook/mbart-large-50-many-to-one-mmt",
        "csebuetnlp/mT5_multilingual_XLSum",
        "microsoft/CodeGPT-small-py",
        "textattack/bert-base-uncased-imdb",
        "textattack/bert-base-uncased-ag-news",
        "textattack/bert-base-uncased-yelp-polarity",
        "textattack/bert-base-uncased-rotten-tomatoes",
        "nlptown/bert-base-multilingual-uncased-sentiment",
        "finiteautomata/bertweet-base-sentiment-analysis",
        "valhalla/bert-base-uncased-finetuned-sst-2-english",
        "sshleifer/distilbart-xsum-12-6",
        "facebook/blenderbot-400M-distill",
        "facebook/blenderbot-90M",
        "facebook/blenderbot-3B",
        "sentence-transformers/distilbert-base-nli-stsb-mean-tokens",
        "nlpaueb/legal-bert-small-uncased",
        "emilyalsentzer/Bio_ClinicalBERT",
        "google/pegasus-xsum",
        "google/pegasus-large",
        "allenai/led-base-16384",
        "allenai/led-large-16384",
        "microsoft/xtremedistil-l6-h384-uncased",
        "cardiffnlp/twitter-roberta-base-emotion",
        "cardiffnlp/twitter-roberta-base-hate",
        "dbmdz/bert-large-cased-finetuned-conll03-english",
        "neuralmind/bert-base-portuguese-cased",
        "pierreguillou/bert-base-cased-squad-v1",
        "ProsusAI/finbert",
        "allegro/herbert-klej-cased",
        "allegro/herbert-base-cased",
        "TurkuNLP/bert-base-finnish-uncased",
        "indobenchmark/indobert-base-p1",
        "indolem/indobert-base-uncased",
        "HooshvareLab/bert-fa-base-uncased",
        "HooshvareLab/bert-fa-base-uncased-clf",
        "asafaya/bert-base-arabic",
        "asafaya/bert-base-arabic-sentiment",
        "hfl/chinese-bert-wwm-ext",
        "hfl/chinese-roberta-wwm-ext",
        "DeepPavlov/rubert-base-cased",
        "dccuchile/bert-base-spanish-wwm-cased",
        "dccuchile/bert-base-spanish-wwm-uncased",
        "dbmdz/bert-base-italian-cased",
        "ai4bharat/indic-bert",
        "dbmdz/bert-base-turkish-cased",
        "vinai/bertpho-cased",
        "Vamsi/T5_Paraphrase_Paws",
        "ramsrigouthamg/t5_paraphraser",
        "mrm8488/t5-base-finetuned-common_gen",
        "Salesforce/codet5-small",
        "Salesforce/codet5-base",
        "Salesforce/codet5-large",
        "j-hartmann/emotion-english-distilroberta-base",
        "bhadresh-savani/bert-base-uncased-emotion",
        "unitary/toxic-bert",
        "mrm8488/distilbert-base-uncased-finetuned-emotion",
        "lvwerra/distilbert-imdb",
        "facebook/bart-large-xsum",
        "google/pegasus-cnn_dailymail",
        "pszemraj/t5-small-finetuned-summarize-news",
        "pszemraj/t5-base-finetuned-summarize-news",
        "pszemraj/t5-large-finetuned-summarize-news",
        "valhalla/t5-small-qa-qg-hl",
        "valhalla/t5-base-qa-qg-hl",
        "valhalla/t5-large-qa-qg-hl",
        "google/pegasus-newsroom",
        "facebook/bart-base-samsum",
        "patrickvonplaten/t5-tiny-random",
        "sentence-transformers/roberta-large-nli-stsb-mean-tokens",
        "mrm8488/bert-tiny-finetuned-sst2",
        "papluca/xlm-roberta-base-language-detection",
        "microsoft/CodeBERTa-small-v1",
        "microsoft/mdeberta-v3-base",
        "csebuetnlp/mT5_small_MNLI",
        "bigscience/bloom-560m",
        "bigscience/bloom-1b1",
        "bigscience/bloom-1b7",
        "bigscience/bloom-3b",
        "bigscience/bloom-7b1",
        "cardiffnlp/twitter-xlm-roberta-base-sentiment",
        "mrm8488/distilroberta-base-finetuned-financial-news-sentiment",
        "j-hartmann/emotion-multi-class-multilingual-roberta",
        "flax-community/longformer-large-4096-finetuned-squad2",
        "cahya/bert-base-indonesian-1.5G",
        "ktrapeznikov/albert-xlarge-v2-squad-v2",
        "ynie/roberta-large-snli_mnli_fever_anli_R1_R2_R3-nli",
        "google/t5-v1_1-small",
        "google/t5-v1_1-base",
        "google/t5-v1_1-large",
        "google/t5-v1_1-xl",
        "cardiffnlp/twitter-roberta-base-offensive",
        "deepset/minilm-uncased-squad2",
        "vblagoje/bert-english-uncased-finetuned-squad",
        "microsoft/graphcodebert-base",
        "microsoft/graphcodebert-large",
        "philschmid/bart-large-cnn-samsum",
        "indobenchmark/indobert-base-p1-squad2",
        "flax-community/led-base-16384-finetuned-summarization",
        "openai/roberta-large-openai-detector",
        "google/vit-base-patch16-224",
        "google/vit-base-patch16-384",
        "google/vit-large-patch16-224",
        "google/vit-large-patch16-384",
        "facebook/deit-base-distilled-patch16-224",
        "facebook/deit-base-patch16-224",
        "facebook/deit-small-patch16-224",
        "facebook/detr-resnet-50",
        "facebook/detr-resnet-101",
        "microsoft/swin-tiny-patch4-window7-224",
        "microsoft/swin-small-patch4-window7-224",
        "microsoft/swin-base-patch4-window7-224",
        "microsoft/swin-large-patch4-window7-224",
        "facebook/convnext-base-224",
        "facebook/convnext-small-224",
        "facebook/convnext-tiny-224",
        "huggingface/beit-base-patch16-224",
        "huggingface/beit-large-patch16-224",
        "huggingface/convnext-base",
        "huggingface/convnext-small",
        "huggingface/convnext-tiny",
        "jonatasgrosman/wav2vec2-large-xlsr-53",
        "facebook/hubert-large-ls960",
        "facebook/hubert-base-ls960",
        "microsoft/CodeGPT-small-java",
        "google/bert-base-multilingual-cased",
        "allenai/scibert_scivocab_uncased",
        "allenai/specter",
        "rinna/japanese-gpt2-small",
        "rinna/japanese-gpt2-medium",
        "rinna/japanese-gpt2-large",
        "rinna/japanese-gpt2-xl",
        "flaubert/flaubert_large_cased",
        "dmis-lab/biobert-v1.1",
        "squeezebert/squeezebert-uncased",
        "prajjwal1/bert-tiny",
        "microsoft/layoutlm-base-uncased",
        "microsoft/layoutlm-large-uncased",
        "cl-tohoku/bert-base-japanese",
        "cl-tohoku/bert-base-japanese-char",
        "monologg/koelectra-base-v3-discriminator",
        "monologg/koelectra-small-v3-discriminator",
        "monologg/koelectra-base-v3-generator",
        "huggingface/distilgpt2",
        "google/electra-small-generator",
        "google/electra-base-generator",
        "google/electra-large-generator",
        "superb/wav2vec2-base-superb",
        "superb/hubert-base-superb",
        "allenai/biomed_roberta_base",
        "microsoft/deberta-base",
        "openai/whisper-small",
        "distil-whisper/distil-small.en",
        "google-t5/t5-base",
        "BAAI/bge-small-en-v1.5",
        "laion/larger_clap_general",
        "facebook/wav2vec2-large-960h-lv60-self",
        "openai/clip-vit-base-patch16",
        "openai/whisper-large-v3-turbo",
        "meta-llama/Meta-Llama-3.1-8B-Instruct",
        "distil-whisper/distil-small.en",
        "Qwen/Qwen2-7B",
        "llava-hf/llava-interleave-qwen-0.5b-hf",
        "lmms-lab/LLaVA-Video-7B-Qwen2",
        "llava-hf/llava-v1.6-mistral-7b-hf",
        "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
        "TIGER-Lab/Mantis-8B-siglip-llama3",
        "microsoft/xclip-base-patch16-zero-shot",
        "google/vit-base-patch16-224",
        "MCG-NJU/videomae-base",
        "MCG-NJU/videomae-large",
        "laion/CLIP-ViT-H-14-laion2B-s32B-b79K",
        "lmms-lab/llava-onevision-qwen2-7b-si",
        "lmms-lab/llava-onevision-qwen2-7b-ov",
        "lmms-lab/llava-onevision-qwen2-0.5b-si",
        "lmms-lab/llava-onevision-qwen2-0.5b-ov",
        "Qwen/Qwen2-VL-7B-Instruct",
        "OpenGVLab/InternVL2_5-1B",
        "OpenGVLab/InternVL2_5-8B",
        "OpenGVLab/PVC-InternVL2-8B",
        "AIDC-AI/Ovis1.6-Llama3.2-3B",
        "BAAI/Aquila-VL-2B-llava-qwen"
        ]

    # Iterate over the list and try to load each model's configuration.
        found_models = [],]
        found_classes = [],]
        for model_id in model_list:
            try:
                config = AutoConfig.from_pretrained(model_id)
                ## what is the model type for this config object?
                model_type = config.model_type
                print(f"\1{model_type}\3")
                found_models.append(model_id)
                found_classes.append(model_type)
            except Exception as e:
                print(f"\1{e}\3")
            # A short pause to avoid overwhelming the server (optional)
                time.sleep(0.1)


                print(f"\1{len(model_list)}\3")
                print(f"\1{len(set(found_classes))}\3")
                mapped_classes = [],]
                mapped_models = [],]
                model_mappping = dict(zip(found_models, found_classes))
        for k, v in model_mappping.items():
            if v not in mapped_classes:
                mapped_classes.append(v)
                mapped_models.append(k)

                mapped_models = dict(zip(mapped_classes, mapped_models))
                print(f"Found {len(mapped_classes)} unique classes")
                this_dir = os.path.dirname(os.path.abspath(__file__))
        with open(os.path.join(this_dir, "mapped_models.json"), "w") as f:
            json.dump(mapped_models, f)        
                pass
            return mapped_models